genericunivariateselect has silly defaults it uses ``percentile`` but gives the param as ``1e 5`` not sure it is worth the deprecation cycle though >>>need_contributor
selectpercentile checks for percentile and percentile 100 it looks like selectpercentile disallows the boundaries in `` check params`` but then treats them differently that seems odd either one of the cases must not be covered or something odd is happening >>>easy need_contributor
indexerror invalid slice in test non meta estimators called with genericunivariateselect when running the tests with python against numpy and scipy dev from http travis dev wheels scipy org >>>bug moderate need_contributor
overflowerror value too large to convert to int in sgdregressor when running the tests with python against numpy and scipy dev from http travis dev wheels scipy org similar error happens with passiveaggressiveregressor >>>bug moderate need_contributor
mrg fix test label binarizer on windows fix 6280 looks like fill is not very friendly with unicode numpy scalar on windows this pattern was used here https github com scikit learn scikit learn blob 74475bc929960d98939a12fc2fb68c0e006895be sklearn preprocessing label py l614 >>>needs_review
add numpy dev wheel to travis build matrix >>>build_/_ci
stacking add ensemble selection from libraries of models the following paper by rich caruana http www niculescu mizil org papers shotgun icml04 revised rev2 pdf presents an simple greedy algorithm to stack models it is reported by many people to be an excellent strategy in data competitions we should implement it the paper has only 333 citations which is on the low side for our criteria but hear lot of good on it >>>new_feature
add timeseriescv and homogeneoustimeseriescv get this asked about once day so think we should just add it many people work with time series and adding cross validation for them would be really easy the standard strategy is described for example here http stats stackexchange com questions 14099 using fold cross validation for time series model selection there are basically two cases homogeneous time series one sample every seconds days or heterogeneous time series where each sample has time stamp for the homogeneous case we can just put the first ``n samples folds`` in the first fold etc so it very simple variation of kfold for heterogeneous case we need to get ``labels`` array and split accordingly if we cast that to integers people could actually provide pandas time series and they would be handled correctly they will be converted to nanoseconds remember arguing against this addition but changed my mind >>>need_contributor new_feature
lda doesn produce probabilities not sure if this is bug or documentation issue but `latentdirichletallocation` doesn produce normalized probabilities from `transform` and doesn explain how to get these either >>>bug
example document clustering py has broken it seems that `examples text document clustering py` has broken since `silhouette score` don accept sparse matrix following is the error messages traceback most recent call last file examples text document clustering py line 202 in metrics silhouette score km labels sample size 1000 file users yenchen desktop python scikit learn sklearn metrics cluster unsupervised py line 84 in silhouette score labels check labels file users yenchen desktop python scikit learn sklearn utils validation py line 516 in check ensure min features warn on dtype estimator file users yenchen desktop python scikit learn sklearn utils validation py line 375 in check array force all finite file users yenchen desktop python scikit learn sklearn utils validation py line 242 in ensure sparse format raise typeerror sparse matrix was passed but dense typeerror sparse matrix was passed but dense data is required use toarray to convert to dense numpy array >>>bug
added boruta and mutual information based feature selection modules dear fs module maintainers have implemented the boruta algorithm https m2 icm edu pl boruta with sklearn like interface here my original repo https github com danielhomola boruta py and here blog post wrote about this fs method http danielhomola com 2015 05 08 borutapy an all relevant feature selection method also implemented famous mutual information based fs methods jmi mrmr plus the pretty recent jmim and wrapped them up in single module with parallelized execution here my repo https github com danielhomola mifs and here blog post wrote about this method http danielhomola com 2016 01 31 mifs parallelized mutual information based feature selection module wrote docstrings and examples at the end of docstings also ran autopep8 on these files but could you help me with unit testing bit what is expected from feature selectors in terms of unittesing should simulate some data run these fs methods and assert that the discovered features are the same as in previous run also should write test for every single internal function as well best wishes dan >>>new_feature
partial dependence plot example not rendering on website see here http scikit learn org dev auto examples ensemble plot partial dependence html example ensemble plot partial dependence py >>>documentation easy need_contributor
test label binarizer failure under windows this failure is revealed by enabling missing tests in 6274 occurs under python either 32 bit or 64 bit >>>bug need_contributor
test pls test scale and stability failure under windows this is bug revealed by enabling some missing tests as part of 6274 it occurs under windows with python 64 bit and numpy with mkl https ci appveyor com project sklearn ci scikit learn build 5133 >>>bug need_contributor
some test directories are not included in installation in ``sklearn setup py`` there are several ``tests`` modules that are not included in the subpackage list for the configuration ``neighbors tests`` ``metrics tests`` etc this means that running ``nosetests sklearn`` outside the source directory will not run these tests is there reason for this >>>build_/_ci
inconsistent attribute naming cv mse path and mse path in lassolarscv and lassocv respectively there is inconsistent attribute names in cross validation lasso models `cv mse path in `lassolarscv` code https github com scikit learn scikit learn blob c957249 sklearn linear model least angle py l1127 `mse path in `lassocv` code https github com scikit learn scikit learn blob c957249 sklearn linear model coordinate descent py l1151 as seen already in this tutorial http scikit learn org stable auto examples linear model plot lasso model selection html unification suggested>>>easy
pr tags used in sklearn was thinking of adding small section in the documentation telling what each tag eg mrg enh etc stands for and when it used somewhere over here http scikit learn org dev developers contributing html contributing code might help beginners in scikit learn and even in other organisations for that matter >>>documentation
inconsistent tsne results seem to be getting inconsistent unexpected results when using sne compared to other implementations ve uploaded and documented reproducible example together with some observations which may help someone to arrive to the root cause as stuck https github com jjvalletta sneissue tree master thanks in advance for anyone looking into this cheers jj>>>bug
partial fit should raise an error when `y` in each batch contains labels not present in `classes first rng randn ovr onevsrestclassifier sgdclassifier ovr partial fit first ovr partial fit first this is the problem with the base estimator as well sgd sgdclassifier sgd partial fit first sgd partial fit first >>>bug
labelbinarizer returns dense output when input has only single label even when sparse output is set to true lb labelbinarizer sparse output true lb fit transform array >>>bug
implement different cut criteria for agglomerative clustering we currently have clusters as criterion and single way to cut the tree not sure what our strategy is called scipy implements many more strategies in particular distance http docs scipy org doc scipy 14 reference generated scipy cluster hierarchy fcluster html scipy cluster hierarchy fcluster gaelvaroquaux do we have the inconsistent in this nomenclature >>>enhancement
unexpected dbscan result guess hitting some boundary case but bit confused by this 894 1168 the second cluster has only one sample but ``min samples 5`` doesn that mean that the cluster should have at least samples ping larsmans jnothman >>>documentation
silhouette score error message this doesn give helpful error message indexerror index is out of bounds for axis with size the error is that ``silhouette score`` takes the data as an input not true labeling it gives these warnings so in 19 it will raise an error because the first input is 1d like to have better error for 18 though >>>easy
make weighted percentile more robust as reported by maniteja123 true weights weighted percentile true weights 50 do we want to do some sort of linear interpolation as described in this method https en wikipedia org wiki percentile the weighted percentile method>>>enhancement
sgd classification unnecessarily slow multiclass prediction using sgd on sparse data is unnecessarily slow it seems to be copying large arrays on every call to `predict` `predict proba` etc which kills its performance think the main culprit is `safe sparse dot` which uses scipy csr dense routine because `b` is transposed and scipy multiplication invokes `b ravel this is very slow copies `clf coef internally keeping `clf coef t` as contiguous array here https github com scikit learn scikit learn blob master sklearn linear model base py l252 improved the prediction performance of sgd classifier 7300x for us 1s vs 137s per call the exact speedup numbers will vary depending on `coef size the number of sgd classes and features this could be raised as an issue in scipy as well see no good reason for such inefficiency that `ravel is just too generous but since the fix seems trivial maybe it worth addressing on sklearn side as well this is using scipy 16 and sklearn 17 >>>bug need_contributor
check array in polynomialfeatures incompatible with sympy it would be really nice to be able to use `sympy` with `preprocessing polynomialfeatures` but can because this line https github com scikit learn scikit learn blob master sklearn preprocessing data py l1189 calls `check array with the default `dtype numeric it would be great if could pass in kwarg `dtype` to `polynomialfeatures` so that it wouldn fail when using symbolic variables instead of floats >>>easy need_contributor
supporting dropout for neural networks had recently come across this paper that emphasises the use of dropout to prevent overfitting and provides way of approximately combining exponentially many different neural network architectures efficiently it would be really great to add this dropout feature to current implementation of mlps in scikit learn https www cs toronto edu hinton absps jmlrdropout pdf would be really glad to take up this if you guys consider it worthwhile thanks >>>new_feature
birch fails with cfsubcluster object has no attribute centroid this issue occurs to me during an evaluation with growing feature set sizes in under second after the call to fit predict on the birch instance strangely it works for smaller sets of features and does not run out of memory for my evaluation employ 183719 datasets and construct feature sets of growing size it works with features but returns this error with 12 use the version here is the full stack trace sadly not able to reproduce this with an minimal example >>>bug
sklearn datasets make classification should have targets parameter `make regression` has `n targets` parameter it might be useful to have that in `make classification` as well>>>easy
better example for different agglomerative clustering methods maybe it me but feel the current example we have comparing agglomerative clustering algorithms doesn provide great intuition not super familiar with them so maybe it just my lack of understanding we currently have this http scikit learn org dev auto examples cluster plot digits linkage html example cluster plot digits linkage py that currently outputs pictures and timings the timings are the same for the three methods average complete ward guess you see that the average linkage has these two singletons and complete has one relatively small cluster maybe it would be nice to visualize the distribution of cluster sizes for each and show that ward creates more equally sized clusters maybe it would also be nice to compute ari or similar supervised clustering measure if there is an obvious clustering solution where the clusters have very different sizes how do the results of ward and the others differ what is happening on the cluster comparison plot in the top row why doesn ward get the circles right ping gaelvaroquaux agramfort >>>documentation enhancement
spectralbiclustering fit doesn return self need to check for coclustering too >>>easy
eigenface example is confusing there is an example comparing decomposition methods on the olivetti faces http scikit learn org dev auto examples decomposition plot faces decomposition html it prints something like the top is wrong for everything but pca sparse pca and fa think it not the top nmf components they are not ordered and if you gave it more components they would look very differently same for dictionary learning or means we could fix the print message but maybe it would be better to use more components for the different methods even if we are not showing them not sure if that would blow up the runtime though >>>documentation
fix docstring signature mismatch in cython code similar to 2062 but for cython code am not sure how easy it is to find those >>>documentation easy need_contributor
mrg ensuring consistent transforms for kernelpca previously the following script created an assertionerror that because `kernelpca fit x` has reference to the original `x` for `transform` made one line change to make sure this was not the case as far as know think mblondel did not intend for the following above to raise an error pr includes small change to the source to ensure that copy to the fit data is kept regression test same script as above >>>needs_review
mrg default attribute info for kernelpca added some default attribute information to the kernelpca also added very minimal changes to the rest of the docstring >>>needs_review
make sphinx build without warnings let circle io error on warnings we should fix the remaining sphinx warnings here https circleci com gh scikit learn scikit learn 1629 and then make circle io error if there are any warnings grep ing for ``warnings`` guess this way we immediately see if someone broke anything in the docs >>>build_/_ci documentation
problems with data scaling in sklearn cross decomposition plsregression am trying to fit some spectral data using pls and am having difficulties with the module essentially when use the default value of `scale false` get prediction but all my predictions are scaled and just cannot figure out how to convert back to my original data space the same is true for the example code as you see the scaled prediction is just off assumed that should be able to revert back to the unscaled data using `pls scaled mean and `pls scaled std but that doesn seem to work for me any suggestions would be highly appreciated for those with more visual bend figure https cloud githubusercontent com assets 13950307 11730049 a1fcfa66 9f60 11e5 960f 90fa260eef6a png >>>bug
release broke dev doc updates the dev documentation has not been updated since the release guess the circle ci that does the upload doesn do reset to origin master before trying to commit new version >>>build_/_ci documentation
regression in silhouette score apparently introduced last minute regression here https github com scikit learn scikit learn commit f0f174b74fd9684748e7c425b74909272e6ab22d commitcomment 14775170 not sure if that deserves backport bugfix release >>>bug
mrg fix memory leak in barnes hut sne that seems to fix 5916 for me vighneshbirodkar still seems to have problems here is test program you can run it with `valgrind leak check full track origins yes python test tsne py logfile` and search for barnes in the logfile >>>needs_review
valueerror assignment destination is read only when paralleling with jobs when run `sparsecoder` with jobs there is chance to raise exception `valueerror assignment destination is read only` the code is shown as follow the bigger `data dims` is the higher chance get when `data dims` is small lower than 2000 verified everything works fine once `data dims` is bigger than 2000 there is chance to get the exception when `data dims` is bigger than 5000 it is 100 raised my version infor os os 10 11 python python 10 anaconda numpy 10 sklearn 17 the full error information is shown as follow >>>bug
mrg enh allow float32 to pass through with copy this is the easy path to fixing this was bit surprised to see the float64 default and the copies elsewhere in the sparse functions are there rules for consistency as to float32 vs float64 in the cython code >>>needs_review
mrg enh minor add quantile low high parameters to robust scaler >>>needs_review
original traceback in setup py should be displayed in get numpy status and get scipy status the original traceback when the import of numpy or scipy fails should be displayed to be able to debug issues like 5904 >>>enhancement
mrg issue tf idf computation fixes 4391 >>>needs_review
mrg enh add svdd to svm module hi noticed there was an interest to support vector data description algorithm so want to finish pr started by sklef which already contained working version rebased did some cleanups and extended documentation very interested in your reviews >>>needs_review
mrg add convergence warning in labelpropagation otherwise it remains unclear whether the convergence was reached or whether the algorithm ran out of iterations currently all the test cases trigger this warning that is what triggered the investigation which led to 5774 >>>needs_review
mrg read web pages as utf by default use with blocks to open files on windows the default encoding for `open filename is cp1252 calling `open filename read on utf encoded file the wikipedia pages being fetched fails with an error when it encounters multi byte unicode code points this was blocker bug also wrapped the file ops in with blocks as is the preferred style these days >>>needs_review
mrg maint deprecated spectralembedding renamed to laplacianeigenmap fixes 5875 https github com scikit learn scikit learn issues 5875 >>>needs_review
latentdirichletallocation doesn explain parameters in user guide the lda user guide doesn explain any of the parameters and only uses the greek letter notation not the actual parameter names http scikit learn org dev modules decomposition html latent dirichlet allocation lda>>>documentation easy need_contributor
website link to lda broken in example in the topic model example the lda class should link to the api docs but it doesn http scikit learn org dev auto examples applications topics extraction with nmf lda html example applications topics extraction with nmf lda py not sure what the problem is >>>documentation
rename spectral embedding to laplacian eigenmaps from what understand from this http sas uwaterloo ca aghodsib courses f06stat890 readings smdr ssl05 pdf and this https lvdmaaten github io publications papers tr dimensionality reduction review 2009 pdf paper on dimensionality reduction spectral embeddings refer to any technique that conducts an eigendecomposition of an affinity matrix not just laplacian eigenmaps those papers and numerous other explicitly state the algorithms spectral embedding includes for spectral embedding algorithms such as metric multidimensional scaling mds cox cox 1994 spectral clustering see weiss 1999 for review laplacian eigenmaps lle and isomap http www mitpressjournals org doi pdfplus 10 1162 0899766041732396 in fact the sklearn documentation was the only source could find that refers to laplacian eigenmaps and spectral embedding synonymously if am correct propose renaming spectral embedding to laplacian eigenmaps or at the very least correct the misleading documentation that says spectral embedding also known as laplacian eigenmaps spectral methods are able to reveal low dimensional structure in high dimensional data from the top or bottom eigenvectors of specially constructed matrices http sas uwaterloo ca aghodsib courses f06stat890 readings smdr ssl05 pdf >>>documentation
logisticregressioncv fails when labels are strings while logisticregression can handle string labels logisticregressioncv fails when labels are strings with `valueerror could not convert string to float` guess the problem comes from calling `y test check array test dtype np float64 ensure 2d false within function log reg scoring path in file logistic py>>>easy enhancement
mrg fixes issue 5482 disambiguation of parameter analyzer docs have been updated with which steps are actually skipped when providing callable as analyzer for countvectorizer additionally previous docstring wrongly indicated that this behavior only applies if analyzer word >>>needs_review
mrg insert versionadded versionchanged directives in docstrings for 18 issue 5505 versionadded versionchanged directives for new stuff in 18 it was not clear to me if should also add these at the file module level for new files did >>>needs_review
mrg compare add changes to whats new for https github com scikit learn scikit learn issues 5842 collaborated with pavlop >>>needs_review
robust pca think it would be great to add robust pca the original algorithm is here http statweb stanford edu candes papers robustpca pdf there is an implementation here https github com dfm pcp by dfm this paper using trimmed grassmann average promises better performance http files is tue mpg de black papers rga2014 pdf glennq implemented it here https github com glennq tga it is bit unclear to me at the moment what the benefits of the two approaches are and how they compare for example for outlier detection also think netflix uses an improved version of the candes algorithm for which didn find reference yet need to ping the netflix people >>>new_feature
mrg fixed issue 5830 capitalization in advanced installation documentation documentation around advanced installation had lost capitalization issue 5830 fixed capitalization and ensured other changes were kept in newer version >>>needs_review
mrg add contingency matrix documentation and example fix https github com scikit learn scikit learn issues 4805>>>needs_review
image links to examples not created as part of latex build the same links are created as part of an html build sample of errors thrown believe that this is related to this bug in sphinx https github com sphinx doc sphinx issues 372 originally posted here https github com scikit learn scikit learn issues 5729 >>>bug build_/_ci documentation
check if all changes in master are actually in whatsnew someone should check whether all changes that happened since the 17 branch split from master are actually present in the whatsnew rst file see also 5640>>>documentation easy
mrg fix issue 4964 model evaluation docs fix 4964 which was also addressing 4804 brier score loss documentation added >>>needs_review
mrg minor edits to multiclass documentation minor changes to become familiar with the process>>>needs_review
add contributors to whatsnew for 17 and 16 using git shortlog we should add all contributors to the whatsnew though without commit count and ordered alphabetically you can get the names using something like ``git log git shortlog n``>>>documentation easy
meta estimators for multi output learning think it would be useful to have meta estimators for turning classifier or regressor into multi output classifier or regressor it recurrent pattern and find myself reimplementing it every once in while this is of course useful for estimators that don have native multi output support but even for those that have like rf find that estimating model independently for each output sometimes works better class names `multioutputclassifier` and `multioutputregressor` >>>easy new_feature
add the diffusion map dimensionality reduction technique the diffusion map is dimensionality reduction technique that uses transitional probability as its distance measure it is noise resistant and non linear moreover the algorithm itself is fast and scalable ve used it with 50k samples wrote quick implementation here https gist github com rohanp 2b950640c94c8e293c40 it has been of interest in numerous scientific fields diffusion maps http www sciencedirect com science article pii s1063520306000546 1119 citations geometric diffusions as tool for harmonic analysis and structure definition of data diffusion maps http www pnas org content 102 21 7426 short 672 citations functional diffusion map noninvasive mri biomarker for early stratification of clinical brain tumor response http www pnas org content 102 15 5524 short 491 citations diffusion maps and coarse graining unified framework for dimensionality reduction graph partitioning and data set parameterization http ieeexplore ieee org xpls abs all jsp arnumber 1661543 450 citations furthermore google scholar search for diffusion map dimensionality reduction yields more results than isomap locally linear embedding or hessian eigenmapping if fix up my implementation would you all be interested in having this in sklearn >>>new_feature
expose base estimator interface in isolation forest as followup of 5678 it is clear that it is very important to have fine control over the tree structure of the base estimators in the forest for now the only way to tune this is through `max samples` which both acts on the effective number of training samples and on the `max depth` value of the trees believe we should decouple this and make it possible to adjust `max samples` without necessarily changing the tree structure and vice versa practically this simply means adding `max depth` and other tree hyper parameters to the interface of the `isolationforest` and plug default auto values wherever it is fit >>>enhancement
random state impossible to set for the entire run for testing replicability it is often important to have the entire execution controlled by seed for the pseudo random number generator in pure python it can be done with `random seed in numpy with `numpy random seed it seems that sklearn requires this to be done in every place separately it rather troublesome and especially so since it not immediately obvious where it needed for example `dummyclassifier` needs one but `bernoullinb` doesn thought it would worth implementing something like `np random seed functionality which seeds the entire execution of sklearn for all its functions and classes >>>documentation easy need_contributor
mrg improve benchmark on nmf previous benchmark used simulated data did not use the new coordinate descent solver 4852 and found the plot very uninformative scikit learn non negative matrix factorizationbenchmark results https cloud githubusercontent com assets 11065596 11060014 b3e180f6 879d 11e5 944e bfebc103d4b0 png scikit learn non negative matrix factorizationbenchmark results2 https cloud githubusercontent com assets 11065596 11060013 b3df64ce 879d 11e5 8dc4 40af574d676b png this new benchmark tests nmf on two datasets 20 newsgroup dataset sparse shape 11314 39116 olivetti faces dataset dense shape 400 4096 it uses three different solvers projected gradient deprecated coordinate descent multiplicative update 5295 not merged and three different initialization schemes random nndsvd nndsvdar the total running time is about minutes for 20 newsgroups dataset and minute for olivetti faces dataset on the plots each point corresponds to one more iteration figure https cloud githubusercontent com assets 11065596 11059895 de69c334 879c 11e5 97c7 25fea07acd3a png figure https cloud githubusercontent com assets 11065596 11059894 de6765bc 879c 11e5 8d77 972e4bf783e6 png >>>needs_review
discretizer `binarizer` transforms continuous values to two states or it would be nice to generalize this to an arbitrary number of states this preprocessor would produce scipy sparse matrix of shape samples features using the one of encoding the thresholds could be chosen uniformly between the min and max of each feature or using the quantiles for example using uniformly chosen thresholds if min max and feature value between and 33 would be encoded as value between 33 and 66 as and value between 66 and as my usecase is that this encoding might be more meaningful than continuous values when using `polynomialfeatures` possibly related to 1062 >>>new_feature
functiontransformer docstring is missing link to user guide >>>documentation easy
mrg fixed isolationforest max features predict fails input validation issue 5732 >>>needs_review
remove replace usage of scipy misc lena in the upcoming 17 release of scipy the lena image has been removed https github com scipy scipy pull 5162 in scikit learn several examples and tests https github com scikit learn scikit learn search python lena rely on the lena image these should be updated to maintain compatibility with scipy >>>documentation easy need_contributor
use detected os to give installation instructions see example https circle artifacts com gh nilearn nilearn 287 artifacts home ubuntu nilearn doc build html introduction html installation and discussion here https github com scikit learn scikit learn pull 5257 issuecomment 154098632>>>documentation moderate need_contributor
introduce solver auto for logisticregression logistic regression has many different possible solvers choosing the best is difficult and requires expert knowledge think that it would be useful to introduce solver auto that tries to make good guess on the best approach it would have to be tuned using heaving benchmarks on many different problems good place to start is using the script bench rcv1 logreg convergence py in benchmarks and applying it to variety of problems would people agree with such an idea >>>enhancement moderate new_feature
roc auc score does not handle nans infs well normally when given scores which are the same roc auc score will return but in the case of all nan inf it seems to return the auc for the labels if ranked in the order given 	in roc auc score np inf np inf np inf np inf 	c winpython 64bit python amd64 lib site packages numpy lib function base py 1114 runtimewarning invalid value encountered in subtract return slice1 slice2 	out 	in roc auc score np nan np nan np nan np nan 	out 	in 10 roc auc score np inf np inf np inf np inf 	out 10 	in 11 roc auc score np nan np nan np nan np nan 	out 11 	in 12 roc auc score 	out 12 >>>bug
isolationforest max features predict fails input validation when subsampling features `isolationforest` fails the input validation when calling `predict gives the following in `predict` one of the individual fitted estimators is used for input validation `self estimators validate predict check input true but it is passed the full `x` which has all the features after looking into it bit `bagging py` sub samples the features itself where as `forest py` delegates it to the underlying `decisiontree` >>>bug
unusable target reference when doing pdf doc build when building the pdf documentation make latexpdf there are many unusable target reference warnings think they are all dead links at least some of them seem to be it would be great to check fix them all >>>documentation easy
applications plot stock market py segfault with openblas and jobs not sure if should delay the release for this but this example is broken ``quotes historical yahoo`` was moved to ``quotes historical yahoo ochl`` or ``quotes historical yahoo ohlc`` don know which is the right one but they both give me segfaults that could just be my broken setup though can anyone with new matplotlib try >>>documentation need_contributor new_feature
mrg mlp bug fixes and removing `decision function` amueller fix the wrong way of using random state and mistakenly modifying parameter `loss` some minor fixes and more tests also included remove decision function to make the design particularly with forward pass` cleaner btw have already implemented dropout for mlp in my fork going to make pull request for that once this one is merged >>>needs_review
deprecation warnings in examples in 17 many but hard to find >>>blocker
mrg fix mlp batch size test warning for default value initializing batch size to auto and setting it to samples value for the default case >>>needs_review
mrg split data using safe split in permutaion test scorer to fix error when using pandas dataframe series related to issue 5696 >>>needs_review
aim to raise warnings as errors in test suite going through the fun of seeing all the deprecated usages of numpy in master and 17 think it would be nice to let nose break on warnings don think there should be any warnings when running the tests and that would put the effort much more on the contributors instead of the maintainers >>>build_/_ci
boolean mask misshape in gaussiannb warning in test sklearn tests test naive bayes test gnb under numpy 10 looks suspicious >>>easy
lda bugfix 5092 lost in refactoring noticed that 5092 did not make it into the refactored code in `discriminant analysis py` line 58 >>>bug
mlp batch size warning the mlp tests raise many warnings in particular they warn because the default batch size is larger than some datasets suggest setting the default batch size to auto which is ``min whateveritisnow shape `` raising warning about our default value makes no sense to me >>>easy
gaussian process uses string comparison on arrays the new gp raises lot of futurewarning elementwise comparison failed in the tests this should be avoided by checking if the parameter is string as done here https github com scikit learn scikit learn pull 5413 files diff a84a355d447d21d71c4f82afbeba65edr647>>>easy
neural network tests print the tests for the mlpclassifier print stuff it is good to test the verbosity but the output should be caught similarly to here https github com scikit learn scikit learn blob master sklearn cluster tests test means py l305>>>easy
sklearn tree tests test tree test huge allocations sometimes hangs on 32 bit needed to be killed in one of two runs on this 32bit vagrant box http files vagrantup com precise32 box>>>blocker bug build_/_ci
rfe doc should mention that the estimator can also have feature importances attribute right now it is mentioned that recursivefeatureelimination works only when the estimator has `coef attribute however that is not the case >>>documentation easy
mrg fix estimators to work if sample weight parameter is pandas series type >>>needs_review
using pandas series for ``sample weights`` leads to an error python from sklearn linear model import ridgecv temp1 pd dataframe np random rand 781 21 temp2 pd series temp1 sum weights pd series np random rand 781 result ridgecv normalize true fit temp1 temp2 sample weight weights python exception traceback most recent call last in weights pd series np random rand 781 result ridgecv normalize true fit temp1 temp2 sample weight weights users jakevdp anaconda envs python3 lib python3 site packages sklearn linear model ridge py in fit self sample weight 868 gcv mode self gcv mode 869 store cv values self store cv values 870 estimator fit sample weight sample weight 871 self alpha estimator alpha 872 if self store cv values users jakevdp anaconda envs python3 lib python3 site packages sklearn linear model ridge py in fit self sample weight 793 else alpha 794 if error 795 out errors weighted alpha qt 796 else 797 out values weighted alpha qt users jakevdp anaconda envs python3 lib python3 site packages sklearn linear model ridge py in errors self alpha qt 685 alpha 686 np dot self diag dot qt 687 diag self decomp diag 688 handle case where is 689 if len shape users jakevdp anaconda envs python3 lib python3 site packages sklearn linear model ridge py in decomp diag self prime 672 def decomp diag self prime 673 compute diagonal of the matrix dot dot diag prime 674 return prime sum axis 675 676 def diag dot self users jakevdp anaconda envs python3 lib python3 site packages pandas core ops py in wrapper left right name na op 618 return left constructor wrap results na op lvalues rvalues 619 index left index name left name 620 dtype dtype 621 return wrapper 622 users jakevdp anaconda envs python3 lib python3 site packages pandas core series py in init self data index dtype name copy fastpath 217 else 218 data sanitize array data index dtype copy 219 raise cast failure true 220 221 data singleblockmanager data index fastpath true users jakevdp anaconda envs python3 lib python3 site packages pandas core series py in sanitize array data index dtype copy raise cast failure 2838 elif subarr ndim 2839 if isinstance data np ndarray 2840 raise exception data must be dimensional 2841 else 2842 subarr asarray tuplesafe data dtype dtype exception data must be dimensional>>>bug need_contributor
mrg fix nearestneighbors algorithm auto to work with all supported metrics by default 4931 >>>needs_review
contributing page should mention existence of common tests the existence of `estimators checks py` and common infrastructure for testing estimators in scikit learn should be mentioned in the contributing page imho ogrisel>>>documentation need_contributor
mrg enh support threshold auto in birch use `estimate bandwidth` from meanshift to provide an optional `threshold auto parameter for birch meanwhile it would be good if we are able to set heuristic for `n samples` so that we avoid calculating the nearest neighbors for all samples >>>needs_review
example of nested cross validation we recently merged change to cross validation generators that allows kfold to be performed within the training of an outer kfold operation this should be illustrated with an example be creative >>>documentation easy
mrg add balanced accuracy score in metrics work on this one 3506 adding balanced accuracy score during the sprint it my first contribution there was already prs on that issue but not completed 4300 3929 3511 implement simple version which works only for binary classification case also add test and doc inputs thanks for the feed back>>>needs_review
mlp docs are messing practical intuitions and simple messages there should be one or two images linking to the pictures of the mlp examples http scikit learn org dev auto examples index html neural networks maybe http scikit learn org dev auto examples index html neural networks and or http scikit learn org dev auto examples neural networks plot mnist filters html the topic box in the example page linking to the examples is not complete only one listed and the link is incorrect there needs big warning sign in red using the warning directive that says that this mlp is not appliable for deep architectures or convnets or any serious neural nets and that points to other packages for that related to 5581 >>>documentation easy need_contributor
example using warmstart in mlp doesn actually use warmstart see file home andy checkout scikit learn doc build html stable modules neural networks supervised html more control with warm start>>>documentation easy
mrg fix 5269 overflow error with sklearn datasets load svmlight fix for this issue https github com scikit learn scikit learn issues 5269 svmlight implementation uses long data type to read and store qid so think it makes sense to do same in scikit learn implementation >>>needs_review
mrg maint reuse unaltered classes functions from model selection also clean up some of the `cross validation` and `grid search` tests note lot more lines can be removed from more redundant tests but feel leaving them as such which will cost extra seconds will ensure that we haven regressed on the old modules to be more clear if we remove redundant test for say `cross val score` and later modify the `model selection` version of the function and test people using the deprecated module will be surprised by that change since we simply import that from `model selection` having the test will make sure that such situation won happen by failing the old tests and forcing us to leave copy of the old implementation at the old module amueller vene jnothman gaelvaroquaux ogrisel reviews >>>needs_review
mrg add how lda lda differs from sckit learn lda closes 5529>>>needs_review
mrg bf not mutating alpha in vbgmm using alpha instead fixes issue 5547>>>needs_review
vbgmm mutates alpha in `` init `` we should probably fix this this means ``set params`` was broken think an easy fix that doesn break backward compatibility would be introduce property `` alpha alpha components`` and then use `` alpha`` instead of ``alpha`` everywhere that would mean behavior stays the same for people not using ``set params`` ``clone`` >>>bug easy
mrg remove complicated equality checks in clone as init shouldn touch anything simplifies the check in ``clone`` that checks if `` init `` and ``get params`` play nicely together previously we could clone objects that copy numpy arrays in `` init `` afterwards we can not doing anything in init is not good idea have an additional check for `` `` because ``pipeline`` is evil working on it note this is fix for 5522 >>>needs_review
mrg added colorblind compatibility for plot examples in calibration reviewed and edited calibration for colorblind compatibility as discussed in 5435 >>>needs_review
remove the lda reference in related packages now that we have it in scikit learn think that we should remove the pointer to lda in http scikit learn org dev related projects html what do people think >>>documentation easy
document when new function class method attribute was introduced for the 18 release sphinx makes it possible to declare when new feature was added using thie `versionadded` directive http sphinx doc org markup para html directive versionadded we should review all the classes functions methods attributes introduced in 17 see the changelog and add versionadded 17` declaration in the docstring for those elements it also good opportunity to add the missing links to the github pr numbers or issue number if no pr when those features where added to the matching entry of `doc whats new rst` similar issue is used to track the work related to the 17 release 5504 >>>documentation easy
document pairwise in the dev docs there is `` pairwise`` attribute that some estimators have to allow cross validation with that is samples samples for example when is precomputed kernel matrix this should be documented somewhere in the dev docs >>>documentation easy
cblas not present but scikit does not add the local copy my installation with pip fails because cblas is not present even though atlas is present and the scipy pip installation worked the error is error command pthread shared build temp linux x86 64 sklearn svm liblinear build temp linux x86 64 sklearn svm src liblinear linear build temp linux x86 64 sklearn svm src liblinear tron usr lib64 atlas lbuild temp linux x86 64 lcblas lm build lib linux x86 64 sklearn svm liblinear cpython 35m x86 64 linux gnu so failed with exit status usr bin ld cannot find lcblas collect2 error ld returned exit status when checking the get info see get info blas opt local workspace master thesis 2015 flickr crawl env lib python3 site packages numpy distutils system info py 635 userwarning specified path is invalid warnings warn specified path is invalid language library dirs usr lib64 atlas define macros have cblas none no atlas info include dirs usr include libraries tatlas which indicates that no atlas info is so scikit learn doesn add its own cblas implementation but have cblas none is indicating that it should changing line 64 in sklearn setup py to fixes the problem have cblas was only added in the latest numpy 10 >>>bug build_/_ci
nan errors in mds test on osx el capitan python am seeing an error when running nosetests on fresh developer build from commit 53bf94cdd686e41195d2ef4d9fbc3c7e83be48c1 my environments are managed with anaconda environment details at the bottom of this message have tried scipy 15 and 16 in python and both presented an error message however the error is not present in python was not able to reproduce the error on different mac with yosemite my machine details are as follows the error the error seems to be related to an interdependency between the ledoitwolf and mds estimators in test common py test non meta estimators when ledoitwolf is removed from the testing list the mds test passes see code below environment versions in that failed the test environment versions in that pass the test >>>bug
countvectorizer with custom analyzer ignores input argument example same for `input file not sure if this should be fixed or just documented don like changing the behavior of the vectorizers yet again >>>bug documentation
estimators should not try to modify and inplace in order to handle readonly memory maps pr 4807 reveals variety of estimators that fails on memory mapped input once we allow check array to process memory map without copying its content in memory estimators failing on read only memory maps the whole pls family factor analysis incremental pca nusvc transformers kernelcenterer maxabsscaler minabsscaler robustscaler standardscaler most of these should be easy to fix mean should be replaced etc >>>bug easy
approximate runtimes for sklearn functions this is not an issue with any library but something that cannot find information that could be helpful for new users would be tables listing the approximate runtime of the different machine learning algorithms depending on the size of the input and the other specified parameters sgd logistic regression svm etc thanks >>>documentation easy
coordinate descent should work on float32 data in addition to float64 data for large scale application constraining `elasticnet` input to be `float64` is waste of space as same quality results should be obtainable with `float32` or event `float16` input use of cython in coordinate descent module make the type flexibility bit tricky but should be doable >>>enhancement
mrg convert to boolean arrays for boolean distances ex jaccard cf https github com scikit learn scikit learn issues 4523 issuecomment 149247925 ping amueller jakevdp >>>needs_review
doc add example for normalization vs standardization for linear models standardize vs normalize in linear models we need an example as explained here https github com scikit learn scikit learn issues 2601 issuecomment 148071040 one can take inspiration from this gist https gist github com gaelvaroquaux 2465c9f3421a393b785f gaelvaroquaux could you add the relevant labels please >>>documentation easy
red green scatter plots colorblindness friendlyness in documentation noticed this while reading the documentation on svms http scikit learn org stable modules svm html but it might also be problem on other pages the plots on that documentation page make heavy use of red and green to separate different types of classes in the scatter plot given that red green is the color combination that is the most common colorblindness consider this very bad choice >>>documentation easy
remove deprecated stuff that will no longer be supported by 18 best way to find deprecated stuff is to do `git grep 18 and see where it says will be removed in 18 or will no longer be supported from 18 or something similar to that and explore the file where that line is found once you have removed the deprecated stuff please commit your work and send pr similar to 5437 with the issue description that it partly fixes 5434 the mrg at the start of the pr indicates that the pr is ready for review or if you are not done working with it fully you could use wip zermelozf 5451 `threshold attribute in the `outlierdetectionmixin` at file `covariance outlier detection py` support for `class weight auto in `utils class weight py` `pooling func` in `transform` of `agglomerationtransform` in file `cluster feature agglomeration py` `random state` parameter in `cluster dbscan py` `n components` in `cluster hierarchical py` `max iterations` in `cluster mean shift py` the `allow lists` `allow nd` and `dtype` options in `train test split` of `cross validation py` remove support for `class weight subsample in `ensemble forest py` `estimator params` in `sklearn feature selection rfe py` `precompute` in `sklearn linear models coordinate descent py` `fixed vocabulary` property in `feature extraction text py` lot more aabadie deprecated code in metrics rvraghav93 5437 5469 5528 helper functions in `multiclass py` remove support for uppercase values for loss in sv test in file `svm tests test svm py` uppercase support in fit liblinear` function in `svm base py` refer 4261 and 4260 also svm classes py l192 remove support for `gamma 0` `svm base py` change the default value of `decision function shape` to ovr `svm base py` `fixed vocabulary removing deprecations fully from metrics left over stuff >>>easy
mrg affinity prop fix commit changes defaults to address issues in 5340 commit issue warning if affinity propagation doesn converge error if fit is called after >>>needs_review
pca fit transform changes array order pca fit transform apparently changes order arrays into order arrays import numpy as np from sklearn decomposition import pca np random uniform size 10 10 print flags contiguous pca fit transform print flags contiguous prints `true false` >>>documentation easy
mrg elkans means it is just minor modifications to amueller pr 2008 >>>needs_review
add multi label classification task add dataset with multi label data refer 5105 preferably the emotions dataset http www eecs qmul ac uk mmv datasets deap >>>easy
wip labelkfold balance folds without sorting this changes labelkfold so that the original or shuffled order of samples is reflected in the folds instead of sorting the labels by frequency balance is achieved just by looking at the smallest fold at each iteration this means shuffling has an effect beyond tie breaking and the order of samples can be used as simple way of achieving stratification closes 5390 see also 5300>>>bug
labelkfold shuffling and preserving original order currently labelkfold sorts samples by weight so as to create balanced folds in terms of size however this is at odds with shuffling moreover the regular `kfold` partitions the samples without changing their order which is useful if the order is meaningful or reflects stratification my suggestion is to make the sorting for balanced fold sizes in `labelkfold` optional so that shuffling or the original order of samples is honored by keeping the labels in the order in which they are first encountered >>>bug
memory leak in gbm implementation when warm start is set think there is memory leak or other unnecessary data copying going on when warm start is used for gbm here is minimal program which reproduces the issue >>>bug moderate need_contributor
add common tests for sample and feature estimators should either set ``min samples`` and ``min features`` or work testing in ``check fit2d 1feature`` and ``check fit2d 1sample`` for good error message is probably the way to go >>>moderate
kernel means several people including nellev seem to be interested in merging my kernel means code in scikit learn https gist github com mblondel 6230787 am open to it if someone wants to work on it this will of course need docs tests and examples note that this implementations requires the kernel matrix to fit in memory but this also the case of kernel pca and kernel ridge in scikit learn >>>new_feature
request more criterion for random forest regression current random forest regressor only support for mse can more criterions such as mean square of percentage error can be support by scikit learn >>>enhancement moderate
ridgecv and ridge produce different results when fitted with sample weight import numpy as np from sklearn linear model import ridgecv ridge from sklearn datasets import load boston from sklearn preprocessing import scale boston scale load boston data target load boston target alphas np linspace 200 weight np logspace len target print ridgecv eigen fit0 ridgecv alphas alphas store cv values true gcv mode eigen fit boston target sample weight weight print alpha fit0 alpha print cv fit0 cv values print coef fit0 coef print ridgecv svd fit1 ridgecv alphas alphas store cv values true gcv mode svd fit boston target sample weight weight print alpha fit1 alpha print cv fit1 cv values print coef fit1 coef print ridge fit2 ridge fit0 alpha fit boston target sample weight weight print coef fit2 coef gives if `sample weight` is `none` all three models give the same coefficients >>>bug
mrg fix precomputation of gram matrix in lars this is fix for the bug reported in 1856 the parameter `precompute` in randomizedlasso lars larslasso larscv larslassocv and larslassoic were not consistent and some values proposed in the docstrings false array could raise errors this pr includes the following steps improve `lars path` to allow `gram true` and `gram false` change the get gram` method to handle `precompute` in the same way in every class add warning when an array is given in `precompute` in larscv and larslassocv update the docstring add some tests `test randomized l1 py` is also updated to be shorter from 582s to 817s `test least angle py` goes from 305s to 465s >>>needs_review
mrg maint center data for linear models fixes 2601 no more to do update the docstring saying that we do normalization to reduce surprise clean and factor `center data` and `sparse center data` into new private function and deprecate them add necessary deprecation update docs review all the todo` and xxx` and not to do see 2601 and discussion below deprecate `normalize` signature and property and introduce `standardize` behaviour introduce the same behaviour for every class calling `center data` internally related tests make sense of this old test https github com scikit learn scikit learn blob master sklearn linear model tests test base py l63 on `sample weights` with unused variables done in https github com scikit learn scikit learn pull 5526 change `center data` behaviour `fit intercept` currently input data is not touched https github com scikit learn scikit learn blob master sklearn linear model base py l119 if we not are fitting the intercept see separate issue 5799 3459513455 d1288a14b9 https cloud githubusercontent com assets 2871319 10516143 1afb092a 7357 11e5 81e9 a2835af3aacf jpeg >>>needs_review
rfe rfecv docstring should say estimator can supply feature importances not just coef >>>documentation easy
mrg add fast replacement for np cov this is inspired by some recent prs over at numpy concerning `np cov` performance timings with numpy and atlas np random rand 13000 60 timeit np cov loops best of 82 per loop timeit fast cov loops best of 71 per loop memory use is also halved compared to numpy 10 at least for `n samples` `n features` numpy 11 will have more memory efficient `cov` implementation >>>needs_review
custom warnings filter for deprecations it would be good to have custom ignore deprecation warnings that would accept tag parameter and would ignore the warnings only of certain release this will enable us to quickly filter and modify tests after the release while making sure other warnings are not ignored >>>api
mrg pairwise distances outputs nan and negative values fixes 4475 the problem is about pairwise distances and not sne refer to the issue for discussion 4495 deals with the same issue but it does not seem active any more tests for problems related with negative values now pass regarding `nan` am going to see if fix on the scipy side is possible until then have written wrappers of the scipy functions changes added breaking tests for `tsne` as from 4475 but covering all distances from `sklearn metrics pairwise pairwise distances` added breaking tests for the same `pairwise distances` implemented robust sklearn version of `correlation` almost the same as `cosine` wrote wrappers for scipy yule dice sokalsneath >>>needs_review
sklearn cluster agglomerativeclustering can we do without completing the matrix userwarning the number of connected components of the connectivity matrix is completing it to avoid stopping the tree early have tried this both on the latest 16 version and on the latest bleeding edge version of sklearn 17 dev0 and this appears to be an issue in both use `sklearn cluster agglomerativeclustering affinity precomputed connectivity cmat linkage complete where cmat is connectivity matrix in which there are disconnected components as indicated by the source code get the error message userwarning the number of connected components of the connectivity matrix is completing it to avoid stopping the tree early however reading the source code see that when completing the connectivity matrix the developers are wondering whether the clustering can take place without completing the matrix xxx can we do without completing the matrix am interested exactly in this development do you think sklearn is planning to fix this and make it possible to do the clustering without completing the matrix think it should not be too hard best zhana>>>bug
mrg optional verbosity for pipeline adds named verbosity argument in pipeline constructor after each step verbose pipelines print to standard output lines like pipeline fit or transform 5298>>>needs_review
types in dataset bunches are inconsistent just saw that iris feature names is list but iris target names is an array that is odd not sure how it is for the other datasets this is pretty minor but some consistency would be nice >>>easy need_contributor
make lars and lassolars parallel on the multiple targets lars and lassolars in least angle py could have the loop on the number of targets made parallel using joblib parallel as everywhere in the codebase >>>easy enhancement
mrg check should copy also if copy is set to be true is it acceptable to expect also to be copied when `copy` is set to true this makes sure that is not modified when is centered inplace hit this bug here https github com scikit learn scikit learn pull 5291 files diff 7416ccedd45a5840c67ff7877d24e1cer52 also added test case that fails in master >>>needs_review
mrg collapsing pca and randomizedpca fixes 5243 and 3930 to do collapse the two classes old tests passing integrate `svd solver arpack benchmark the solvers and establish the best `auto` policy fix docstrings uniform with `incrementalpca` by inheritance from basepca` see 3285 add `flip sign` param true by default we flip by default without introducing param for controlling the behaviour arpack backport updated>>>needs_review
verbose option on pipeline would like to see timing information for individual steps in pipeline perhaps named variable for verbose in the pipeline constructor >>>easy enhancement
mrg add scaling to alpha regularization parameter in nmf separated this modification from 4852 for further discussion about it in nmf scaled here the regularization parameter `alpha` with `n samples` and `n features` indeed without scaling two problems appear the regularization penalizes the sum of coefficients in and if the number of element in and is not the same `n samples features` the constraint is unbalanced one of or goes to zero and the other one which is less penalized increases to compensate the value of alpha that makes the coefficients of and collapse is proportional with `sqrt features samples it makes the scaling of alpha depends on the size of the data test to prove the point tested several sizes for the input `x` and plotted how the coefficients in and collapse with respect to the `alpha` parameter without scaling alpha noregul https cloud githubusercontent com assets 11065596 10025176 31ef9e1a 615b 11e5 86b5 70f42ec45442 png with properly scaling alpha regul https cloud githubusercontent com assets 11065596 10025177 31f14080 615b 11e5 90b3 a26a751ce7a6 png scaling used used `alpha alpha features` and `alpha alpha samples` conclusion the effect of the `alpha` parameter is much more consistent if we scale it as l1 and l2 regularizations in nmf is fresh new 4852 it would not really break any code before 17 at least but do we want to add this is it consistent with other estimators in scikit learn what do you think vene mblondel >>>needs_review
mrg add huber estimator to sklearn linear models add robust regression model that filters outliers based on http statweb stanford edu owen reports hhu pdf add fix for random overflowerrors add documentation to the helper function add extensive testing add narrative docs add example support for sparse data support sample weights>>>needs_review
remove files from the repo we should only generate files for releases and not distribute them with the repo as scipy does it pretty easy to find out whether you are doing an dev build and cythonize or just build the files that are shipped with the release >>>build_/_ci enhancement
mrg implement fabia biclustering algorithm this pr adds the fabia biclustering algorithm to sklearn fabia is general biclustering algorithm based on matrix factorization had discussed this addition with kemal eren who implemented the rest of the biclustering package during the gsoc 2013 way back in 2013 as far as know his mentors iirc gaelvaroquaux welcomed the idea back then ve had the code lying around in 2476 but never got around to polish it until now if there is still interest in the addition here it is notes also have cython version that is 20 200 faster the larger the amount of biclusters to detect the smaller the speedup in realistic settings it will be close to 20 however that implementation is based around the `cython lapack` module that comes with scipy 16 without that it would require pulling large ish amount of lapack into sklearn didn think that was worth it >>>needs_review
mrg one hot encoder now errors on any unknown categorical feature see the new test case added which fails on the current master >>>needs_review
overflow error with sklearn datasets load svmlight file scikit learn version 16 os yosemite 10 10 ve created svmlight file with only one line from pandas dataframe when open the file in an editor the result looks like this qid 72048431380967004 1440446648 72048431380967004 236784985 when try to load the file with query id true get an overflow error overflowerror signed integer is greater than maximum if load the file with query id false there appears no error message but the value for the query id is wrong this is the output 1440446648 72048431380967008 236784985 72048431380967004 appears now as 72048431380967008 how do avoid this error the maximum value of np uint64 is 9223372036854775807 so there should be no overflow error have tried to load with np int64 as data type too but the output is the same >>>bug need_contributor
add scaling to sgdclassifier sgdclassifier only really works well with scaled data think we should add some scaling to it by default >>>enhancement
doc further documentation building tweaks this extends 5195 to update documentation on building documentation and to make `optipng` execution more economical from within `doc makefile` >>>documentation
don hard code kernel parameters in nystroem the current nystroem kernel approximation interface is bad as it assumes some default parameters for certain kernels it should get dictionary of kernel args instead think see 5211 this needs deprecation >>>easy enhancement need_contributor
add svc documentation for properties issue https github com scikit learn scikit learn issues 4687 documented `fit status `proba and `probb >>>documentation easy
mrg support arbitrary init estimators for gradient boosting in 2691 it was brought up that passing an estimator as base class to gradient boosting classifiers or regressors would cause it to crash due to various shape issues on the main issue being that it was expected that the predictions of the base estimator should be of shape samples classes for multinomial classification or samples for binary classification and regression this pr solves this issue by handling each case separately if the initialization estimator is one of those already in `gradient boosting py` it uses those predictions as normal however if classifier was passed in it will check to see if it has `predict proba` method and use that if possible collapsing into the log odds if only two classes if the classifier does not have `predict proba` method it will use the `predict` method and hot encode that into matrix if this needs to be collapsed because there are only two classes it adds small epsilon to the matrix before calculating the log odds if regressor is passed in then the predictions are just reshaped to make sense also reordered the code little bit for it to be more organized and added two unit tests to make sure that it works it now works with arbitrary estimators as long as they take sample weights into their fit method and the unit test includes tests on support vector machines and ridge regression initializations ping ogrisel agramfort glouppe pprett >>>needs_review
rfc tree module improvements am planning on submitting several prs in an attempt to merge 5041 in slowly with the ultimate goal being clean implementation of multithreaded decision tree building so that gradient boosting can be faster with one of the main concepts merged 5203 here is list of separate prs which like to merge in the near future reorganize tree pyx into several files see pr 5230 merged add proxy impurity improvement methods to both gini and entropy see pr 5233 closed reevaluate constant feature caching closed support sparse data for gradient boosting see pr 5252 add caching of computation between different split levels to avoid recomputation ensure feature importance converge in ensemble see pr 5261 add tests to ensure the correctness of impurity values wrt hand computed values on toy data longer range goals which like to work towards but have no clear plan as of right now are the following add an approximate splitter add multithreading support for single decision trees add partial fit method for tree building support categorical variables support missing values at this point it will be clearer to me what specific changes to splitter criteria and treebuilder need to be added to make multithreading possibility glouppe arjoly gaelvaroquaux pprett if you have any comments love to hear them >>>enhancement moderate
gaussian process based hyper parameter optimizer following discussion on issue 474 https github com scikit learn scikit learn issues 474 this pr aims at implementing gp based hyper parameter optimizer this is based on sklearn gaussian process http scikit learn org stable modules generated sklearn gaussian process gaussianprocess html sklearn gaussian process gaussianprocess and randomized search optimizer http scikit learn org stable modules generated sklearn grid search randomizedsearchcv html implementations given budget number of iterations at each step model through gp sample randomly candidates within the hyper parameter space compute the value of the acquisition function for each candidate thanks to the gp model where the acquisition function could be the expected improvement and the upper confidence bound select the candidate that maximizes the acquistion function as the next point to test current acquisition functions implemented are the expected improvement ei and the upper confidence bound ucb examples are provided in `examples model selection gp search py` results obtained with simple pipeline on the iris dataset http scikit learn org stable auto examples datasets plot iris dataset html comparison random based in green vs gp based in blue the 20 first iterations are random iris results https cloud githubusercontent com assets 7746635 9564205 bdc8193c 4e9e 11e5 9de0 938be3aec166 png >>>moderate new_feature
unexpected memoryerror from incrementalpca used with memmap prompted by question asked in stackoverflow chat http chat stackoverflow com transcript message 25344627 25344627 investigated why user would encounter memoryerror when using the following minimal code ut np memmap my array mmap dtype np float16 mode shape 140000 3504 clf incrementalpca copy false train clf fit transform ut found that the memoryerror was called by this call to `check array` https github com scikit learn scikit learn blob master sklearn decomposition incremental pca py l167 check array dtype np float the problem is with adding the `dtype np float` to the call to `check array` this means that the `copy false` default of `check array` is ignored because the `dtype` has changed in that call to `np array and the docs state that change of `dtype` will force copy irrespective of the `copy` argument made simple gist demonstrating the issue here https gist github com rsnape bd1f30db4b789a5f7665 propose that the line above could be changed to check array but am not expert enough in this tool to understand whether that might have negative consequences further along the chain of execution if that is good solution happy to submit pr >>>bug
dense svm and zeroed weight for samples of entire class this bug appears in current master and for any dense svm class output here we see that svmlib internally have lost 2nd class at the same time sklean wrapper class keeps all class labels inside that why predict proba returns matrix of shape samples instead of sample what is expected by bagging classifier implementation understand that it insane usage of weights by itself but together with bagging and dataset with many labels bagging randomly zeroes complete classes and this bug shows itself because bagging expects that svm return probability of classes which they hold all classes investigated this little bit and can try to fix this if someone will say that all this usage with bagging makes sense because don really sure about this >>>bug
lda predict proba should use softmax it uses an ovr normalization for multi class for some unknown to me reason see 5134 >>>bug
pca score is log density right the docstring says it log likelihood same for score samples kde correctly says log density xuewei4d gmm also now says log density right >>>documentation easy need_contributor
mrg added predict proba functionality to cross val predict method modified the call signature of cross val predict to accept proba keyword argument and fit and predict to require proba boolean see signatures below when set to true predict proba is called on the estimator and predict when false default value is false to preserve existing functionality cross val predict estimator none cv none jobs proba false verbose fit params none pre dispatch jobs fit and predict estimator train test verbose fit params proba >>>needs_review
wip text vectorizers memory usage hi the other day ve tried to vectorize some big text data and wondered why sklean would use that much memory so looked into it and think ve found some points for testing purposes used the enron data set http www cs cmu edu enron enron data set to reproduce my results without having to modify sklearn you can use this little script https gist github com ephes 90193567b8c0501b13ef enron test script now including the vectorizer from 4968 just unpack the enron dataset and start the script with here little table showing the results countvectorizer and hashingvectorizer are the unmodified vectorizers from sklearn leancountvectorizer and leanhashingvectorizer are the new modified versions aupiffcountvectorizer is the modified version from 4968 inheriting from leancountvectorizer from countvectorizer in parantheses fastaupiffcountvectorizer uses if else instead of get in inner loop and keys and values insted of iteritems an explicit del feature counter after adding new doc vectorizer after 300k after 510k peak time countvectorizer 43 gb 84 gb 14 gb 237 24s leancountvectorizer 08 gb 41 gb 87 gb 268 19s aupiffcountvectorizer 41 gb gb 86 gb gb 287 43s 280 02s fastaupiffcountvectorizer 27 gb 43 gb 69 gb 253 92s hashingvectorizer 46 gb 75 gb 82 gb 243 02s leanhashingvectorizer 28 gb 55 gb 64 gb 232 62s why is the default countvectorizer using gb memory to produce feature matrix which is only 15 gb in size when dumped to disc via joblib maybe it the vocabulary let see well it not the vocabulary it the feature matrix and if using gb is not bad enough either sum duplicates or sp csr matrix are copying some data because saw 25 gb memory usage even with an sys exit at the end of count vocab in the leancountvectorizer iterate chunk wise over the documents create temporary feature matrix for each chunk and append this temporary feature matrix then to the final feature matrix in place so there never this big values array and the data gets also copied but only for the small chunk based feature matrices after count vocab returned the feature matrix the method sort features is called although it modifies the vocabulary in place as stated in the docstring it does not modify the feature matrix in place but makes copy via fancy indexing return map index so changed the sort features method for leancountvectorizer to modify the feature matrix in place this operation is rather slow and the main reason why leancountvectorizer is slower than countvectorizer maybe there better way to swap the columns of the feature matrix and finally the limit features method also makes full copy of the feature matrix via fancy indexing because it high and low arguments are set to numerical values per default so the if high is none and condition is always false in leancountvectorizer this is also changed the leanhashingvectorizer has only the iterate in chunks over documents modification and since the values array would not get as big it uses less memory best regards jochen>>>need_contributor
gridsearchcv freezes indefinitely with multithreading enabled jobs ve been intermittently running into this issue in the subject with gridsearchcv over year now across python and two jobs several different mac osx platforms laptops and many different versions of numpy and scikit learn keep them updated pretty well ve tried all of these suggestions and none of them always work https github com scikit learn scikit learn issues 3605 setting multiprocessing start method to forkserver https github com scikit learn scikit learn issues 2889 having issues only when custom scoring functions are passed ve absolutely had this problem where the same gridsearchcv calls with jobs freeze with custom scorer but do just fine without one https github com joblib joblib issues 138 setting environment variables from mkl thread counts have tried this when running numpy sklearn built against mkl from an anaconda distribution scaling inputs and making sure there are no errors with jobs completely sure that the things trying to do on multiple threads run correctly on one thread and in small amount of time it very frustrating problem that always seems to pop back up right when confident it gone and the only workaround that works 100 of the time for me is going to the source for gridsearchcv in whatever sklearn distribution on an manually changing the backend set in the call to paralell to threading instead of multiprocessing haven benchmarked the difference between that hack and setting jobs but would there be any reason to expect any gains with the threading backend over no parallelization at all certainly it wouldn be as good as multiprocessing but at least it more stable btw the most recent versions ve had the same problem on are mac os 10 python continuum analytics inc scikit learn 16 scipy 16 numpy pandas 16 joblib 4>>>bug
bug in cross val score python from sklearn datasets import make regression from sklearn cross validation import cross val score leaveoneout from sklearn linear model import ridge coef make regression random state 42 noise samples 200 coef true cross val score ridge cv leaveoneout len >>>api bug
according to description of epsilon above default value must be 1>>>documentation
as user want function similar cross val predict which instead returns probabilities intro cross val predict is useful because it returns the predicted targets opposed to cross val score and hides the parallelization logic behind simple interface there should be similar function for predicting the probability distribution predict proba over classes discussion would this be welcomed new feature should this feature be flag for cross val predict probability true should cross val predict then return both the classes and probabilities or only probabilities should this feature be its own function cross val predict proba>>>new_feature
setting search parameters on estimators the underscore notation for specifying grid search parameters is unwieldy because adding layer of indirection in the model `pipeline` wrapping an estimator you want to search parameters on means prefixing all corresponding parameters we should be able to specify parameter searches using the estimator instances the interface proposed by amueller at https github com scikit learn scikit learn issues 4949 issuecomment 127289568 and elsewhere suggests syntax like calling `search params` would presumably set an instance attribute on the estimator to record the search information questions of fine semantics that need to be clarified for this approach include does call to `search params` overwrite all previous settings for that estimator does `clone` maintain the prior `search params` should this affect the search space of specialised cv objects `lassocv` questions of functionality include is `randomizedsearchcv` supported by merely making one of the search spaces `scipy stats` rv making some searches `gridsearchcv` incompatible is there any way to support multiple grids as is currently allowed in `gridsearchcv` have proposed an alternative syntax that still avoids problems with underscore notation and does not have the above issues but is less user friendly than the syntax above here parameters are specified as pair of estimator parameter name but they are constructed directly as grid and passed to `gridsearchcv` `randomizedsearchcv`>>>api
polymorphic clone `sklearn base clone` is defined to reconstruct an object of the argument type with its constructor parameters from `get params deep false recursively cloned there are cases where think the one obvious way to provide an api entails allowing polymorphic overriding of clone behaviour in particular my longstanding implementation https github com jnothman scikit learn tree remember of wrappers for memoized and frozen estimators relies on this and would like to have that library of utilities not depend on change to `sklearn base` so we need to patch the latter let me try to explain let say we want way to freeze model that is cloning it should not flush its fit attributes and calling `fit` again should not affect it syntax like the following seems far and away the clearest it should be obvious that the standard definition of `clone` won make this operate very easily we need to keep more than will be returned by `get params` unless `myestimator dict becomes param of the `freeze model` instance which is pretty hacky alternative syntax could be class decoration `freeze model myestimator or mixin `class myfrozenestimator myestimator frozenmodel pass` such that the first call to `fit` then sets frozen model these are not only uglier but encounter the same problems ideally this sort of estimator wrapper should pass through set get params` of the wrapped estimator without adding underscored prefixes not that this is so pertinent for frozen model but for other applications of similar wrappers it should also delegate all attributes to the wrapped estimator without making mess of `freeze model init this is also not possible imo without redefining `clone` so can we agree that it would not be bad thing to allow polymporphism in cloning on name for the polymorphic clone method `clone` or `clone params` or `sklearn clone` >>>api enhancement
docstring for sgdclassifier and sgdregressor are misaligned some docstring for sgdclassifier some docstring for sgdregressor ll post my pull request here soon >>>documentation
selectkbest default and parameter name the default parameter of features to select in selectkbest is 10 this is not reasonable default maybe it should be 10 or 50 but 10 leads to it crashing when there are less then 10 features and it rarely seems reasonable also the name is ``k`` usually we don like single letter names maybe it should be ``n selected`` don have good idea really >>>enhancement
mrg deprecate iter in sgdclassifier and implement max iter solve 5022 in sgdclassifier sgdregressor perceptron passiveagressive deprecate `n iter` default is now `none` if not none it warns and sets `max iter iter` and `tol 0` to have exact previous behavior implement `max iter` and `tol` the stopping criterion in `sgd fast plain sgd is identical to the one in sag new solver for ridge and logisticregression add `self iter after the fit for multiclass classifiers we keep the maximum `n iter over all binary ova fits >>>needs_review
more intuitive scoring argument for loss and error using the grid search meta estimator with the mean square error the mean absolute error the median absolute error or the log loss as scoring parameters leads to the negation of those metrics this is confusing especially for new users suggest that we prefix those strings by neg or negative this would make clear from the start that the score is obtained from the negation of the loss error >>>api
our rand is duplicated and should not be seeded with `our rand r` is fast and simple random generator defined in `sklearn tree tree pyx` problems the functions only return 0s if seeded with we need to audit the use of this function and make sure it is never seeded with the code is duplicated `sklearn linear model cd fast pyx` and will be in 4738 in 4873 and possibly other pr my first attempt to re use only one code by moving the inline function in tree pxd file has failed every tests passed on linux travis but it broke 45 tests on windows appveyor did not figure out why >>>enhancement moderate
add table dictionary for good parameter values feel we should add good grids to search over for all estimators both in the docs and as dict think good source would be https github com automl auto sklearn and possibly carret >>>documentation
huber loss regression in linear models just realized we have huber loss for sgd but we don have nice out of the box solver for huber loss is there any reason for that should we add huberregressor or add loss to ridge linearregression >>>new_feature
mrg metric precision at score this pull request implement precision at score for multilabel classification >>>needs_review
mrg add grouped option to scaler classes as per discussion in issue 4892 add per feature option to the scaler classes when false defaults to the previous behavior of true this modifies behavior such that scaling is based on consideration of the entire data array at once instead of one feature at time also allow axis none in addition to axis or axis in the standalone scaling functions this pr includes tweaks to functions in the sparsefuncs module where it makes axis none behavior easier to code todo add per feature option to robustscaler add axis none option to `preprocessing data scale` add axis none option to `preprocessing data maxabs scale` add axis none option to `preprocessing data robust scale`>>>needs_review
mrg added average option to passive aggressive classifier regressor differently from the sgdclassifier and sgdregressor the passiveaggressiveclassifier and passivieaggressiveregressor does not expose the average option of the basesgd class the average option helps smoothing out the impact of rarely observed variables for example when used in combination with the hashingvectorizer on text it helps to compensate for the lack of idf information lowering the impact low idf rare features the following is an example of using averaging both on sgdclassifier and passiveaggressiveclassifier the data is the bo pang movie review dataset https www cs cornell edu people pabo movie review data review polarity tar gz the results show that averaging in basesgd works perfectly for both classifiers to remove noise features averagedemo https cloud githubusercontent com assets 6543521 8570575 8e7df796 2581 11e5 8fa0 61000a12b73f png the following is the code to replicate the experiment that generated the above plot >>>needs_review
mrg fixes 4577 adds interpolation to pr curve fixes 4577 added boolean parameter interpolated to sklearn metrics precision recall curve returns an interpolated de noised precision score if true>>>needs_review
onehotencoder should accept string values how hard would it be to hash incoming string values or use the labelencoder internally be willing to contribute some code for this if there are no outstanding design blockers cc jnothman >>>new_feature
random failure on sklearn linear model tests test ridge test class weight vs sample weight under windows the following seems to happen randomly on appveyor at least under 32 bit python >>>bug
add documentation on class weight and sample weight think these should be documented in the quick start guide http scikit learn org dev tutorial basic tutorial html>>>documentation easy
perf use joblib parallel backend threading for sparse encode when called with algorithm lasso cd since we released the gil in most of the cython coordinate descent solver 3102 we could now sparse encode in parallel efficiently with threads when using that solver making this change in the code of `sparse encode` should be straightforward and the tests should stay the same but accepting pr for that will require running some benchmarks to check that switching to the threading backend improves memory usage reduces scheduling overhead and therefore should slightly improve overall sparse encoding speed note using threading for the lars solver might not be efficiently parallelizable with threads the lars solver is primarily written in python numpy although we should check as numpy releases the gil often >>>enhancement
sklearn preprocessing minmaxscaler not preserving symmetry add axis none minmaxscaler does not preserve symmetry scikit learn 15 and scikit learn 16 windows sp 64 bit python 32 bit an affected numpy matrix and the script to reproduce the problem are available at https www dropbox com vkcuq71wa69jrw7 sklearn bug tar dl >>>documentation easy
improve featurehasher compatibility with dictvectorizer `featurehasher` does not currently support string values for its dict input format see 4878 ideally its input format should be compatible with `dictvectorizer` one hot encoding any string valued items >>>easy need_contributor
mrg misc add alternate compact logo for ads merchandise this is more compact form for the logo that feel is better suited for print mugs signs etc>>>needs_review
add contingency matrix to reference and user guide fix 4805>>>documentation easy
has fit parameter will not work with kwargs bagging and boosting use `sklearn utils validation has fit parameter` to produce clear error when `sample weight is not none` and the base estimator does not support `sample weight` however the method of checking the argspec will not work if `sample weight` is supported as kwargs` in `fit` thus raising an exception when none applies similar concern exists for the proposed alternative to `sample weight` as keyword arg 4696 >>>bug
document and adapt isclose usage `sklearn metrics` introduces `isclose in https github com scikit learn scikit learn blob master sklearn metrics ranking py which can leave the unaware data practitioner with hours of debugging in very unbalanced classification probabilities scores can be very small and yet meaningful this however will cause unexpected missing precision recall points due to `isclose` treating values within 10e as equal suggest to place warning about `isclose` in the documentation and also replace the absolute epsilon by relative closeness comparison in order to avoid the problems with small probabilities in unbalanced classification >>>documentation
sparsepca is online in the direction of features presentlyu sparsepca used `dict learning` or `dict learning online` on `x t` and sets its `components to be the outputted code of these functions which is sparse though it is simple way to reuse code from `dict learning py` it has for drawback that it does not allow the user to learn in the sample direction which prevents the use of `partial fit` and is not adapted in setting where data is streamed solution for this would be to use another formulation for sparse pca found in which enforces an elastic net constraint for and thus implies projecting on the elastic net ball instead of the l2 ball as it is done today efficient projection algorithms exists as described in mairal bach ponce sapiro 2010 online learning for matrix factorization and sparse coding the journal of machine learning research 11 19 60 rodola torsello harada kuniyoshi cremers 2013 december elastic net constraints for shape matching in computer vision iccv 2013 ieee international conference on pp 1169 1176 ieee >>>new_feature
mrg multioutput bagging this pull request brings multi output support 3449 to the bagging meta estimators it different of https github com scikit learn scikit learn issues 3449 since the implementation to make the averaging is shared for single output and multi output data haven implemented multi output decision function as no base estimator currently support this >>>needs_review
mrg add knn strategy for imputation trying to solve 2989 imputation strategy is following find all rows with full features as complete set and impute the missing features in row by taking the mean of those features among its nearest neighbors in complete set use scikit learn neighbors nearestneighbors to find neighbors now it can only handle dense matrix based on this paper http web stanford edu hastie papers missing pdf similar package for impute http www bioconductor org packages release bioc manuals impute man impute pdf similar function for matlab knnimpute http www mathworks com help bioinfo ref knnimpute html still working on documentation and examples >>>needs_review
rfc api objects for out of core fitting partial fit pipelines gaelvaroquaux and me recently discussed better support for out of core processing this issue is to collect comments and summarize discussions to me the two core issues are representing streamed datasets and out of core pipelines out of core pipelines should not be that hard if we only support stateless transformers this was discussed previously though don remember where it would also be possible with transformers implementing ``partial fit`` by going over the dataset multiple times that is not possible in streaming infinite dataset setting but would work for reading from disk for representing streamed datasets think gaelvaroquaux proposed conductor object that handles the stream and passes it to the estimator another approach he mentioned is creating transformer that reads files form disk that would work if the data is already in batches on disk by transforming file names to samples if we can change the number of samples that might even work with single file it would be cool to read csvs in chunks with pandas not sure we should prototype this in scikit learn but think we should at least discuss it here some libraries that work on similar concepts are rosetta http pythonhosted org rosetta examples fuel https github com mila udem fuel and pescador https github com bmcfee pescador>>>api
implementing stacking and other ensemble techniques don find open issue for this just wondering what the plan to implement them in sklearn is there roadmap some of the techniques are listed in wiki http en wikipedia org wiki ensemble learning>>>new_feature
mrg read only input data in common tests following pr 4775 added checks in estimator checks in order to verify `estimator` behavior on read only mem mapped data few issues there registering clean temp memory with atexit yield failure as it called at the end of every test and delete temp memory whereas the next test has already begun overloaded make blobs into make blobs in order to be able to easily yield read only memmap which is only positive this looks quite messy though cf pr 4775 this does not introduce tests that fails on current master whereas `sklearn linear model cd fast pyx` still raise errors on some use cases this is related to the fact that we cannot make lasso fails using simple read only memmap as input >>>needs_review
contingency matrix missing from references and user guide metrics cluster contingency matrix is public and should therefore be in the references and the user guide or be made private >>>documentation easy
brier loss has no real explanation in the narrative docs it is mentioned in the calibration docs but not in the metrics docs and it is not really explained >>>documentation easy
eigenvalue in kernel pca if follow strictly the formulation of kpca as in schoelkopf et al 1999 the eigenvalues of kernel matrix should be scaled by the number of examples first then we can normalize the eigenvectors by sqrt lambda think by this way you get the interpretation and relations to the non linear feature space do you think this is problem >>>question
wip metric learning nca gsoc 2015 project iteration neighborhood component analysis for details refer to my gsoc blog especially post on nca http barmaley exe blogspot ru 2015 05 nca html >>>needs_review
change graph lasso to exploit block diagonal structure took stab at implementing the optimization described here http faculty washington edu dwitten papers jcgs 2011 pdf the block diagonal structure of the graphical lasso solution can be identified by thresholding the sample covariance and the exact solution is found by solving the graphical lasso for each block separately the authors find that there is huge speedup when the solution is very sparse when the solution is mostly dense the results are basically the same or very slightly slower due to the extra thresholding step this modification was made in the `glasso` package some time ago timing results are given in the paper above but also ran test of my implementation with 1000 100 and block diagonal population covariance matrix couple of questions the `glasso` package was changed to only use this algorithm so followed the same convention and did not allow the user to choose whether to perform the block diagonal screening procedure it would be very easy to add this just not sure if there case where it would ever be desired there bug in our connected components function that was fixed while ago in scipy https github com scipy scipy pull 3819 included this in my commit but maybe it should be separate pull request does the overall logic make sense here only added couple of comments but if it not clear what going on then can try to clarify >>>enhancement
svd and eigen shouldn yield such different results for ridgecv import numpy as np from sklearn linear model import ridgecv from sklearn datasets import load boston from sklearn preprocessing import scale boston scale load boston data target load boston target alphas np linspace 200 fit0 ridgecv alphas alphas store cv values true gcv mode eigen fit boston target fit0 alpha 0816326530612246 fit0 cv values array 37 65055379 38 25669302 38 99731156 39 51049034 39 85507581 fit1 ridgecv alphas alphas store cv values true gcv mode svd fit boston target fit1 alpha fit1 cv values array nan 38 25669302 38 99731156 39 51049034 39 85507581 the problem here appears to be that `gcv mode svd produces `nan` for `alpha 0` the ridge regression docs http scikit learn org stable modules linear model html ridge regression suggests as valid value of alpha of course corresponding to the unregularized regression seems like solution would be either change computation of cv values under svd to produce value warn or change user docs to discourage using alpha under this case >>>bug
dictionary learning is slower with jobs setting jobs in minibatchdictionarylearning and in function dictionary learning online leads to worse performance multi processing is handled in sklearn decompositions function dict learning 249 minimal example https gist github com arthurmensch 091d16c135f4a3ba5580 output jobs output jobs output jobs we can see that transform function of minibatchdictionarylearning relying on sparse encode function benefits from multi processing as expected dictionary learning relies on successive calls of sparse encode function slowness may come from this >>>bug
prediction interval for random forests hi there is there any plan to add prediction interval range prediction to the randomforecastregressor library think that will be super useful and necessary addition thanks chen>>>enhancement moderate
extend stratifiedkfold to float for regression it is important to stratify the samples according to for cross validation in regression models otherwise you might possibly get totally different ranges of in training and validation sets however current `stratifiedkfold` doesn allow float in case may miss something is there any reason why `stratifiedkfold` does not work properly for float >>>new_feature
nearest neighbor chaining for ward someone just complained to me that we should do neighbor chaining in ward and we are not https en wikipedia org wiki nearest neighbor chain algorithm have not really looked it up and not very familiar with our implementation >>>enhancement
doctests download datasets as mentioned in 4711 the doctests seem to download some datasets as they are not using the fixtures >>>bug need_contributor
error gradient boosting py igbrt typeerror fit takes exactly arguments given class baselearner object def init self est self est est def fit self self est fit def predict self return self est predict proba np newaxis print begin train igbrt careful tuning is required to obtained good results rf base randomforestclassifier estimators 100 jobs verbose base estimator baselearner rf base igbrt gradientboostingclassifier loss deviance estimators 100 init base estimator learning rate 025 subsample max depth min samples leaf verbose igbrt fit train train print end train igbrt error file python27 lib site packages sklearn ensemble gradient boosting py line 960 in fit self init fit sample weight typeerror fit takes exactly arguments given >>>documentation easy
test 20news fails in master branch work on debian gnu linux wheezy the test `test 20news` fails on my three conda environnements python scipy 15 numpy python scipy 15 numpy python scipy 11 numpy >>>bug
sparse ridge regression with intercept is incorrect ridge regression with `fit intercept true` does not give the same result if is dense or sparse the call to center data` in baseridge fit` should probably be call to `sparse center data` test example returns while with `alpha 0` >>>bug
mrg randomactivation tasks add doc in code add more weight initialization methods add another example this is meant to be the first stage of the pipeline for the random neural network algorithm it fits on the input data by considering the number of features and then randomly generates an `n features activated` coefficient matrix where `n activated` is the number of the hidden layer features defined by the user the coefficient matrix can be used to transform the input data to different space http homepage tudelft nl a9p19 papers icpr 92 random pdf>>>needs_review
making dictionary learning closer to the sparsenet algorithm the dictionary learning algorithm was assuming that the norm of the filters was equal to one by using heuristic to control for the norm of the filters we allow for more equilibrated learning the implementation is simplification of the one used in the original paper from olshausen the dictionary learning is tested in http blog invibe net posts 2015 05 05 reproducing olshausens classical sparsenet html and this pr is tested in http blog invibe net posts 2015 05 06 reproducing olshausens classical sparsenet part html >>>needs_review
gradient of mixture of gaussians gmm and possibly list of modes hi would be nice to have not only the probability associated to certain point in the gmm as returned by `score but also similar function `gradient that returns the gradient is that possible having list of the modes http en wikipedia org wiki mode 28statistics 29 would also be great this article mode finding for mixtures of gaussian distributions http ieeexplore ieee org xpl articledetails jsp reload true tp arnumber 888716 provides some information on both the gradient and the modes >>>new_feature
doc svc fit status using 16 and trying to understand the svc fit status logic when it correctly fitted it is gives >>>documentation easy
revisit dtype numeric introduced ``check array dtype numeric `` to handle ``dtype object`` gracefully in 4057 in 4645 tomdlt added much more fine grained control over dtypes we should revisit if the `` numeric `` ever adds anything or if we want to always be more explicit about whether we want integers or floats >>>api enhancement
mincovdet gives different results compared to matlab libra am comparing scikit mcd to matlab libra https wis kuleuven be stat robust libra revision from 01 08 2007 and getting different results matlab results 9839 1287 1287 9619 python results 6928877 12337967 12337967 92182858 matlab code 53356657 49146826 53464831 77884215 179271 54235304 82244695 1602308 80207316 46638638 98169198 72384052 84574713 15995118 12067491 30272032 63592934 53520174 36629677 66372903 11739395 04946755 79636059 83721967 53642667 13695351 06039201 39190745 68697525 29121661 12986671 64405471 97052261 65406501 2143484 88733004 80609268 40985212 95484672 31931632 33245641 75440015 29094285 09554176 77083199 21533913 37560369 15368846 77095853 36153146 mcd mcdcov mcd cov python code import numpy as np from sklearn covariance import mincovdet np asarray 53356657 49146826 53464831 77884215 179271 54235304 82244695 1602308 80207316 46638638 98169198 72384052 84574713 15995118 12067491 30272032 63592934 53520174 36629677 66372903 11739395 04946755 79636059 83721967 53642667 13695351 06039201 39190745 68697525 29121661 12986671 64405471 97052261 65406501 2143484 88733004 80609268 40985212 95484672 31931632 33245641 75440015 29094285 09554176 77083199 21533913 37560369 15368846 77095853 36153146 mcd mincovdet support fraction 75 fit print mcd covariance >>>bug
linearmodelcv objects compute mse for cross validation but score gives r2 related to 1831 and bit to 4667 feel it is odd that the cv objects use different scoring function than the one used for model selection >>>bug
ridgecv should provide best score think the best score should be easily accessible in ``best score `` attribute that should be done for all cv objects imho >>>easy enhancement
invalid or missing encoding declaration for sklearn tree tree pyd is thrown when invalid parameter is specified for gridsearchcv specifying invalid parameter for `gridsearchcv` leads to throwing exception `invalid or missing encoding declaration for anaconda3 lib site packages sklearn tree tree pyd was able to reproduce it using here `none` is not valid for `min samples leaf` as the later requires integer however it would be preferred to indicate that passed parameter is invalid or not supported after replacing `none` by `1` was able to start search am using python anaconda 64 bit default mar 2015 12 06 10 msc 1600 64 bit amd64 on win32 >>>bug
establish global error state like np seterr as discussed in 4497 having global error warning level in sklearn seems like good idea possible things that we want to control deprecations numerical instability convergence shape casts type conversions unused sample props as in 4497 unscaled features in estimators that want scaled features changedbehaviorwarning>>>api enhancement
wip sphinx gallery use this is proposal to incorporate sphinx gallery into scikit learn there are still some issues to work on so for know one works with the github pr of sphinx gallery https github com sphinx gallery sphinx gallery pull 26 as the main development version to work on backreferences the documented api gives at the end of every module small gallery of the examples using that module examples are properly identified but the html output fails recognizing the correct path of the image and the html file in scikit learn gen rst py this paths are hard coded to its website structure sphinxgallery uses configuration dictionary this issue is mostly solved with https github com sphinx gallery sphinx gallery pull 26 the sidebar hide behavior it just messes up with the scikit learn design and it is manually deactivated in the gallery implementation with the promise to fix it later https github com scikit learn scikit learn blob master doc sphinxext gen rst py l494 508 the carousel there is an additional image processing step for some images displayed in the carousel done in gen rst py that sphinx gallery does not perform image naming all images generated by sphinx gallery have sphx glr prefix if images of the gallery are manually called by the documentation this would be broken more stuff not aware of maybe as in nilearn put local copy of sphinx gallery inside scikit learn >>>needs_review
more feature selection metric needed couldn find bns metric when need it to do feature selection will it or any other metrics be supported in the near future >>>new_feature
rfc use explicit module levels skip lists for estimator checks amueller recently 4550 refactored the `test common` `estimators checks` plumbing to provide single `check estimator` utility method to 3rd party developers who want to automate the compatibility checks of their own estimators with scikit learn standards to be pragmatic some checks have to be skipped for some specific estimators at the moment this is done with the following idiom in the body of the checks them selves instead of hardcoding the list of the names of the estimators to skip in the body of the check functions think it would be cleaner to extract those as module levels constants such as and then in the body of the function furthermore we should probably use the fully qualified names or type objects in the list to make it easier for 3rd party developers to register there in classes in the skip list of specific check without risking class name conflicts with classes from scikit learn wdyt >>>enhancement
svc with kernel poly hangs when using small and large values hello am not sure if it is mathematical or implementation problem but it definitely annoying and user should be at least warned when try this code from sklearn svm import svc clf svc kernel poly degree 99999 99999 clf fit process then takes forever can see that it taking full cpu so guess it does not hang but have never got the result even on pretty good pc `scikit learn 16 and `python 3` what is interesting got results immediately for from sklearn svm import svc clf svc kernel poly degree 9999 9999 clf fit method is hence relatively unstable is this somehow solvable or are there some ways by which can predict if it solvable by poly kernel or not in it working ok and got the results for much larger and much complicated larger dimensions and larger dissimilarities datasets by this code not minimal example just for illustration svmfit svm yy xx kernel polynomial degree type classification thank you>>>easy enhancement
broken example for multilabel classification example with cca the multilabel classification example seems to be broken with cca http scikit learn org 14 auto examples plot multilabel html http scikit learn org stable auto examples plot multilabel html http scikit learn org dev auto examples plot multilabel html >>>bug documentation
documentation of label binarizer and multi label format from the docs feel it is not entirely clear that using label indicators means doing multi label classification think this should be better documented in the multi class narratives the onevsrestclassifier and the labelbinarizer applying label binarizer to your will result in quite different results which is obvious if you know that this will switch the problem to multi label problem but is somewhat unintuitive for the uninitiated >>>documentation easy need_contributor
mcd exact fit fixes issue 3367 the following patch fixes issue 3367 to the best of my knowledge however while testing with various exact fit scenarios encountered an issue which believe may be somewhere else in the code the data used to test this is simple plane all with few outliers present the exact fit scenario works and no issues arise however taking the same data as above converting it to the affine subspace spanned by the data which is basically rotation about the principal axes and translation by the mean the data occasionally raises an error where the determinant is larger than that of the previous determinant both datasets can be found at https gist github com thatgeoguy 713bd1355b87ea2d5d07 where `good data txt` is the original data and `bad data txt` is the same data transformed into its affine subspace the final result is still correct in the end despite couple of the trials triggering the above issue and it only happens with the `bad data txt` dataset am unsure what is causing this but it appears to be something different than what caused 3367 >>>bug
add shuffle parameter to cross val score and gridsearchcv the underlying cv functions in both `cross val score` and `gridsearchcv` have an option to shuffle inputs but these aren exposed this requires the user to shuffle their examples ahead of time this is somewhat inconsistent with `train test split` that uses `shufflesplit` and for the uninformed user might cause confusion in output results in cases where the underlying inputs have some order like in the wine quality dataset for example would it be possible to expose `shuffle` as parameter in both `cross val score` and `gridsearchcv` and simply pass it through to check cv` and on to `stratifiedkfold` and `kfold` >>>new_feature
python crashes when calculating large sne python scikit learn 16 macbook pro 16gb ram macos 10 10 when running the following code in an ipython notebook it runs for long time producing no output and then the ipython kernel crashes and has to restart running the same code from the command line produces one line of output sne computing pairwise distances and then it dies the error is pretty much the same as reported in this older issue https github com scikit learn scikit learn issues 3088 ll attach crash log next but think it out of memory because if reduce the rows of to 10000 then it runs to completion without error some suggested improvements estimate the memory required to carry out this operation and warn or error out earlier try to recover from oom errors in libblas more gracefully if possible reduce the memory requirements of sklearn manifold tsne or implement out of core document the memory requirements in sklearn manifold and sklearn decomposition and perhaps warn about potential oom errors and what they ll look like to users you ve already nicely documented the runtime complexities of the algorithms but have not documented their space complexities >>>enhancement
cross validation shufflesplit setting train size without setting test size the sum of train size and test size is not equal to cross validation shufflesplit setting train size without setting test size the sum of train size and test size is not equal to now the sum of them is default value of test size train size when setting test size without setting train size the train size is autocomputed by test size so we hope when setting train size the test size is autocomputed by train size as well not defualt value bug https cloud githubusercontent com assets 2970920 7217998 166c617e e685 11e4 9753 bf820d1313d4 jpg >>>bug
add example of inductive clustering clusters can be found within training dataset transductive then used to train classifier so as to extend the clustering model to new instances see snippet at https github com scikit learn scikit learn issues 4543 issuecomment 91073246 selwyth is apparently working on this https github com scikit learn scikit learn issues 4543 issuecomment 91185832 >>>documentation easy enhancement
make sure that the output of pca fit transform is contiguous otherwise would rather use because of fortran data for inner product is very slow for example in the kmeans >>>easy enhancement
add icl to mixture gmm suggest adding icl integrated completed likelihood or its approximation icl bic which is an information criteria useful for determining the number of clusters essentially it is icl bic entropy of clustering code to demonstrate probs gmm score samples entropy sum sum prob np log prob for prob in probs icl gmm bic entropy biernacki celeux and govaert assessing mixture model for clustering with the integrated completedlikelihood ieee transactions on pattern analysis and machine intelligence 22 2000 719725 mclachlan and peel finite mixture models john wiley sons inc 2000>>>new_feature
precision recall numbers computed by scikits are not interpolated non standard hi scikit learn seems to implement precision recall curves and average precision values auc under pr curve in non standard way without documenting the discrepancy the standard way of computing precision recall numbers is by interpolating the curve as described here http nlp stanford edu ir book html htmledition evaluation of ranked retrieval results html the motivation is to smooth out the kinks and reduce noise contribution to the score in any practical application if your pr curve ever went up then you would strictly prefer to set your threshold there rather than at the original place achieving both more precision and recall hence people prefer to interpolate the curve which better integrates out the threshold parameter and gives more sensible estimate of the real performance this is also what standard code for pascal voc does and explain this in their writeup http citeseerx ist psu edu viewdoc download doi 10 157 5766 rep rep1 type pdf vl feat also has options for interpolation http www vlfeat org matlab vl pr html and as shown in their code here https github com vlfeat vlfeat blob edc378a722ea0d79e29f4648a54bb62f32b22568 toolbox plotop vl pr the concern is that people using the scikit version will see incorrectly reported lower performance than what they might see reported in other papers >>>bug documentation
classification report not explicit about averaging classification report computes micro averages https github com scikit learn scikit learn blob master sklearn metrics classification py l1265 however that is not documented and can not be changed wonder whether we should add an average keyword or just document it >>>documentation easy
preprocessing label docs it seems to me that if using any sklearn estimators people never have to use anything in preprocessing label as it is done under the hood if this is the case think we should mention that in the user guide >>>documentation
fix bug the result is wrong when use sklearn metrics log loss with one class when there is only one class such as true np array class1 class1 class1 predict np array log loss true predict the result would be wrong which is not the logloss of predict but the logloss of predict the code in log loss if shape np append axis should be change to if shape np append axis >>>bug
error on lasso on constant input variables if lassolarscv is used on dataset where std then the function returns an error here there is an example http jpst it y0rf>>>bug easy
mrg listed valid metrics for neighbors algorithms 4521 >>>needs_review
jaccard distance in trees very different from pairwise distances jaccard distance as observed in 4522 balltree and ``pairwise distances`` have very different results for ``metric jaccard `` >>>bug
wip metrics testing there have recently popped up some issues about support for different metrics see 4520 and 4452 for example am trying to add more tests but am not sure am familiar enough with the metrics neighbors modules jakevdp your help would be much appreciated currently travis fails because the jaccard distances in the trees seem to be very different from the scipy ones think this is because our trees cast everything to bool while scipy uses floats this seems to be pretty big issues as the algorithm that is used might change automatically depending on the dataset >>>bug
list valid metrics for neighbors algorithms see 4520 it would be nice to have listing of the valid metrics for kd tree ball tree brute in the neighbors module possibly by just calling the function in doctest or listing them like done here for evaluation metrics http scikit learn org dev modules model evaluation html common cases predefined values >>>documentation easy
dbscan does not accepts distance seuclidean here is quick example if calculate distance matrix with it the everything is just fine but want to use kd tree or something to speed things up >>>bug
mrg add distance threshold on hierarchical clustering see 3796 it is now possible to define distance threshold that will be used as an early break criterion in hierarchical clustering it is used in both ward tree and linkage tree an error is now raised if cluster and distance threshold are not set or are both set don know yet what should be the behavior if the number of clusters and the distance threshold are set together >>>needs_review
api consistent api for attaching properties to samples this is an issue that am opening for discussion problem sample weights in various estimators group labels for cross validation objects group id in learning to rank are optional information that need to be passed to estimators and the cv framework and that need to kept to the proper shape throughout the data processing pipeline right now the code to deal with this is inhomogeneous in the codebase the apis are not fully consistent ie passing sample weights to objects that do not support them will just crash this discussion attempt to address the problems above and open the door to more flexibility to future evolution core idea we could have an argument that is dataframe like object ie collection dictionary of 1d array like object this argument would be sliced and diced by any code that modifies the number of samples cv objects train test split and passed along the data proposal all objects could take as signature fit sample props none with optional for unsupervised learners sample props name to be debated would be dataframe like object ie either dict of arrays or dataframe it would have few predefined fields such as weight for sample weight group for sample groups used in cross validation it would open the door to attaching domain specific information to samples and thus make scikit learn easier to adapt to specific applications proposal could be optionally dataframe like object which would have as compulsory field target serving the purpose of the current and other fields such as weight group in which case arguments sample weights and alike would disappear into it people at the paris sprint including me seem to lean towards proposal implementation aspects the different validation tools will have to be adapted to accept this type of argument we should not depend on pandas thus we will accept dict of arrays and build helper function to slice them in the sample direction also this helper should probably accept data frame but given that data frames can be indexed like dictionaries this will not be problem finally the cv objects should be adapted to split the corresponding structure probably in follow up to 4294 >>>api enhancement
4475 add safe pairwise distances function dealing with zero varian 4475 add safe pairwise distances function dealing with zero variance samples when using correlation metric the best fix would be to have the metric not returning nan values but as the correlation metric is actually computed by scipy we can modify it directly so when metric correlation we replace rows and cols corresponding to zero variance samples by the maximum distance here >>>bug
wip ridgegcv with sample weights is broken what the title says the way it is right now the sample weights weight the eigenspaces of the gram matrix which doesn seem sensible >>>bug
mrg issue 1453 mds fall back to svd when possible this is follow up on pr 3141 ve fixed most of ogrisel comment >>>needs_review
documentation for normalize in ridge regression somewhat unclear for linear models with regularization the input variables should be standardized because the solution is not scale invariant noticed that in the ridge regression source code the input variables are centered by mean subtraction but normalization by std deviation is flag my questions are under what circumstances should normalized be set to true so that both centering and normalizing by std dev are turned on does normalize true affect prediction when predicting from new should be centered and normalized does normalizing have any affect on the range of alpha the regularization parameter it would be nice to be able to restrict alpha to it would be great if the documentation made this clear thanks >>>bug documentation
tsne with correlation metric valueerror distance matrix must be symmetric from sklearn manifold import tsne import numpy as np np random seed 42 data np random rand 10 data model tsne metric correlation model fit transform data valueerror traceback most recent call last in model tsne metric correlation res model fit transform data ran model fit transform ran data users ch miniconda envs sci34 lib python3 site packages sklearn manifold sne py in fit transform self 522 embedding of the training data in low dimensional space 523 524 self fit 525 return self embedding users ch miniconda envs sci34 lib python3 site packages sklearn manifold sne py in fit self 447 self training data 448 449 joint probabilities distances self perplexity self verbose 450 if self init pca 451 pca randomizedpca components self components users ch miniconda envs sci34 lib python3 site packages sklearn manifold sne py in joint probabilities distances desired perplexity verbose 52 conditional conditional 53 sum np maximum np sum machine epsilon 54 np maximum squareform sum machine epsilon 55 return 56 users ch miniconda envs sci34 lib python3 site packages scipy spatial distance py in squareform force checks 1479 raise valueerror the matrix argument must be square 1480 if checks 1481 is valid dm throw true name 1482 1483 one side of the dimensions is set here users ch miniconda envs sci34 lib python3 site packages scipy spatial distance py in is valid dm tol throw name warning 1562 if name 1563 raise valueerror distance matrix must be 1564 symmetric name 1565 else 1566 raise valueerror distance matrix must be symmetric valueerror distance matrix must be symmetric>>>bug easy
cca userwarning scores are null at iteration each of is 300 dimension array each of is 5000 dimension array am trying to run cca for getting an transform matrix from to but keep getting following warning and an error which think are related usr local lib python2 site packages sklearn cross decomposition pls py 290 userwarning scores are null at iteration warnings warn scores are null at iteration >>>bug
implementation of mdlp for discretization of continuous attributes believe this would be good feature in sklearn preprocessing how do others feel about this >>>new_feature
mrg implement haversine metric in pairwise fixes 4453 4452 add haversine distrance for pairwise distances and document it did not find any test for pairwise py so did not add any test cases >>>needs_review
mismatching dimensions in extmath weighted mode apparently this check here https github com scikit learn scikit learn blob master sklearn utils extmath py l397 isn enough when using neighbors radiousneighborsclassifer predict keep getting errors due to the mismatching dimensions in the next line https github com scikit learn scikit learn blob master sklearn utils extmath py l398 valueerror operands could not be broadcast together with shapes 8792 8807 valueerror operands could not be broadcast together with shapes >>>bug
implement haversine metric in pairwise document properly see 4452 the haversine metric is implemented in the neighbors trees but not in pairwise module which results in odd crashes also the neighbors module does not list the haversine metric as one of the available metrics in the docstring >>>easy
dbscan and haversine metric hi am observing somewhat weird behavior when using dbscan and am not sure what going on sometimes when am using dbscan with the haversine distance get the following error delorean affinity argparse basemap distribute joblib matplotlib mock nose numpy pygmaps pymongo pyparsing python dateutil pytz 2014 scikit learn 15 scipy 14 six wsgiref 2>>>bug
problems in sklearn decomposition pca with components mle option we have found several problems in the implementation of the method to automatically tune the number of components of the pca algorithms the algorithm never tests full rank this is most probably due to the fact that loops using the rank end always at rank ``for in range rank `` if two eigen values are equals there is log issue zeros eigen values are not treated explicitly possible solutions for checking the loops ranges for predetecting small eigen values lower than the numerical noise excluding them from rank scan have no idea for we had the problem here with very small eigen values in numerical noise which were totally identical never managed to create syntetic dataset which reproduce the problem since the even with symetric datasets there is always small difference in the order of numerical precision between theoretically identical eigen values >>>bug
out of bag probability estimates for random forests this is as much proposal as question does it make sense to use out of bag samples for estimating the distributions in the leaves that should give you better estimate of uncertainty right think there is something like that in and it seems to make sense to me ping arjoly glouppe >>>enhancement
partial dependence plots for random forests does scikit learn have any capacity for partial dependence plots and associated data arrays for random forest analyses can find the plot for gradientboostingregressor here http scikit learn org stable auto examples ensemble plot partial dependence html doing the same for rf outputs >>>enhancement moderate
weighted kde not sure this is the correct place but would very much appreciate the ability to pass weight for each sample in kde density estimation there exits adapted version of scipy stats gaussian kde http stackoverflow com questions 27623919 weighted gaussian kernel density estimation in python >>>enhancement
issue tf idf computation hi have some concerns about what is currently implemented as tf idf in https github com scikit learn scikit learn blob master sklearn feature extraction text py tf idf tf idf my concern is regarding the the comment in the source code says the actual formula used for tf idf is tf idf tf tf idf instead of tf idf the effect of this is that terms with zero idf that occur in all documents of training set will not be entirely ignored this is indeed true the computation is effectively linearly interpolating between tf idf and tf however adding one to idf is not standard practice even after extensive search reading many ir and nlp papers regarding tf idf and consulting ir and nlp researchers have not yet encountered an instance of anyone doing this am therefore concerned that this is the default behavior in sklearn all the more so because there is no mention of this in the documentation had to read the source code to figure it out although there are several standard variants for computing tf and for computing idf these are actually pretty well documented on the wikipedia page they are always combined as tf idf not tf idf granted and taking the standard definition of idf for simplicity you can subsume the into the idf score itself giving idf log log log log in effect this means you re pretending you saw times as many documents as you actually did and none of them contained any terms in the vocabulary this means that all terms frequent and infrequent will be treated as if they occurred in fewer documents than they actually did however this is not equivalent to any of the standard variants of idf since sklearn tf idf is widely used think it should reflect standard behavior definitions in the ir and nlp literature and thus the should be removed to be clear happy for the to be left as an option or even changed to be with speficied by the user thereby allowing the user to linearly interpolate between tf and standard tf idf but think users should be aware that they are doing this rather than doing this and thinking they re using standard tf idf >>>bug
mrg add warning for kfold cv if random state must be changed but shuffle is false trying the pr in another issue >>>needs_review
scalable kmeans can see kmeans implementation here but do you have scalable kmeans implementation which is supposed to be better than kmeans >>>enhancement large_scale
how to transform categories before treating missing values hi would like to compare missing values techniques for this need to deal with missing values after transform categorical features by transformers any idea how this can be done the current transformers don like missing values causes an error thanks >>>question
mrg choose number of clusters this module implements algorithms to find optimal number of clusters stabiliity gap statistic distortion jump silhouette calinsky and harabasz index and elbow methods if metric is needed for example to compute distortion the mean distance of point to its cluster center all distance of scipy spatial distance can be used >>>needs_review
sklearn cluster dbscan allow for nan values for user defined metric import sklearn def user defined metric cluster algorithm sklearn cluster dbscan metric user defined metric cluster algorithm fit matrix >>>enhancement
density doesn normalise in vbgmm and dpgmm having trouble using the vbgmm and dpgmm for density estimation as far as understand both should have the same interface as the normal gmm however while the normal gmm produces good fit the vbgmm and dpgmm produce bad fits and non normalised densities this leads me to wonder whether there is something deeper wrong than me incorrectly using the code the problem presents itself both in the density estimation example http scikit learn org stable auto examples mixture plot gmm pdf html by appending the line print np sum np exp this is approximately when using normal gmm but much smaller when using the vb or dp gmm the same behaviour is shown on toy 1d density estimation problem import numpy as np import numpy random as rndn import sklearn mixture as skmix import matplotlib pyplot as plt rnd randn 300 np vstack rnd randn 300 gmm skmix gmm gmm skmix dpgmm gmm fit np linspace 10 10 1000 np exp gmm score plt hist bins 50 normed true plt plot plt show integral np sum print integral is this behaviour just the result of poor fit due to local optimum or something the fact that the predictive densities don normalise lead me to believe it something else asked the same question on stackoverflow http stackoverflow com questions 28575943 mixture model predictive distributions in scikit learn >>>bug
roc curve should returns fix size for the same size input input length true score is 9177 output fp tp th size is 5816 but when re sample the data it is 5756 guess due to sampling with replacement creates duplicate data which is about in matlab perfcurve returns fp tp th the same size as the input size or at specified fp tp th values >>>enhancement
make spectral embedding deterministic there is possible sign flip in the eigensolver that makes ``spectral embedding`` and therefore ``spectralembedding`` not deterministic think we should fix that using similar method to ``sklearn utils extmath svd flip``>>>easy enhancement
ensure sparse `y` is supported in grid search etc as reported at https github com scikit learn scikit learn issues 1233 issuecomment 73467547 `gridsearchcv` currently breaks if it is fit with sparse `y` at least for some supported `scipy` versions this is due to `len` call when we should probably be using `check consistent length` in any case test is needed since we are meant to be preferring sparse matrices for multilabel data now >>>bug easy
mrg neighbors refactor big refactor of the nearest neighbors space partitioning code currently kd tree and ball tree include textually the source of their base class `binary tree pxi` so that the code in that module gets compiled twice the code in this pr merges the modules and compiles the base class once also removal of workaround for old cython and numpy versions tiny optimizations less python api calling preliminary to optimizing radius neighbor queries as promised in 4157 >>>needs_review
mrg gbm meta ensembles support for class weight refactoring 4114 on top of 4190 and removing the gbm coding style changes from the scope of this pr for ease of review basically adding `class weights` to the remaining ensemble classes as was done in 3961 for forests and trees amueller pprett glouppe you have all at least glanced at 4114 so perhaps you ll have chance to review the new refactored version here >>>needs_review
rbm partial fit should use batches as pointed out in 4205 by treora rbm partial fit should use batches and then fit can be implemented in terms of ``partial fit`` >>>enhancement
interpreting output of pred proba have dataset of 35 samples and 28 classes in `decisiontreeclassifier` `predict proba` returns list of 28 `numpy ndarray` all with size 35 or 35 don understand what are the two values in the `ndarray` shouldn there just be one the probability of that class being predicted in which case the array would just be 28 35 why sometimes there are only 35 values providing some data https www dropbox com n9rv0ughbg6z8nx sampledata pickle dl to replicate my issue >>>bug
kneighbors graph param check ineffective in `sklearn neighbors graph` the check params` function is called by both of the graph methods in the module the idea seems to be to ensure that when the neighbors are precomputed that the parameters passed into say `kneighbors graph` match what was used in the computation of the `balltree` however if the `balltree` itself was from prior fit the check misses that fact demonstration not sure any of this matters since `kneighbors graph` and the things it calls doesn do anything with the name of the metric if the balltree has been precomputed but it certainly the case the check params is not having its intended effect so does this matter should `nearestneighbor fit be modified so that it copies all relevant data over when is precomputed this is bit contrived but ran across it while trying to modify `isomap` to handle non euclidean metrics the issue is that isomap initializes `nearestneighbors` instance with metric equal to minkowski and `p 2` so if you call `fit` with precomputed `nearestneighbors` using different metric then this fact is not transferred to the initialized instance this can all be avoided if the api to isomap was changed but this particular point of check params not always doing an effective check still holds >>>bug
l1 feature selection docs confusing here http scikit learn org dev modules feature selection html selecting non zero coefficients mentions selecting non zero coefficients via transform in linearsvc logisticregression and lasso lasso doesn have transform the other two use the median coefficient for feature selection not being nonzero >>>documentation
contamination and threshold should be part of covariance outlierdetectionmixin predict the settings of contamination and threshold are currently possible only at the init of the covariance ellipticenvelope and covariance outlierdetectionmixin https github com scikit learn scikit learn blob master sklearn covariance outlier detection py l37 https github com scikit learn scikit learn blob master sklearn covariance outlier detection py l168 these may be the precomputed defaults however it would be much more useful if the predict method accepted the contamination threshold parameters too these parameters are used only to make the output of the decision function binary turn values above the threshold into and the rest into thanks for improving these >>>enhancement
partial fit incremental online learning support for multiclass classifiers the multiclass meta estimators in sklearn multiclass do not currently support `partial fit` but think they could easily where the base estimator does >>>easy enhancement
test that sparse matrix with 64bit index are supported or give sensible error message basically adding new common test to the sparse matrix test that checks for the other index type and see what happens >>>api
allow for transformers on following up on 3113 and 3112 what about arbitrary transforms to the `y` values those issues dealt primarily with label transforms but would like to use transformers to mean or range center the `y` values as well ideally would have some transform that can be applied to the values before fitting and then applied in the inverse to the predicted `y` values coming out of `predict` ideally this transformer could be added to pipeline currently the signature for `transform` for `standardscaler` https github com scikit learn scikit learn blob a413f875f1a7c9528290e01e1c030c336b9f32e4 sklearn preprocessing data py l338 allows for transforming `y` but as pointed out in the linked issues not all transform methods have signature allowing for `y` to be passed in further even for `standardscaler` there is an inconsistency with the `inverse transform` https github com scikit learn scikit learn blob a413f875f1a7c9528290e01e1c030c336b9f32e4 sklearn preprocessing data py l366 not taking `y` >>>api new_feature
cross decomposition module needs work recently ve spent some time exploring `cross decomposition` http scikit learn org dev modules classes html module sklearn cross decomposition module haven paid attention to `cca` noticed several problems with it the module needs much better narrative documentation now it not clear what algorithms actually do where they might be useful and how they are different the layout of files and import approach are unusual for sklearn the files are named with trailing underscores all is put into individual files but not in init py` the implementation is rather obscure for short and conventional description of pls refer here http statmaster sdu dk courses st02 module08 index html nipals twoblocks inner loop` in fact computes both weights and scores but only weights are returned and scores are then recomputed parameter `norm weights` crucially determines how the algorithm works along with `deflation mode` and `mode` but does it in tricky way it took me lot of time to understand why implemented here `plsregression` is equivalent to pls algorithm described in most sources parameter `norm weights` set to `false` for `plsregression` in order to make regression coefficient see link above between and equal to it is done purely by convention and to compare with implementations in suppose alternative svd cross product` solver is provided but never used in code only in tests also it can be substitution for nipals twoblocks inner loop` with `norm weights false` there are no required early stop checks when `x` or `y` matrix are deflated to zeros iterations become invalid and assumed properties are no longer held that causes the bug 3932 computed `y rotations is not correct for `plsregression` `y scores np dot yc rotations think there is no way to compute these rotations in this case because `y` is deflated on `x scores` several obvious bugs such as modifying non existing variable etc >>>enhancement
implement single linkage clustering see here https github com scikit learn scikit learn pull 2199 issuecomment 70141935 it should be added as an additional ``linkage`` option to agglomerative clustering had an implementation here but it rebuilds the tree which is very stupid https github com amueller information theoretic mst blob master heuristics py l10>>>new_feature
mrg support sample weight in silhouette score sought `sample weight` in `silhouette score` to account for multiple points that are merged into one when calculating average distances hacking it into the current implementation resulted in very slow solution thus this pr also rewrites the implementation yielding something that bit slower than the solution at master but supports `sample weight` ve also added tests for correctness which haven otherwise found in the code >>>needs_review
importerror home yifengli local lib python2 site packages sklearn metrics pairwise fast so undefined symbol intel fast memcpy when ran the random forests of sklearn got the following error exception memoryerror memoryerror in sklearn tree tree tree resize ignored segmentation fault thus tried to install the latest sklearn from github git clone https github com scikit learn scikit learn git cd scikit learn python setup py build python setup py install user however when run from sklearn ensemble import randomforestclassifier got the following error it maybe complier issue traceback most recent call last file main random forests debug py line 14 in from sklearn ensemble import randomforestclassifier file home yifengli local lib python2 site packages sklearn ensemble init py line in from forest import randomforestclassifier file home yifengli local lib python2 site packages sklearn ensemble forest py line 55 in from feature selection from model import learntselectormixin file home yifengli local lib python2 site packages sklearn feature selection init py line 20 in from rfe import rfe file home yifengli local lib python2 site packages sklearn feature selection rfe py line 16 in from cross validation import check cv as check cv file home yifengli local lib python2 site packages sklearn cross validation py line 31 in from metrics scorer import check scoring file home yifengli local lib python2 site packages sklearn metrics init py line 29 in from import cluster file home yifengli local lib python2 site packages sklearn metrics cluster init py line 19 in from unsupervised import silhouette samples file home yifengli local lib python2 site packages sklearn metrics cluster unsupervised py line 10 in from pairwise import pairwise distances file home yifengli local lib python2 site packages sklearn metrics pairwise py line 25 in from pairwise fast import chi2 kernel fast sparse manhattan importerror home yifengli local lib python2 site packages sklearn metrics pairwise fast so undefined symbol intel fast memcpy >>>bug build_/_ci
sync docstring sphinx processing with the numpydoc project numpy has split off their docstring processing into separate project at https github com numpy numpydoc scikit learn has local copy https github com scikit learn scikit learn tree master doc sphinxext numpy ext which has diverged due to stylistic fixes as well as some new feature or data checking needs our copy should be re synced with the `numpydoc` project such that we benefit from any cleaning there so that our changes contribute back to that project allowing other numpydoc using projects to benefit from our enhancements >>>easy
kerneldensity and gmm interfaces are unnecessarily confusing to evaluate the density of ``data`` at ``x`` the current interface is ``density np exp kde score samples data `` see in action here http nbviewer ipython org gist jakevdp 0b3e332015ec2de7b973 this seems really silly and unnecessary for such common operation how would people feel about adding ``density`` method to ``kerneldensity`` and to ``gmm`` to make this easier for users to figure out >>>enhancement
vectorizing distance calculations in approximate neighbor search for batched queries in exact nearest neighbor search when brute force method is used it simply uses `pairwise distance` with entire batch and the calculation is vectorized always in `kneighbors` and sometimes in `radius neighbors` approximate neighbors with `lshforest` does not have this advantage since for different queries neighbors will be searched from different candidates in the input space therefore even for batched queries distance calculations have to be done in conventional loops this issue affects any lsh based technique not only lsh forest since final distances are calculated only in set of selected candidates for each query this is bottleneck when using lsh techniques because most of the time for queries are spent on distance calculations not candidate selections does anyone have suggestions tackle this problem >>>enhancement
metrics log loss fails when any classes are missing in true when calling log loss with label array not an indicator matrix for true it uses labelbinarizer to construct the indicator matrix if not all classes in pred are present in true this has the wrong shape and it raises>>>enhancement
mrg cemoody bhtsne barnes hut sne introduction this pr presents the barnes hut implementation of sne sne is used to visualize high dimensional data in low dimensional space that attempts preserve the pairwise high dimensional similarities in low dimensional embedding the barnes hut algorithm which is used by astrophysicists to perform body simulations allows the calculation of the sne embedding in nlogn time instead of this effectively allows us to learn embeddings of data sets with millions of elements instead of tens of thousands example an ipython notebook is available where we ve put together simple install instructions demo of both methods http nbviewer ipython org urls gist githubusercontent com cemoody 01135ef2f26837548360 raw 76ce7f0bac916a516501ead719b513b22430cad0 barnes hut 20t sne 20demo ipynb bhtsne https cloud githubusercontent com assets 3419930 5566392 24d8b7dc 8edb 11e4 9e67 4f319fc6dc59 png performance this compares the timings for the barnes hut approximation against the current method note the log axis timing https cloud githubusercontent com assets 3419930 5566592 9f8ba956 8ee2 11e4 88e9 2504485f06c2 png the following diagram shows the relative speedup between both versions speedup https cloud githubusercontent com assets 3419930 5566360 f4802f94 8ed9 11e4 891c da999a0426e8 png todo create fit transform methods to update using new data add test for transform called before fit add test for skip num points raise error if method barnes hut and using sparse data raise error if method barnes hut and embedding dim transform 64bit input floats to 32bit update the narrative documentation in doc modules manifold rst add toy data to test perplexity fix failing tests expose tree consistency checks as unit tests fix memory leaks segfaults on data larger than 50k elements pep8 the code include usage documentation remove extra imports print statements pdb statements ensure python works ensure output dimensionality works for 2d or 3d am using memviews or returning full arrays appropriately incorporate into sklearn remove gil as much as possible in cython code add answer tests ensure old sne tests works changed perplexity calculation to nearest neighbors changed positive gradient calc to nearest neighbors learn you can read more about the technique in general here http lvdmaaten github io tsne the barnes hut approximation is here http lvdmaaten github io publications papers jmlr 2014 pdf >>>needs_review
error sklearn tests test pipeline test feature union parallel when test the scikit learn installation it occurs the error message like that >>>bug
support sample weight in clusterers currently no clusterers or clustering metrics support weighted dataset although support for dbscan is proposed in 3994 weighting can be compact way of representing repeated samples and may affect cluster means and variance average link between clusters etc ideally birch global clustering stage should be provided weighted dataset and is current use of unweighted representatives may make its parametrisation more brittle this could be subject to an invariance test along the lines of there is also minor question of whether `sample weight` should be universally accepted by `clustermixin` or whether `weightedclustermixin` should be created etc sample weight support for clusterers affinity propagation don know this well enough to know the applicability birch dbscan hierarchical ward link hierarchical complete link as far as can tell hierarchical average link means minibatch means mean shift spectral>>>enhancement moderate
support approximating euclidean manhattan metrics in lshforest only cosine distance is currently supported while most neighbors algorithms assume minkowski distance euclidean this may depend on 3988 or we could replace the current hasher as default it would be really good to have this in the next release so that we can allow this to be the default >>>new_feature
support alternative metrics hashers in lshforest the lshforest implementation in scikit learn currently only supports single hasher `gaussianrandomprojectionhash` and its corresponding metric cosine distance it would be much more useful if it supported minkowski distance incl euclidean and distances facilitated by other hash functions minkowski distance might use the stable distributions technique http www immorlica com pubs pstable pdf propose the following parameters thus the auto hasher for `metric cosine might be `gaussianrandomprojectionhash 32 but user could specify an equivalent based on `sparserandomprojections` for instance the following extensions may be best implemented as separate prs for the moment we should retain the requirement that the hash be represented as 32 bit integer we should also implement the stable distributions hash or better alternative if it exists to support minkowski metrics and make this the default hasher before the next scikit learn release see 3990 very low priority in some cases calculating the exact distance in the original feature space may be waste of time while an approximate metric can be evaluated as the hamming distance between the query and returned hashes assuming all hash functions are independent which they are currently one might thus be able to set `metric approximate and rely on `hasher` to calculate the returned approximate distances this also means fit x` does not need to be stored >>>moderate
scikit gridsearch and python in general are not freeing memory hi recently asked question on stackoverflow here http stackoverflow com questions 27508844 scikits gridsearch and python in general are not freeing memory about an issue that encountered with scikit learn `gridsearch` and memory utilization basically it allocated more and more memory the longer it runs after the job fails when it reaches the 128 gb on the system am running it on have more details written in the stackoverflow question linked above and also created github repo where put the script and data if you want to reproduce this issue https github com rasbt bugreport tree master scikit learn gridsearch memory>>>bug
accelerated coordinate descent for elastic net this contribution implements the accelerated coordinate descent method for the elastic net based on the paper fercoq richtrik 2013 accelerated parallel and proximal coordinate descent arxiv preprint arxiv 1312 5799 it does not replace the existing coordinate descent code although also propose some marginal improvements >>>needs_review
pipeline doesn work with label encoder ve found that cannot use pipelines if wish to use the label encoder in the following wish to build pipeline that first encodes the label and then constructs one hot encoding from that labelling however the following error is returned it seems that the problem is that the fit method for label encoder only takes argument whereas the pipeline assumes that it will take an and an optional >>>api enhancement
mrg refactor of sgd code to use only one call to plain sgd average sgd this code also makes all methods private and extracts the multiprocessing function into helper function also note that this does not change the functionality of the code at all it just makes it more clean and maintainable in my opinion >>>needs_review
mrg don set trailing attrs in init as said over at gh 3627 we need to fix all the init `s that set final underscore attributes some of them set those to none or similar values we should do systematic search of the codebase and introduce smoke test did just that note code uses `getattr self someattr none is none` instead of `hasattr` in an attempt to be backwards compatible with pickled estimators that have these attributes not sure how important that is who would pickle before fit and unpickling is not tested >>>needs_review
fix enh pca and randomizedpca now have inconsistent meanings of `` components `` eickenberg introduced new api meaning of the components attribute to pca https github com scikit learn scikit learn commit 3f6c61fed71563cb91d9291d851af5fa5a662eb4 the randomizedpca code still stores whitened components instead of unit scale components this should be changed right cc agramfort ogrisel >>>api bug
improve guide to contributing don think the contributor guide http scikit learn org dev developers index html at present is very easy to read and it should contain bit more on certain matters at the moment it is mostly focused on technical how tos the main additional things to get across are what belongs in scikit learn and what doesn and if something doesn where can it go this may belong on separate page more ways to contribute what makes good code contribution the pull request process this needs to be concise as possible one stop shop for the hundreds of requests to contribute would like to see the guidelines have shape like the following scikit learn scope this may belong in the contributor guide or may belong elsewhere in the documentation new about the project or in the faq in any case it should be referenced from the contributor guide well established algorithms and occasionally recent variants of well established algorithms to be mostly usable out of the box in distinction to pylearn2 et al specifically for ml problems whose targets have minimal structure binary regression multiclass multilabel and multitask problems from more or less flat feature vectors these all involve sometimes spare arrays as input and targets ways to contribute this is already mostly there but needs some reordering it needs to begin with an emphasis on how overtasked the core developer group is and how important it is to help complete existing highlights contributor competence leading way for larger contributions reporting bugs easy issues and how to adopt an issue finding and adopting stagnant pull request pr perhaps look for old wips new features see scope and discussion of contributions below first suggest following mailing list documentation examples testing and improving consistency reviewing an extremely valuable contribution after showing you are familiar with what makes good contribution spreading the word how to contribute the pull request process perhaps adopting an issue wip status mrg status mrg mrg adding to what new refinement and rebasing along the way the ideal code contribution well written clear and reasonably efficient code consistent with the project style tests that ensure it works understandable complete docstrings narrative documentation in the user guide examples that highlight interesting features of the algorithm how to inspect its model and how to interpret its hyperparameters code management git coding guidelines api design documentation guidelines>>>documentation moderate
improve the pdf rendering of the documentation this is sequel to 3830 those are improvements identified by jnothman to improve the pdf output of running the `make latexpdf` command in the doc folder there is couple of useless blank pages before the first chapter there should be table of contents there seem to be many chapters that should really be sections of other chapters the one on fetching covertype changelong entries what would be thumbnails corresponding to examples on the web site show up as duplicated images in the examples themselves as well as as in the examples associated with each class function in the reference assume within documentation references should be hyperlinked but do not seem to be in cases where ve looked >>>documentation enhancement
invariance testing for partial fit much of the common functionality across estimators is tested within `sklearn tests test common` as far as can tell there are no tests of what `partial fit` should do in general such as returning `self` not sure what else is general to all estimators supporting `partial fit` test should be added >>>easy
landscape io for code quality what do you think of using landscape io to track code quality has anybody already tried it here an example of usage for python project https landscape io github cokelaer pypiview 9>>>build_/_ci
mrg add feature extraction columntransformer fixes 2034 todo docstrings simple example test documentation test feature names also see here http zacstewart com 2014 08 05 pipelines of featureunions of pipelines html for how this would help people >>>needs_review
mrg matern kernel add matern kernel to the kernels in pairwise py the matern kernel https en wikipedia org wiki mat c3 a9rn covariance function and rasmussen and williams 2006 pp84 is generalization of the rbf and the absolute exponential kernel with an additional hyperparemeter nu which allows interpolating between these two rbf nu inf absolute exponential nu in contrast to the rbf kernel it makes less strict assumptions on the smoothness of the function to be learned this is shown in an example for step function for different values of nu figure https cloud githubusercontent com assets 1116263 5189587 64c46cb2 74df 11e4 9b94 684e9751bc78 png todos support for arbitrary values of nu coef0 tests>>>needs_review
dbscan documentation suggestions for parallel processing think it is worth adjusting the dbscan documentation to note that if you ask dbscan to calculate pairwise distances for you then it will do so using only one core if you want to take advantage of multiple cores you should pre calculate distance matrix using `pairwise distances jobs and then run dbscan with your pre calculated distance matrix >>>documentation
bug in metrics roc auc score pred 1e 10 sol metrics roc auc score sol pred wrong is correct pred sol metrics roc auc score sol pred correct>>>bug
resampler estimators that change the sample size in fitting some data transformations including over under sampling 1454 outlier removal instance reduction and other forms of dataset compression like that used in birch 3802 entail altering dataset at training time but leaving it unaltered at prediction time in some cases such as outlier removal it makes sense to reapply fitted model to new data while in others model reuse after fitting seems less applicable as noted elsewhere https github com scikit learn scikit learn pull 1454 issuecomment 11313359 transformers that change the number of samples are not currently supported certainly in the context of `pipeline`s where transformation is applied both at `fit` and `predict` time although hack might abuse `fit transform` to make this not so `pipeline`s of `transformer`s also would not cope with changes in the sample size at fit time for supervised problems because `transformer`s do not return modified `y` only `x` to handle this class of problems propose introducing new category of estimator called `resampler` it must define at least `fit resample` method which `pipeline` will call at `fit` time passing the data unchanged at other times for this reason `resampler` cannot also be `transformer` or else we need to define their precedence for many models `fit resample` needs only return `sample weight` for sample compression approaches that in birch this is not sufficient as the representative centroids are modified from the input samples hence think `fit resample` should return altered data directly in the form of dict with keys `x` `y` `sample weight` as required it still might be appropriate for many `resampler`s to only modify `sample weight` if necessary another `resampler` can be chained that realises the weights as replicated or deleted entries in `x` and `y` >>>api new_feature
add examples to class docs most of the docs in classes lack examples it would be great to add one or two examples similar to this http scikit learn org stable modules generated sklearn linear model lasso html>>>easy
fetch 20newsgroups failed http error 500 internal server error when try to follow the tutorial of the text analysis got this error have check the internet can download the gzipped data file manually from http people csail mit edu jrennie 20newsgroups 20news bydate tar gz but it seems when run fetch 20newsgroups always failed at urlopen url >>>bug
log responsibilities in gmm hello the current implementation of `gmm score samples` https github com scikit learn scikit learn blob master sklearn mixture gmm py l277 computes log responsibilities exponentiates them and then returns only the exponentiated version wonder if there is there reason for that having way to extract log responsibilities might improve the accuracy of more complex models built on top of `gmm` for example `gmmhmm` https github com hmmlearn hmmlearn blob master hmmlearn hmm py l1209 >>>enhancement
loading my own datasets hi all am very new in scikit learn my questions is how to download my own dataset csv file will be highly appreciated any answers thanks martin>>>documentation question
hierarchical clustering distance threshold so the output of hierarchichal clustering results can be determined either by number of clusters or by the distance thereshold to cut the tree at that threshold however scikit learn only supports one way class sklearn cluster agglomerativeclustering clusters as suggestion is it possible to add the other option to give distance threshold as input argument and get as many clusters are created in the output so one of this argument should be given either clusters or the threshold not both >>>new_feature
consistency in documentation think common things in documentation should be written in consistent way right now can think of the following points whether to name entities which can be transformed to `ndarray` as `array` or `array like` how to list different input options should curly braces always be used for example array like sparse matrix optional arguments default values some convention should be adopted like kernel string default rbf or kernel string default rbf the word `optional` should be dropped anyway as redundant how to name `x` and `y` arrays in `fit` and `predict` methods the one adopted convention would be helpful how to mention shapes of arrays was told that `x shape samples features was adopted is it true how about arrays samples or samples anyway do you think it is an issue at all maybe core developers could decide and update coding guidelines listed only some points >>>documentation
ensemble models and maybe others don check for negative sample weight when sample weights are negative the probabilities can come out negative as well >>>bug
possible memory leak for ridge solvers running into some memory issues with the linear model ridge class in short when fitting ridge with the lsqr or sparse cg solver getting large memory increases when doing the fitting in loop in cross validation the peak memory used is roughly linear with the number of repetitions in the loop here quick gist http nbviewer ipython org gist choldgraf 6a7be7866f2a3a3d3f98 to show what talking about >>>bug
fix fixing issue 3550 hard clamping fixing issue 3550 >>>bug
nested parallelism hi wanted to bring up an issue with the parallel execution model of scikit learn that has bitten me numerous times often find myself wishing to run sklearn task that can be parallelized with jobs however may wish to run the task itself in parallel with even more tasks an example of this is running grid search on multiple different training sets at once or with different cv generators or may wish to run validation curves on multiple different parameters at once this isn currently possible because you can kick off joblib parallel inside of subprocess in some cases it makes sense to simply break the tasks out into separate programs however often that breakdown doesn make sense and it would really be best for the code if it could all be part of the same program the workaround have used is to reimplement various parts of the sklearn internals in such way that rather than create all the joblib delayed jobs and execute them with joblib parallel in the same place create the jobs in one place and return them and then collect each task delayed jobs and run them together under one parallel outside then take the results of the completed delayed jobs and send them back into their respective tasks to be aggregated and processed so my question is would there be interest in adding this capability to sklearn think basic pattern of providing an option to either return the completed results or delayed jobs result aggregator function tuple would go long way would be interested in working on the pull request for it if there was interest in such thing thanks dan spitz>>>new_feature
initialize the code book for multiclass problem just looked at the implementation of outputcodeclassifier for multiclass problem the process for randomly initializing the code book is bit buggy it should be fine if code size is big but when code size is relatively small for example classes code size the only possible code boos is it is very easy to have same binary code for the two different class here if using randomly initialization think it should be an easy fix but there should be some restriction on the output codebook otherwise it could have many possible choices like to hear some suggestion about what kind of code book would be desirable for example the one restriction can think of is that for each column number of and should not differ more than this makes sense because for the binary problem of each column would be relatively balanced in terms of number of classes in and and it make the codes for classes spread over the code space which makes the method of finding the best classes for output by minimzing euclidean distances more reasonable >>>enhancement
mrg svd implemented svd algorithm as free function notes questions haven done any optimizations yet but left todo notes in places where it can be done am going to profile the code later and see where if the optimizations are necessary the algorithm currently uses omp as sparse coder and it is possible to make it customizable however this will make the function interface more complicated as different sparse coders have different parameters should do it or should leave it like this there are now no unit tests for the implementation but am bit confused on how to create them for such functions should just check the api and simple functions like worst represented example` >>>needs_review
dbscan clustering of string data so want to use the dbscan clustering algorithm from scikit learn on dataset of string charcaters so used metric levenshtein distance and my data is given in numpy array but why it wants to convert them to float am using the following packages and my data shape and dbscan object is defined as below >>>enhancement
sample weight for lasso elastic etc am using lasso for time series data and would like to decay old data with weight vector any reason why this is not available for lasso meanwhile if fit lasso with dot sqrt diag weights and dot sqrt diag weights would it mean the same thing thanks >>>enhancement
pca complexity formula in decomposition reference documentation http scikit learn org stable modules decomposition html it is stated that the complexity for the regular pca is which becomes for typical setting where there is more sample than feature considering pca only requires to work on the covariance matrix nfeatures nfeatures which construction cost is isn the squared nmax an error >>>documentation question
balanced shuffle folds hi was wondering whether there was an in built function to make fold cross validation where each class is artificially equalized within the training and optionally the testing set train test know it not optimal to throw some data out and we should favor stratified foldings but some do sometimes want to get chance level at 50 if this feature do not already exist do you think it be better to add it as an option in stratifiedkfold and stratifiedshufflesplit or do you think separate function would be more appropriate thanks >>>new_feature
mrg added metrics support for multiclass multioutput classification fix for 3453 ping arjoly added support for `zero one loss` and `accuracy score` >>>needs_review
efficient grid search for random forests to get the best performance off random forests it is necessary to tune parameters like `max depth` `min samples split` and `min samples leaf` if we wrap rf in gridsearchcv trees are built from scratch every time however for depth first tree induction deeper trees share the same base as shallower trees an idea to speed up grid search is thus to not rebuild trees from scratch every time here an example for `max depth 10 set `max depth 10` build `n estimators` fully developed trees prune trees to have maximum depth of `max depth` create rf for this `max depth` and evaluate it using the current train test split decrease `max depth` and go to step such an algorithm could be wrapped in `randomforestclassifiercv` `randomforestregressorcv` classes >>>enhancement
failing tests on the armel platform apparently on the armel division by zero is not detected by numpy even within `with np errstate all raise block this makes tests in the sklearn suite fail https buildd debian org status fetch php pkg scikit learn arch armel ver 15 stamp 1410187161>>>bug
hashingvectorizer tfidftransformer fails because of stored zero reported on so http stackoverflow com 25650601 166749 crashes with `valueerror` the problem is that the sparse matrix contains stored zero which becomes inf` in the sublinear tf transformation and that causes normalization to fail the position changing seems to have to do with unsorted indices in the matrix >>>bug
jobs support in gradientboostingclassifier the following loop is embarrassingly parallel https github com scikit learn scikit learn blob master sklearn ensemble gradient boosting py l552 edit ogrisel removed the easy tag and put moderate tag instead based on the discussion below the most beneficial way to add `n jobs` support for gbrt would be deep inside the cython tree code to benefit gb regression and adaboost models as wells instead of just gb classification >>>enhancement moderate
mrg label encoder unseen labels this is pull request to adopt the work done by mjbommar at 3483 this pr intends to make preprocessing labelencoder more friendly for production pipeline usage by adding new labels constructor argument instead of always raising valueerror for unseen new labels in transform labelencoder may be initialized with new labels as none current behavior raise valueerror to remain default behavior update update classes with new ids for new labels and assign an integer value set newly seen labels to have fixed value corresponding to this integer value add classes parameter to transform function classes parameter during initilization >>>needs_review
not saving indices in ensemble py when constructing the estimators in parallel build estimators` the code save which samples where used `estimators samples append samples however in the case of boostraping when one example may be used more than once that information is not stored since so you cant actually reconstruct the estimator glouppe >>>enhancement
labels don stay clamped in labelpropagation in some cases the labels don stay clamped in labelpropagation for example think the problem occurs in label propagation py https github com scikit learn scikit learn blob master sklearn semi supervised label propagation py at line 235 https github com scikit learn scikit learn blob master sklearn semi supervised label propagation py l235 and line 249 https github com scikit learn scikit learn blob master sklearn semi supervised label propagation py l249 when alpha is equal to static is set to in line 235 https github com scikit learn scikit learn blob master sklearn semi supervised label propagation py l235 and then line 249 https github com scikit learn scikit learn blob master sklearn semi supervised label propagation py l249 doesn change self label distributions whereas it should clamp the values of the labelled data points my understanding from the documentation http scikit learn org stable modules label propagation html is that for labelpropagation it is always supposed to do hard clamping and thus it should be completely independent of alpha not actually sure what the intention was here since the same fit method is used for labelspreading >>>bug
enh add parameter pmax that is present in glmnet glmnet has parameter called `pmax` that limits the non zero coefficients which would help lot in the sparse case with l1 norm was wondering if we needed to add this parameter to elasticnet in sklearn >>>enhancement
parallelise nearest neighbors methods the knn data structures kdtree balltree could perform queries in parallel or at least enable multithreading by releasing the gil within their methods >>>enhancement moderate
problem with setup py on osx hi tried to install scikit learn on osx but got this error >>>bug
add balanced accuracy score metrics there have been some discussion about adding balanced accuracy metrics see this article for the definition http en wikipedia org wiki accuracy and precision on the mailing list this is good opportunity for first contribution this implies coding the function checking correctness through tests and highlight your work with documentations in order to be easily used by many balanced accuracy scorer is good idea as bonus it could also support `sample weight` >>>easy new_feature
make catch warnings blocks in tests more robust some tests use with `warnings catch warnings` to check that warning is raised without calling `sklearn utils testing clean warning registry first as python can decide to not raise the same warning twice it makes those tests sensitives to test suite ordering and complicated to understand in case of failure we should review all the existing tests to ensure that we call `clean warning registry` explicitly or even use `sklearn utils testing assert warns instead of manual `with` block the list of tests to review can be found with >>>easy
mrg make labelencoder more friendly to new labels this is final cleanly rebased version of pr 3243 https github com scikit learn scikit learn pull 3243 incorporating discussions summary this pr intends to make ``preprocessing labelencoder`` more friendly for production pipeline usage by adding ``new labels`` constructor argument instead of always raising ``valueerror`` for unseen new labels in transform ``labelencoder`` may be initialized with new labels as `` raise `` current behavior raise ``valueerror`` to remain default behavior `` update `` update classes with new ids `` `` for new labels and assign an integer value set newly seen labels to have fixed value corresponding to this integer value `` classes `` is not property to support the ``new labels update `` behavior tests and documentation updates included >>>needs_review
generalized additive models gams hello there thanks for making this fantastic library use it every day in my bioinformatics research we re developing toolkit for single cell rna seq analysis http github com yeolab flotilla and want to add all current state of the art analyses unfortunately most of these are in can reimplemement some of them but they rely on certain packages in particular vgam http cran project org web packages vgam index html aka vector generalized linear and additive models ve found few mentions of gams here https github com scikit learn scikit learn wiki list of topics for google summer of code gsoc 2012 subtype of gams multiple additive regression mars discussion https github com scikit learn scikit learn issues 845 mars pull request https github com scikit learn scikit learn pull 2285 has there been any update on creating these libraries >>>new_feature
loss function name consistency in gradient boosting it would be nice if the `loss` option in gradient boosting could be more consistent with the one in sgd rather than deprecating names in gradient boosting suggest adding aliases >>>enhancement
explanation of nu parameter in one class svm in the latest documentation for outlier detection http scikit learn org stable modules outlier detection html it mentioned an important distinction between novelty detection and outlier detection is that in novelty detection the training data is not polluted by outliers and in outlier detection the training data contains outliers and in the example one class svm is used to demonstrate novelty detection however in one class svm it is still possible to accept outliers in the training data particularly the parameter nu is used to tune upper bound on the fraction of outliers in the training dataset as explained in proposition of the original paper estimating the support of high dimensional distribution by schlkopf et al so think that the distinction between outlier detection and novelty detection is not well illustrated in the current documentation by the use of one class svm and in fact would think that we should not differentiate between the two cases besides the current explanation of nu parameter the nu parameter also known as the margin of the one class svm corresponds to the probability of finding new but regular observation outside the frontier should be re written based on the explanation from the original paper to make things clearer >>>documentation
allow to choose the out of bag scoring metric estimators in the forest module random forest and extra trees and in the bagging module allows to compute the out of bag estimates of the performance of the forest nice things to add would to allow the choice of the scoring function using the scorer interface the `oob score` parameter would be equal would be the string corresponding to the appropriate scorer thus you would have >>>easy enhancement
add multioutput multiclass support to metrics some estimators such as trees support multi output multiclass however there isn any metric yet to assess those tasks here list of metrics that could be easily extended to handle this format accuracy score or subset accuracy and zero one loss or subset zero one loss hamming loss ideally it would be one pull request for point and >>>easy enhancement
add multi label support to the confusion matrix metric currently the `confusion matrix` support binary and multi class classification but not multi label data yet >>>easy enhancement
add sample weight support to more metrics most metrics now supports `sample weights` except for the following one confusion matrix hamming loss hinge loss jaccard similarity score log loss matthews corrcoef score median absolute error note that there is already general test in `sklearn metrics tests test common py` for `sample weight` ideally it would be one pull request per metric to ease review >>>easy enhancement
add multi output support to the bagging module this would nice to add multi output support to bagging as in the `sklearn ensemble forest` module some code could be refactored >>>easy enhancement
few docs take orders of magnitude longer than others various agglomerative clustering on 2d embedding of digits snip ward 96 95s average 96 07s complete 97 23s time elapsed 2e 02 sec>>>documentation easy
input validation refactoring propose to refactor the input validation the current zoo of methods is kinda confusing related to 3142 checks that we want are numpy array vs sparse vs list vs anything indexable sparse matrix type inf nan dtype ndims number of samples is consistent in multiple arrays contiguous ll now check if there is anything else that we currently check and see what functions we have remaining issues possibly remove make float array and 1d or column and also make these options of check array currently ensure2d makes vectors into rows which find pretty counter intuitive this is for backward compatibility 1d input is not currently handled consistently >>>api
failing tests on 32 bit platforms there are failing tests on 32 bit platforms am using this issue as unifying place for all of the related issues as they have been reported since some were originally thought to be platform linear algebra specific there are variable failures in lle kernelpca and cca which are mentioned in 3255 1632 >>>bug
fix resourcewarning unclosed file when running tests with python running the tests highlights that we do not close file object explicitly in many places of the code in python this raises warnings such as this is probably matter of wrapping the code that manipulates such recently opened file objects in with statement by run the tests with `nosetests sklearn` to find them all >>>easy enhancement
enh add infomaxica object and function we recently added pure numpy implementation of the ica infomax algorithm to mne python https github com mne tools mne python https github com mne tools mne python blob master mne preprocessing infomax py https github com mne tools mne python blob master mne preprocessing tests test infomax py it should not be too difficult to include it in sklearn since we already have tests that are adapted from the fastica tests if people are interested we could start discussing the api cc gaelvaroquaux ogrisel agramfort >>>new_feature
robust covaraince fast mcd does not handle singular covariance matrices when the step of fast mcd encounters covariance matrix with determinant it raises an error according to the paper rousseeuw van driessen 1999 it shouldn be the case technically determinant covariance is the optimal solution singular case requires special treatment section of the paper new here but can contribute to this >>>bug
making kerneldensity parameter of meanshift it would be really great if one could choose sklearn neighbors kerneldensity object to be used by the sklearn cluster meanshift class for my personal use case need to run meanshift with the haversine distance metric on gaussian kernel what do you think >>>new_feature
joblib breaks automatic memory mapping with large arrays multiprocessing pool self value out of range for format code this reproduces question from stackoverflow the following code runs fine with smaller test samples like 10 for samples at least for the number below 10 the run breaks with the ensuing output this is reproducible on 64 bit windows server 2012 with python from anaconda and put the pool py from anaconda multiprocessing package and parallel py from scikit learn external package on my dropbox for reference all packages are the highest version numbers compatible with py2k from repo continuum io pkgs free pro win 64 could not use conda to install consistent install base the test script is import numpy as np import sklearn from sklearn linear model import sgdclassifier from sklearn import grid search import multiprocessing as mp def main print started print numpy np version print sklearn sklearn version samples 1000000 features 1000 train np random randn samples features train np random randint size samples print input data size 3fmb train nbytes 1e6 model sgdclassifier penalty elasticnet iter 10 shuffle true param grid alpha 10 np arange l1 ratio 05 15 95 99 gs grid search gridsearchcv model param grid jobs verbose 100 gs fit train train print gs grid scores if name main mp freeze support main this results in the output vendor continuum analytics inc package mkl message trial mode expires in 28 days started numpy sklearn 15 0b1 input data size 8000 000mb fitting folds for each of 48 candidates totalling 144 fits memmaping shape 1000000l 1000l dtype float64 to new file users laszlos appdata local temp joblib memmaping pool 6172 78765976 6172 284752304 75223296 pkl failed to save to npy file traceback most recent call last file anaconda lib site packages sklearn externals joblib numpy pickle py line 240 in save obj filename self write array obj filename file anaconda lib site packages sklearn externals joblib numpy pickle py line 203 in write array self np save filename array file anaconda lib site packages numpy lib npyio py line 453 in save format write array fid arr file anaconda lib site packages numpy lib format py line 406 in write array array tofile fp valueerror 1000000000 requested and 268435456 written memmaping shape 1000000l 1000l dtype float64 to old file users laszlos appdata local temp joblib memmaping pool 6172 78765976 6172 284752304 75223296 pkl vendor continuum analytics inc package mkl message trial mode expires in 28 days vendor continuum analytics inc package mkl message trial mode expires in 28 days vendor continuum analytics inc package mkl message trial mode expires in 28 days vendor continuum analytics inc package mkl message trial mode expires in 28 days vendor continuum analytics inc package mkl message trial mode expires in 28 days vendor continuum analytics inc package mkl message trial mode expires in 28 days vendor continuum analytics inc package mkl message trial mode expires in 28 days vendor continuum analytics inc package mkl message trial mode expires in 28 days traceback most recent call last file laszlo gridsearch largearray py line 33 in main file laszlo gridsearch largearray py line 28 in main gs fit train train file anaconda lib site packages sklearn grid search py line 597 in fit return self fit parametergrid self param grid file anaconda lib site packages sklearn grid search py line 379 in fit for parameters in parameter iterable file anaconda lib site packages sklearn externals joblib parallel py line 651 in call self retrieve file anaconda lib site packages sklearn externals joblib parallel py line 503 in retrieve self output append job get file anaconda lib multiprocessing pool py line 558 in get raise self value struct error integer out of range for format code https www dropbox com 7xhf5qiw6bpudv4 pool py https www dropbox com p0risowfjwd8pwl parallel py http stackoverflow com 24406937 938408 >>>bug
mrg gsoc 2014 standard extreme learning machines finished implementing the standard extreme learning machines elms am getting the following results with 550 hidden neurons against the digits datasets training accuracy using the logistic activation function 999444 training accuracy using the tanh activation function 000000 fortunately this algorithm is much easier to implement and debug than multi layer perceptron will push test file soon ogrisel larsmans>>>needs_review
support for multi class roc auc scores low priority feature request support for multi class roc auc score calculation in `sklearn metrics` using the one against all methodology would be incredibly useful >>>new_feature
extract the nudge dataset function to sklearn feature extraction image the nudge dataset https github com scikit learn scikit learn blob master examples plot rbm logistic classification py l48 function would benefit from being extracted as helper function with an additional parameter to configure the shape it should probably be renamed to `nudge samples 2d` to highlight the fact that it interprets samples as 2d images this function will then be reusable to build examples for instance for the upcoming mlp class on larger versions of the digits without having to download large dataset such as mnist >>>new_feature
partial auc suggest adding partial auc to the metrics this would compute the area under the curve up to specified fpr in the case of the roc curve this measure is important for comparing classifiers in cases where fpr is much more important than tpr the partial auc should also allow applying the mcclish correction see here http cran project org web packages proc proc pdf>>>new_feature
prediction variance in bagging based regressors it would be nice if we could do same for baggingregressor >>>easy enhancement
kerneldensity docstring had troubles understanding how parameters related to kd tree and ball tree affect densityestimation in my understanding kde uses the average evaluation of kernels centered on every training point so didn quite understand how kd tree and ball tree are useful would understand if it used say the average of the nearest neighbors only the docstring says that we can specify tolerance options `rtol` and `atol` with respect to what stopping criterion are these constants used regarding the breadth first option the docstring says use breadth first approach to the problem didn understand what problem it refers to what is the practical impact of of the tree related parameters do they only affect speed or can they also affect quality of estimation sorry for the naive questions but it would help my understanding if we could add couple of words to clarify the docstring btw thanks jakevdp for this great module >>>documentation
enh support unlabeled output in label propagation currently when the label propagation algorithm https github com scikit learn scikit learn blob master sklearn semi supervised label propagation py l244 hits `max iter` it possible for some rows of `self label distributions to be entirely zero this makes sense because labels haven yet propagated to those samples unfortunately during the normalization step this turns those label distributions into nan values which results in the first label being assigned `np argmax nan array 0` this also issues runtimewarning about the divide by zero believe it would be better to give the user an option in this case perhaps to set these labels to to indicate no label similar to the input format set these labels to some user supplied value perhaps defaulting to to preserve behavior currently only the warning gives any indication that some labels may not have propagated fully this is awkward to catch in user code and there no easy way to determine which samples failed to be labeled if this makes sense ll be happy to provide pr implementing the change >>>enhancement
kmeans transform should allow metric optional parameter inspired by this http fastml com the secret of the big guys looking to contribute to scikit is it ok if work on this >>>enhancement
transformers need to be explained better in tutorial material there just appeared yet another so question http stackoverflow com questions 23899057 saving and using tfidf vectorizer for future examples about the tf idf transformer and how to use it since this comes up again and again and again think our documentation needs improving currently `tfidftransformer fit transform` has the description learn the representation and return the vectors while `transform` has transform raw text documents to tf idf vectors which is bit better think we need to explain the concept of transformer in the text processing tutorial because ml novices don seem to get it not too surprisingly since most other tools either hide the vectorization from the user or require them to hack it up themselves >>>documentation
build errors with clang had an issue compiling sklearn on my new mac the issue has to do with clang and is documented elsewhere http stackoverflow com questions 22313407 clang error unknown argument mno fused madd python package installation fa the issue is that clang doesn like the shorten 64 to 32 cpp flag the build crashes when trying to compile libsvm in order to compile had to do export archflags wno error unused command line argument hard error in future sudo python setup py install edit it finished building with the previous fix but it crashes when use sklearn in python terminal >>>bug build_/_ci
cross val score parallel hangs with large matrices note opened this in joblib as well because not sure which repo would be better to report this in since this occurs with relatively simple use case in sklearn posting this here in case it more appropriate let me know if you want me to move take one of them down recently my processes have started freezing when run models with large ish data and not sure why unfortunately can follow this back to specific change that might have made basically this works fine while running the following code results in the forked processes hanging what happens is that first number of processes spawn off and they churn away at the data for while this is top after few seconds of running the above code image https cloud githubusercontent com assets 1839645 3073244 38cd08a2 e2e1 11e3 9b40 728a6a7aec22 png however after another 10 seconds or so these processes have finished and another set of processes are created that hang image https cloud githubusercontent com assets 1839645 3073251 5c8cb062 e2e1 11e3 9993 a804e6302a36 png you can see the processes that spawned off and that none of them are chewing up any cpu time it remains in this state indefinitely thought this might be problem with joblib trying to memmap things but both matrices are well over the max nbytes default for parallel at least according to nbytes note that these matrices and ones larger than them have worked totally fine in the past for fitting these kinds of models not really sure what going on using sklearn version 14 joblib version 0a3 though this also breaks on all packages linked against mkl though tried it after removing mkl in anaconda and it still hangs unix machine centos >>>bug
winsorization trimming in preprocessing useful preprocessing step especially when doing prototyping is to winsorize trim or clip the data to some hard limit ve searched through the repo and mailing list but don see anything like this currently implemented or proposed be willing to submit patch if there support for this it would provide both standalone method and class that follows the transformer api >>>new_feature
mrg added kernel weighting functions for neighbors classes this patch enables the use of kernel functions for neighbors weighting it adds the following keywords for ``weights`` argument ``tophat`` ``gaussian`` ``epanechnikov`` ``exponential`` ``linear`` ``cosine`` all kernels presented in ``kerneldensity`` class for ``kneighborsclassifier`` and ``kneighborsregressor`` the kernel bandwidth is equal to the distance to the nearest neighbor it depends on query point for ``radiusneighborsclassifier`` and `` radiusneighborsregressor`` the kernel bandwidth is equal to the radius parameter of the classifier it is constant please take look >>>needs_review
deadlock using gridsearchcv with randomforestclassifier gist https gist github com gatapia 11363090 data can be downloaded from http oem picnet com au all data zip rename zip to bz2 basically as long as the grid search takes long enough dead lock will eventually occur during grid search have had as few as 25 out of 32 cpus being used jobs 25 and no memory pressure on those processes sometimes this message is displayed on the console error in usr bin python realloc invalid pointer 0x00007fa3bf253a90 but not always and deadlock can occur without this happening ctrl to prints following thread dump just top printed cv max features 39 min samples split estimators 470 max depth 23 min samples leaf 49 9s cv max features 39 min samples split estimators 475 max depth 23 min samples leaf 49 0s parallel jobs 30 done 105 out of 108 elapsed 7min remaining 3s cv max features 39 min samples split estimators 475 max depth 23 min samples leaf 51 8s cv max features 39 min samples split estimators 470 max depth 23 min samples leaf 55 1s cv max features 39 min samples split estimators 480 max depth 23 min samples leaf 0min cv max features 39 min samples split estimators 480 max depth 23 min samples leaf 51 5s cv max features 39 min samples split estimators 475 max depth 23 min samples leaf 1min cv max features 39 min samples split estimators 480 max depth 23 min samples leaf 53 7s cv max features 39 min samples split estimators 480 max depth 23 min samples leaf 1min cprocess poolworker 18 process poolworker 13 process poolworker 12 process poolworker 10 process poolworker 26 process poolworker 21 process poolworker 16 process poolworker traceback most recent call last traceback most recent call last traceback most recent call last traceback most recent call last traceback most recent call last traceback most recent call last traceback most recent call last process poolworker 33 file usr lib python2 multiprocessing process py line 258 in bootstrap file usr lib python2 multiprocessing process py line 258 in bootstrap file usr lib python2 multiprocessing process py line 258 in bootstrap file usr lib python2 multiprocessing process py line 258 in bootstrap file usr lib python2 multiprocessing process py line 258 in bootstrap file usr lib python2 multiprocessing process py line 258 in bootstrap traceback most recent call last file usr lib python2 multiprocessing process py line 258 in bootstrap file usr lib python2 multiprocessing process py line 258 in bootstrap traceback most recent call last file usr lib python2 multiprocessing process py line 258 in bootstrap process poolworker 29 process poolworker 22 process poolworker 24 ps aux showing no real server activity during deadlock image https cloud githubusercontent com assets 135965 2813976 35f9d63e ce9e 11e3 9d5c 663cf0fc44f4 png >>>bug
mrg optional whitespace normalisation for countvectorizer analyzer char currently getting character grams from countvectorizer or variants automatically normalises whitespace and there is not current option to turn it off this pr adds this option pretty basic pr here `normalize whitespace` option to countvectorizer and variants true by default which matches current behaviour when set to false the normalisation that occurred in `countvectorizer char ngrams` no longer happens current todo test case narrative documentation updates if applicable>>>needs_review
feature request stability of feature subset selection one possible criterion to judge the outcome of feature subset selection is how much the selection depends on the training set the stability estimator runs the feature selection on different subsamples it then computes the cumulative similarity of the selected feature subsets two possible measures for this similarity are the jaccard similarity or kunchevas index see saeys yvan thomas abeel and yves van de peer robust feature selection using ensemble feature selection techniques machine learning and knowledge discovery in databases springer berlin heidelberg 2008 313 325 kunchevas index kuncheva ludmila stability index for feature selection artificial intelligence and applications 2007 edit sorry but it forgot to set label for this issue apparently cannot relabel this once posted >>>new_feature
normalize only applies if fit intercept true number of linear models have `normalize` option to scale all features to equal variance however this only has any effect if `fit intercept true` this interaction does not appear to be documented so this issue requests that one of the following actions be taken the `fit intercept true` requirement should be noted in the comment for `normalize` or `normalize` should apply when `fit intercept false` assuming the data is already centered>>>api bug documentation
imputation by knn adding new strategy knn in sklearn preprocessing imputer class for imputing the missing values usign knn method >>>new_feature
precision recall curve does not take the pos label into account the method metrics precision recall curve has parameter that can be used to define the label of the positive class this parameter is however never used which result in wrong values of precision and recall the problem can be fixed by changing the line 273 of metrics metrics py from to the documentation of the method should also be changed to include the description of the parameter >>>enhancement
adaboost samme algorithm uses predict while fitting and predict proba while predicting probas subj this seems to me to be wrong approach moreover this drives to such mistakes adaboostclassifier algorithm samme base estimator svc fit trainx trainy predict proba testx notimplementederror traceback most recent call last in adaboostclassifier algorithm samme base estimator svc fit trainx trainy predict proba testx library python site packages sklearn ensemble weight boosting pyc in predict proba self 716 proba sum estimator predict proba 717 for estimator in zip self estimators 718 self estimator weights 719 720 proba self estimator weights sum library python site packages sklearn ensemble weight boosting pyc in estimator 715 else self algorithm samme 716 proba sum estimator predict proba 717 for estimator in zip self estimators 718 self estimator weights 719 library python site packages sklearn svm base pyc in predict proba self 493 if not self probability 494 raise notimplementederror 495 probability estimates must be enabled to use this method 496 497 if self impl not in svc nu svc notimplementederror probability estimates must be enabled to use this method >>>enhancement question
trees incompatible between 32bit and 64bit version not sure if this known issue but think trees build on 64bit can not be unpickled on 32bit how hard would it be to allow that think the problem is that ``size t`` is different between the two platforms >>>enhancement
update cython code to support 64 bit indexed sparse inputs in scipy master to be released as 14 scipy sparse matrices can now be indexed with 64 bit integers https github com scipy scipy blob master doc release 14 notes rst scipysparse improvements this means that we will probably need to use fused types for `indptr` and `indices` arrays whenever we deal with csc or csr datastructures in our cython code base >>>enhancement
assertionerror arrays are not almost equal during test of fresh install 14 the test suite fails with this errors see below any help will be appreciated >>>bug
allow imputer to accept strategy some callable it seems like `imputer` can be easily extended to handle arbitrary axis wise strategies at least in the case where the data is dense or sparse with non explicit entries to be imputed the `strategy` parameter could accept function or other callable that reduces given masked array along an axis including scipy stats mstats http docs scipy org doc scipy reference stats mstats html functions more flexibly `strategy` could be callable that either takes 2d array and an `axis` value and returns 1d array or just takes 1d array and returns scalar for example one should be able to use to impute with trimmed mean using absolute bounds nb `tmean` http docs scipy org doc scipy reference generated scipy stats mstats tmean html doesn accept an `axis` argument or for trimmed mean with quantile bounds >>>moderate new_feature
add more useful example to the cluster comparison the clustering comparison at http scikit learn org stable auto examples cluster plot cluster comparison html is somewhat misleading in that the data are totally unlike anything that would be seen in 99 of cases realise that they re toy examples but it would also be good to get something more realistic for comparison here is simple dataset that has one wide gaussian distribution with two smaller gaussian distributions overlapping it to different extents clustering with gaussians https cloud github com assets 167164 2252300 9770a114 9da6 11e3 87b1 d0efbefb0ed8 png this shows the performance of the various models on more realistic data it especially shows that dbscan isn perfect here the dataset the parameters are all arbitrary and the performances for the various algorithms change fair bit given different parameters but since this is just to give rough idea of the relative performances don think that matters that much >>>documentation enhancement
sample weights array can be used with gridsearchcv the internal cross validation isn aware of sample weights so and exception is thrown if sample weights sequence is passed to the grid search because fit grid point does not split the weights into training and test sets >>>api enhancement
gbrt monitor held out set was toying with the new monitor callback and ran into an issue not sure if using it incorrectly or if it bug so have something like the following which results in array decisiontreeregressor compute importances none criterion max depth max features none max leaf nodes none min density none min samples leaf min samples split random state splitter none none none >>>enhancement
odd cca scale true behavior this is probably just result of an misunderstanding of mine but anyway expect that if pass in scaled data setting `scale true` or `scale false` on `cca` should have no effect while that true for the first component it not true for the second component >>>bug
add support for svdd one class svm support vector data description svdd could be nice enhancement to oneclasssvm implementation technical implementation is described in this paper http www csie ntu edu tw cjlin papers svdd pdf source code compatible with libsvm is available here http www csie ntu edu tw cjlin libsvmtools libsvm for svdd and finding the smallest sphere containing all data>>>enhancement
overflow in matthews corrcoef on 32 bit numpy example this runs fine on 64 bit unix box but not on 32 bit windows machine due to overflows and afaik getting 64 bit numpy to work on windows is hard next to impossble >>>bug
doc more docs needed on model and data persistence it seems the only documentation of model persistence is in the quick start tutorial think it needs to be covered in the user guide mentioning security and forward compatibility caveats pertaining to pickle it might note the benefits of joblib for large models at least over pickle most other ml toolkits particularly command line tools treat persistence as very basic operation of the package this was fixed in 3317 similarly there should be some comment on saving and loading custom data input and output indeed can find direct description of the supported data types users are unlikely to have played with `scipy sparse` before although the `feature extraction` module means they may not need to noting the benefits of joblib without which the user may dump sparse matrix `data` `indices` `indptr` using tofile` and memmapping is worthwhile so may be reference to pandas which could help manipulate datasets before after entering scikit learn and provides import export to variety of formats http pandas pydata org pandas docs dev io html have recently discovered new users who think the way to import export large sparse arrays is `load svmlight format` but then note that the loading saving takes much more time than the processing they re trying to do let give them hand >>>documentation easy
add option to isomap for using precomputed neighborhood graph theoretically there no reason why `isomap` can operate on an abitrary graph but the existing implementation assumes nearest neighbor graph adding an option for using precomputed graph would match the api for `spectralembedding` which can accept precomputed affinity matrix in addition to using standard methods for its construction >>>enhancement
baseestimator get params and clone thread safe are not thread safe the handling of deprecated constructor parameters is leveraging the execution python warnings machinery which is not thread safe and could therefore caused hard to diagnose bugs in there was tentative lock based workaround in 2729 better solution however would avoid executing the warning machinery at all in `get params` by leveraging declarative deprecation introspection possibly using class decorator will try to issue pr for this next week >>>bug
correctness issue in lassolars for unluckily aligned values think ve found correctness bug in sklearn linear model lassolars for me it appears only for systems with some exactly aligned non general position values it can be fixed by jiggling the rhs of the system with small random offsets here is test python code import numpy as np import sklearn linear model as sklm np array np array lars sklm lassolars alpha 001 fit intercept false lars fit nojiggle lars coef jiggle np random rand 00001 lars fit jiggle jiggle lars coef print without jiggle nojiggle residual np dot nojiggle print with jiggle jiggle residual np dot jiggle for me with the current anaconda distribution sklearn version 14 the output is without jiggle 998 498 residual 00000000e 03 49800000e 00 with jiggle 49799528 49799561 residual 00200439 00200472 the jiggled version has the expected result whereas the no jiggle version is wrong >>>bug
roc auc score fails if dtype is object to reproduce import numpy as np np random seed 13 classes np array yes no classes np random randint size 10 classes np random randint size 10 from sklearn metrics import roc auc score from sklearn metrics import accuracy score accuracy score 800 roc auc score valueerror data is not binary and pos label is not specified >>>enhancement
add `best score attribute to all cv models gridsearchcv has `best score attribute that is missing from ridgecv lassocv and other cv based objects ideally these would all have consistent api since they re all performing essentially the same task from given set of parameters search for the optimal one according to the cross validation score other attributes that may be beneficial to add `best estimator this may be useful by returning fit object of the non cv version of the class ridgecv ridge `best params this would be helpful especially for elasticnetcv which has multiple parameters set `alpha and `l1 ratio `grid scores this is mostly available for lassocv but will take more work to make available for ridgecv if no one objects ll start working on this ideas for how to improve or other attributes to add are welcome >>>enhancement
gbt fails with rf init here is tiny script which reproduces the crash it also seems that the init param in gradientboostingclassifier is not really tested pprett glouppe ogrisel>>>bug
add varimax rotation for factor analysis and pca rotation methods such as varimax should be added to pca it helps identify the factors that make up the components and would be useful in analysis of data https en wikipedia org wiki varimax rotation http sites stat psu edu ajw13 stat505 fa06 17 factor 13 factor varimax html>>>new_feature
possibly inefficient sparse matrix creation in featurehasher hi it looks like the way that `csr matrix` objects are created by `featurehasher` can result in outputs that explicitly represent lot of 0s `featurehasher` uses the tuple based initialization for `csr matrix` which will just take 0s and store them rather than trying to filter them out for efficient storage think the simplest way to address this is to call `eliminate zeros on the matrix before returning it however not sure this would be efficient enough for very large datasets it might be better to construct one row at time call `eliminate zeros and then stack them or something like that >>>enhancement
add additional bandwidth settings to kerneldensity hi the new kerneldensity class is really great there are few things might like to see added in future releases per dimension bandwidth control like to be able to pass in an array of bandwidths and apply different smoothing in each dimension adaptive smoothing like to be able to apply variable smoothing such that the bandwidth is chosen equal to the kth nearest neighbor distance at each sample point does anyone have an idea of how easy possible these features might be to implement had look through the code but don know enough about tree based evaluation to tell if it could support these functions thanks >>>new_feature
use succinct tries for vocabulary storage hey python dictionary is quite wasteful for string data so `countvectorizer` `tfidfvectorizer` and `dictvectorizer` all could take much less memory with an another data structure for `vocabulary here is quick prototype that persists `vocabulary of `countvectorizer` and `tfidfvectorizer` in marisa trie https code google com marisa trie via python wrapper https github com kmike marisa trie https gist github com kmike 7814472 used this https gist github com kmike 7813450 script to measure memory and speed of `fit` and `dump` and this https gist github com kmike 7815149 script to measure parameters of `load` and `transform` the results are quite cool for 20 newsgroups data fit on training subset transform on test subset memory consumption of different vectorizers after loading is the following `countvectorizer 94mb `countvectorizer ngram range 666mb `marisacountvectorizer 2mb `marisacountvectorizer ngram range 13 3mb so using good succinct trie implementation gives us 50x 80x reduction of memory usage it also makes serializing and deserializing almost instant and such vectorizers don have hashingvectorizer limitations the downside is that `fit` method is 2x 3x slower and requires slightly more memory but think it could be changed to require less memory than countvectorizer fit and `transform` is about 2x slower full output https gist github com kmike 7815156 what do you think about adding something similar to scikit learn >>>new_feature
add support for ml knn add support for the multi label knn algorithm as described here in this paper http cs nju edu cn zhouzh zhouzh files publication pr07 pdf brief description of the algorithm from the above paper as its name implied ml knn is derived from the popular nearest neighbor knn algorithm firstly for each test instance its nearest neighbors in the training set are identied then according to statistical information gained from the label sets of these neighboring instances the number of neighboring instances belonging to each possible class maximum posteriori map principle is utilized to determine the label set for the test instance supporting sparse matrices should be key requirement lot of multi label tasks are for text categorization and these are usually represented as sparse matrices ml knn is actually already implemented in this library http orange biolab si docs latest reference rst orange multilabel ml knn learner but it would be good to bring it under the scikit learn framework as well >>>enhancement
unexpected normalization in the sklearn linear models base center data function when normalize true and fit intercept true is provided the standard deviation of is calculated by std np sqrt np sum axis think it should rather read std np sqrt np mean axis or is there any special reason why you sum here intead of taking the mean if you just sum then while is increased in dimension std will grow also to infinity this seems odd to me and quite unexpected >>>bug
cross validation generators broken for semi supervised learning `labelpropagation` encodes unlabeled samples with 1` those samples cannot be used for testing cross validation generators should have an option such that certain label always goes to the training split >>>enhancement
mrg pipeline can now be sliced or indexed this pr offers an alternative to 2561 and 2562 making it easy to apply inverse transforms or transforms over only sub sequence of steps in pipeline thus schwarty is this sufficient for your needs >>>needs_review
adding get estimated method to pipelines the motivation is the same as pr 2561 in nutshell we want to be able to apply the inverse transform steps from pipeline on an estimated parameter from the last step of the same pipeline pr 2561 aims to achieve that goal by applying the inverse transforms from all steps except the last one if it misses an inverse transform method gaelvaroquaux pointed out that it will fail in the cases where the last estimator implements both transform inverse transform methods and predict score methods the alternative could be to implement get estimated method for pipelines that explicitly retrieves given attribute from the pipeline last step and subsequently applies the inverse transforms the code would look like the following note that the final learner may itself be pipeline gridsearchcv metaclassifier or any valid combination in sklearn any comments suggestions >>>api
port fast better ard regression from mne python https github com mne tools mne python blob master mne inverse sparse gamma map py l17 to replace historical ardregression model this issue is just reminder for me cc mluessi>>>enhancement moderate
mrg support for distance matrices in nearestneighbor support for nearestneighbor the unsupervised one to handle being given distance matrix matrix such that is the distance between samples and >>>needs_review
mrg sklearn tree export dict for converting tree objects into jsonable format this is useful for viewing tree objects in terminal and working with tree objects in simpler more pythonic form than working directly with tree tree one example would be writing utility which can use the pythonic tree to tell the user why particular decision was made rather than just generating prediction this is especially useful for decisiontrees which are often enjoyed in this simple form for their very transparency >>>needs_review
bi coclustering api prevents scalability the biclustering and coclustering estimators promise to store boolean arrays `rows and `columns of size `n clusters samples` and `n clusters features` respectively and convert these to indices only on demand for use with large sparse matrices and large numbers of clusters these should be arrays of indices rather than boolean masks >>>enhancement moderate
bug gmm ``score `` returns an array not value the ``gmm score `` function returns an array rather than single value this is inconsistent with the rest of scikit learn for example both ``sklearn base classifiermixin`` and ``sklearn base regressormixin`` implement ``score `` function which returns single number as do ``kmeans`` ``kerneldensity`` ``pca`` ``gaussianhmm`` and others currently ``gmm score `` returns an array of the individual scores for each sample this should probably be called ``gmm score samples `` and ``gmm score `` should return ``sum gmm score samples `` note that in the last release we renamed ``gmm eval `` to ``gmm score samples `` believe this was mistake the ``score samples`` label has very general meaning it is used within ``kerneldensity`` while the results of ``gmm eval `` return tuple containing the per cluster likelihoods which makes sense only with gmm if this change were made so that ``gmm score `` returned single number then the following recipe would work to optimize gmm model as it does for kde http scikit learn org stable auto examples neighbors plot digits kde sampling html as it is this recipe fails for gmm the result >>>bug
mrg generative classification this pr adds simple meta estimator which accepts any generative model normal approximation ``gmm`` ``kerneldensity`` etc and uses it to construct generative bayesian classifier todo code documentation narrative docs testing examples allow class wise cross validation for the density model >>>needs_review
wip label power set multilabel classification strategy add one of the simplest and common multi label classification strategy which use multi class classifier as base estimator the core code is functional but there is still things to do add some word about binary relevance in ovr narrative doc write narrative doc about lp add some references add some regression tests making your remark about overfitting bit more explicit maybe >>>needs_review new_feature
scaling kills dpgmm was mixture dpgmm not fitting to data am trying out the gaussian mixture models in the package tried to model mixture with two components 1000 500 and 2000 600 the following is the code data np random normal 1000 500 1000 data2 np random normal 2000 600 1000 data list data list data2 model mixture dpgmm components 10 alpha 10 iter 10000 model fit data print model means and got the following means of the components 13436485 13199086 11750537 10560644 12162311 00204134 12058521 11997703 11944384 11890694 it seems the model does not fit properly to the data is it bug or have got something wrong in the application of the model thanks fan >>>bug
mrg bicluster metrics improvements and additions to the bicluster metrics module todo refactor hungarian matching code let lave this for later vlad tests full code coverage implement gene match score update documentation to explain new functionality implement size bias correction implement other similarity metrics dice and goodness measure >>>needs_review
multi label and multi output multi class decision functions and predict proba aren consistent the `decision function` and `predict proba` of multi label classifier `onevsrestclassifier` is 2d arrays where each column correspond to label and each row correspond to sample added in 14 the `decision function` and `predict proba` of multi output multi class classifier `randomforestclassifier` is list of length equal to the number of output with multi class decision function or predict proba output 2d array where each row corresponds to the samples and where each columns correspond to class so this means that multi output problem with only binary class output is multi label task but isn consistent with the multi label format this is problematic if you want to code `roc auc score` function to support multi label output >>>api
mse is negative when returned by cross val score the mean square error returned by sklearn cross validation cross val score is always negative while being designed decision so that the output of this function can be used for maximization given some hyperparameters it extremely confusing when using cross val score directly at least asked myself how the mean of square can possibly be negative and thought that cross val score was not working correctly or did not use the supplied metric only after digging in the sklearn source code realized that the sign was flipped this behavior is mentioned in make scorer in scorer py however it not mentioned in cross val score and think it should be because otherwise it makes people think that cross val score is not working correctly >>>documentation
univariate selection api issues think the univariate selection api is bit unfriendly and doesn follow the project philosophy flat is better than nested my main concern is that we shouldn require the user to import callables such as classif regression and chi2 just using string would do the job as far as can see regression is the only function with an option center it can be made an option of the feature selector find the names classif and regression quite inexpressive are there better alternatives also noticed that there is genericunivariateselect class but it not documented in the user guide >>>api enhancement
changes in ridge py break usage with large sparse matrices following code runs fine on 13 fails with memoryerror on 14 import numpy import scipy sparse 50000 10 scipy sparse rand density 001 numpy random random from sklearn linear model import ridgecv mod ridgecv mod fit >>>bug
minor website tweaks for 14 release aka urgent add link to the citation page on the front page say next to the donate button make carousel with few more examples on the front page later display some sponsors on the front page clean up the examples page that has lot of embedded css that should be fixed by giving proper css class to elements rst has directive class believe to do that make the blue next previous buttons not sliding and fixed at the bottom of the page>>>documentation
fix ml flowchart cut off at bottom without javascript hack after the website facelift the machine learning cheat sheet got cut off at the bottom due to some jquery script interfering its been fixed temporarily for the sake of the online website using quick javascript fix but should ideally be done in css without resorting to script will look into it at later point if nobody else jumps on it>>>documentation
website alignment broken when disabling certain scripts use browser plugin that disables most cross site extensions that can track you on the scikit learn website it finds google analytics and google ajax search api believe the latter being disabled causes the header to be less tall than it should be and the body to shift bit tagging this as enhancement because if you install such blockers you basically void your warranty and shouldn expect greatness >>>enhancement
mrg nudge images to enhance small datasets this pr is just to do something that we did in examples plot rbm logistic classification py at https github com vene scikit learn commit f37668f148e4460d3eb6892313845f8975e6c95a less readably but faster maybe it could be even faster if we did not ask for samples but guess this is convenient to have should check arrays for inputs example >>>needs_review
doc upgrade jquery to version 10 as discussed in the website redo pr https github com scikit learn scikit learn pull 2201 jquery has been updated from the ancient it was working on to however moving any more recent than that creates problems with the collapsible table of contents on the userguide page it would be useful if somebody can figure out what problem is being caused by the change in the `toggle function in jquery between version and ll tend to this at later stage if nobody has time >>>documentation enhancement
wip fix naive bayes coefficient stuff this fixes 2237 and 2240 not yet mrg and 14 rc because not sure if there good way to preserve backward compat and some more refactoring needs to be done >>>needs_review
common tests for feature selection maybe using `` learntselectormixin``>>>enhancement
mrg added code for sklearn preprocessing rankscaler wrote code for doing rank scaling this scaling technique is more robust than standardscaler unit variance zero mean believe that scale is the wrong term for this operation it actually feature normalization this name conflicts with the normalize method though wrote documentation and tests however was unable to get the doc suite or test suite to build for the current sklearn head so couldn double check all my documentation and tests >>>needs_review
norm inconsistency between rfe and selectfrommodel was learntselectormixin in each rfe iteration the `step` features with the lowest importance are discarded https github com djv scikit learn blob master sklearn feature selection rfe py l158 similar thing happens https github com djv scikit learn blob master sklearn feature selection from model py l100 in `selectfrommodel` but selection is by threshold rather than by number of features there are some inconsistencies the mixin admits https github com djv scikit learn blob master sklearn feature selection from model py l44 either `est coef or `est feature importances as the basis of its calculation rfe only `coef fixed in 4496 the mixin uses `np abs est coef while rfe uses `safe sqr est coef the result is the same for selecting features by quantity as long as `coef is 1d but where 2d the sum is taken over axis and the ordering under l1 and l2 norms may differ should rfe support `feature importances for `coef ndim 2` should `selectfrommodel` use `sqrt sqr coef sum axis glouppe think >>>easy
inconsistency between precision recall curve and roc curve the axis and axis for precision recall curve and roc curve are reversed which find confusing bothering for roc curve for precision recall curve not sure if there anything we can do about it without breaking the api >>>enhancement
pls partial least square algorithms occasionally failing attempting to use the cca class in pls py getting some unhelpful errors the example given in the documentation is from sklearn pls import plscanonical plsregression cca 11 12 cca cca components cca fit this works okay and also works okay with the change cca cca components however if is changed to change in last value then the code gives the error valueerror array must not contain infs or nans apologise if am just misusing the function but this error was coming up for me in less contrived example this is using the current master on github >>>bug
enh allow feature agglomeration with any clustering method the ward clusterer is accompanied by `wardagglomeration` which reduces the feature space by pooling `mean` `max` `pca` the values of those that are close in the sample space the implementation only involves fitting on `x t` rather than `x` and inheriting from `agglomerationtransform` to provide `transform` and `inverse transform` methods though the `pooling func` argument to `transform` should probably be an estimator level property it would seem sensible to provide equivalent agglomeration functionality for other clusterers it could be implemented as meta estimator but scikit learn may want to keep the api simpler >>>easy enhancement
add hoverintent effect to the gallery upgrade in 2017 although the gallery in 2017 works well if one for some reason swooshes the mouse pointer about in crazy fashion over the gallery items it can sometimes happen that one or two stay open which goes away once moused over and off again it nothing serious and doesn break or impede the functionality of the gallery however this can be avoided with something like jquery hoverintent http cherne net brian resources jquery hoverintent html not saying that is exactly what will fix it but it more of just reference for what needs to be done in nut shell the thumbnail should only expand due to mouseover when the mouse has stopped on top of it ll look into this bit later if nobody spots and does nice solution in the meanwhile>>>documentation enhancement
shift around docstrings in certain examples this goes with 2017 any type of `rst` code like for example links or math code must be shifted little further down in the docstring as to keep the first 95 characters as normal text this will allow the docstrings to stay legible as the rst doesn render there it just text when seen in the examples gallery will do this shortly if no newcomer grabs it >>>easy
extend `featureunion` to better handle heterogeneous data `featureunion` currently passes identical data to each constituent transformer often one wants to differentiate between groups of features in how they are transformed while this is possible by making each stacked transformer `pipeline` consisting of pre determined feature selector and another transformer this is cumbersome parameter should be added to specify which features are routed to which constituents this is not necessarily trivial to design particularly because the input `x` to `featureunion transform` need not be conventional 2d feature array it may be list array of dicts texts or other objects >>>moderate new_feature
bug need to ensure classification metrics are sane under non stratified cross validation where dataset is split up and not all evaluated at once some classes may be missing from evaluation metrics implementations get around problems relating to classes appearing not in both the `y true` and `y pred` by considering the union of their labels however this is insufficient if label that existed in the training set for fold is absent from both the predicted and true test targets this is at least problem for the family of metrics with `average macro and `labels` unspecified and it should be documented though user shouldn be using macro if there are infrequent labels haven thought yet about whether it is an issue elsewhere or whether it can be reasonably tested >>>bug
enh should be able to ignore majority class in the multiclass case are famous for handling class imbalance in the binary classification case correct me if wrong arjoly but imbalance against majority negative class should also be handled in the multiclass case in particular while the documentation currently states that micro averaged this is not true of the case where negative class is ignored but it should be possible to ignore negative class for any of the `average` settings indeed think the `pos label` argument is mistake except in that you can more reliably provide default value than for `neg label` it only applies to the binary case and overrides the average setting `neg label` would apply to all multiclass averaging methods it should be easy to implement treat the problem as multilabel and delete the `neg label` column from the label indicator matrix it is the case where each instance is assigned or label the tricky part is the interface should `pos label` be deprecated deprecation makes sense as `pos label` and `neg label` should not be necessary together but if so how do we ensure the binary case works by default >>>enhancement
doc explain when to use which metric glouppe wrote https github com scikit learn scikit learn pull 1945 issuecomment 17707603 the model evaluation page http scikit learn org dev modules model evaluation html in the narrative documentation is getting quite big and have the impression that it is becoming copy of the reference documentation it lists bunch of metrics explains what they are but not really what they are for think it would be valuable to add some insights for the users in which cases one should prefer metric over the other arjoly replied yes this is something missing more examples are needed to illustrate the metrics module and others should rewritten to give more insight to our user moreover something similar to the clustering metrics documentation http scikit learn org dev modules clustering html clustering evaluation would be great addition to the documentation >>>documentation
mrg blockwise parallel silhouette computation this pull request introduces blockwise computation of the silhouette using less memory but slightly slower first the vocabulary can be debated have chosen the word global for the original silhouette strategy as the global distance matrix is computed could not come with better word and blockwise for my method the other point is that have implementations of the silhouette the original one blockwise single threaded version blockwise multi threaded version which uses bit more memory than the single threaded version even if jobs have decided to leave the original version untouched as the code is far more readable than mine then have not kept the most efficient blockwise single threaded version because the memory gain is not worth the code complication think it is in fact possible to have one implementation to rule them all this could be done by keeping the same code skeleton and using precomputed distance matrix for the original version but think that the code would become too opaque >>>needs_review
enh move non data transform parameters to the object to be used sensibly in `pipeline` parameter search an estimator `transform` method can have parameters other than the data though it may be possible in the future to support more than one data argument rather it should be possible to set the parameter using the estimator `set params` method as used in parameter search hence additional arguments to `transform` should become object parameters though whether they should be made unavailable as arguments directly to transform is up for debate unfortunately these happen to appear in mixin classes meaning lot of sub classes docstrings need altering examples of additional arguments https github com scikit learn scikit learn blob 71d00d94f48f2131ff7fe661c1069ec234c92305 sklearn cluster feature agglomeration py l22 https github com scikit learn scikit learn blob master sklearn feature selection from model py l20 should have `selection threshold` parameter on object>>>easy enhancement
plot lena segmentation is super slow don remember this example taking so long before anyone know why it may have gotten slower please give it run maybe it just on my box ll look into more soon just thought report it in the meanwhile >>>bug
maint some transform methods accept unused argument `transformer`s generally work with single data argument often `x` and sometimes additional parameters as far as can see whenever `x` and `y` are both arguments to `transform` the `y` goes unused to make the api clearer all such instances of `y` should be removed though perhaps deprecated see http scikit learn org dev developers deprecation to avoid `typeerror`s where users have passed in value for `y` redundantly >>>easy
cross validation documentation error computing cross validated metrics print accuracy 2f 2f scores mean scores std shouldn the above be scores std if the intention is to show the 95 confidence level thanks >>>documentation easy
mrg fixed precompute false error in randomizedlasso >>>bug needs_review
precompute false option in randomizedlasso throws exception in lars path noticed this in 12 but it appears to be unchanged in the current code the documentation for randomizedlasso says that precompute can be true false or auto however when tried to explicitly pass false got an exception the reason is because the value for precompute is passed straight into lars path as the value for gram which takes different set of values none auto matrix or shape so there an exception when lars path assumes it has matrix example demonstrating the error not sure what the interaction between randomizedlasso randomized lasso and lars path should be presumably the interface to lars path should not change so the fix should be to pass none instead of false when it gets called >>>bug
cross validation returning multiple scores `scorer` objects currently provide an interface that returns scalar score given an estimator and test data this is necessary for searchcv` to calculate mean score across folds and determine the best score among parameters this is very hampering in terms of the diagnostic information available from cross fold validation or parameter exploration which one can see by comparing to the catalogue of `metrics` that includes precision and recall with score scores for each of multiple classes as well as an aggregate and error distributions pr curve or confusion matrix solomonm 1837 and ml an implementation within 1768 have independently sought precision and recall to be returned from cross validation routines when f1 is used as the cross validation objective eickenberg on https github com scikit learn scikit learn pull 1381 commitcomment 2607318 posed concern regarding array of scores corresponding to multiple targets thought it deserved an issue of its own to solidify the argument and its solution some design options allow multiple scorers to be provided to `cross val score` or searchcv` henceforth `cvevaluator` with one specified as the objective but since the `scorer` generally calls `estimator predict decision function predict proba each scorer would repeat this work separate the objective and non objective metrics as parameters to `cvevaluator` the `scoring` parameter remains as it is and `diagnostics` parameter provides callable with similar same arguments as `scorer` but returning dict this means that the prediction work is repeated but not necessarily as many times as there are metrics this diagnostics callable is more flexible and perhaps could be passed the training data as well as the test data continue to use the `scoring` parameter but allow the `scorer` to return dict with special key for the objective score this would need to be handled by the caller for backwards compatibility no existing scorers would change their behaviour of returning float this ensures no repeated prediction work add an additional method to the `scorer` interface that generates set of named outputs as with `calc names` proposed in 1837 again with special key for the objective score this allows users to continue using `scoring f1 but get back precision and recall for free note that and potentially allow for any set of metrics to be composed into scorer without redundant prediction work and allows composition with highly redundant prediction work comments critiques and suggestions are very welcome >>>api
use cross validation cross val score with metrics precision recall fscore support like to use cross validation cross val score with metrics precision recall fscore support so that can get all relevant cross validation metrics without having to run my cross validation once for accuracy once for precision once for recall and once for f1 but when try this get valueerror >>>enhancement
metrics confusion matrix usability issue usability issue current confusion matrix fct doesn return the the labels if you don specified it which make the matrix kind of useless for analysis my recommendation is that the function should return cm labels >>>enhancement
mrg evidence accumulation clustering evidence accumulation clustering eac an ensemble based clustering framework fred ana ln and anil jain data clustering using evidence accumulation pattern recognition 2002 proceedings 16th international conference on vol ieee 2002 basic overview of algorithm cluster the data many times using clustering algorithm with randomly within reason selected parameters create co association matrix which records the number of times each pair of instances were clustered together cluster this matrix this seems to work really well like kernel method making the clustering easier that it was for the original dataset the default of the algorithm are setup to follow the defaults used by fred and jain 2002 whereby the clustering in step is means with selected randomly from 10 and 30 the clustering in step is the mst algorithm which have yet to implement will do in this pr after initial feedback think people are happy with the api todo mst algorithm from the paper which was used as the final clusterer completed in pr 1991 there is an improvement to the speed of the algorithm don have the paper on hand that has been published that should be incorporated will be done in later pr examples usage narrative documentation revert test clustering line 508 to only check for spectralclustering use sparse matrix for the co association matrix >>>needs_review
problem with ridgecv centering the design matrix messes the solution when centering the design matrix cancels one of its singular values when this creates numeric mess in the ridgecv computation an example of the bad behaviour is given in https gist github com bthirion 5233416 still not fully sure about the right solution comments welcome best >>>bug
bug `` pipeline featureunion set params`` may overwrite attributes steps in pipelines are named where name equals that of an existing attribute on `pipeline` `transform` `predict` `steps` calling `set params` and setting that name to some value causes the attribute to be overwritten example consider the following this prints `` `` at and ``dummytransformer random state none strategy stratified `` at mechanism the general mechanism of `baseestimator set params` is to set the attribute corresponding to the parameter name in general this is restricted to small set of possible names usually corresponding to constructor arguments that would not conflict with non parameter attributes though don think there test to assure this where double underscore notation is used to set sub estimators parameters the sub estimator name `est` in `est some param` needs to be returned by the estimator `get params` hence `set params` allows `est` to be set in turn calling `setattr` without regard to existing attributes resolution don allow step names to be used as parameter names in `set params` or allow step names to be used as parameter names in `set params` meaningfully while prohibiting constructor arguments and existing attributes as names or prohibiting constructor arguments as names but special casing the setting of steps so that it doesn involve `setattr` rather the modification of the `steps` attribute which needs to happen somehow anyway this approach is taken by 1769 also test should be added for this case and perhaps more generally for all estimators to ensure `set params` does not overwrite class attributes methods etc >>>api
partial least squares fit raises singular matrix error hello pls1 worked fine but when trying to fit rather large matrices pls2 fits sometimes raise linalg error for certain number of components in this case however the fit works for components and works again for 20 traceback most recent call last file line in pls fit file python27 lib site packages sklearn pls py line 322 in fit linalg inv np dot self loadings self weights file python27 lib site packages scipy linalg basic py line 348 in inv raise linalgerror singular matrix linalgerror singular matrix am neither mathematician nor professional programmer so do not really understand what is going on there could upload the pickled arrays if needed shape 529 1486 shape 529 thanks for the great package matthias gerstl>>>bug
mrg enh enable setting pipeline components as parameters until now `get params would return the steps of pipeline by name but setting them would fail silently by setting an unused attribute this changeset also prohibits step names that equal initialisation parameters of the pipeline otherwise `featureunion set params transformer weights foo would be ambiguous it also contributes some refactoring of `pipeline` prediction methods >>>needs_review
dpgmm update concentrations fail implementation in method update concentration dpgmm think it should be self gamma self gamma sz instead of self gamma self gamma sz >>>bug
sparse random projections for randomized pca sparse random projections seem to make more sense for randomized pca since the main point is to deal with large matrices and it is easy to get to the point where the gaussian projection matrix becomes to large for ram it looks like it should be easy to change utils extmath randomized svd to use sparse random projections is there any reason not to do this if do it for my own use should submit it >>>enhancement
mrg training score in gridsearch this pr adds training scores to the gridsearchcv output as wished for by ogrisel >>>needs_review
plsregression outputs float64 when fed float32 this might matter for really large datasets as float32 is half the size would expect both float32 to output float32 however not sure what expect for hybrid 32 64 inputs maybe the type of >>>documentation easy
implement elkan accelerated means see the paper http www quretec com vilo edu 2003 04 dm seminar 2003 ii clustering elkan fast kmeans icml 2003 111 pdf it uses the triangle inequality to speed up updates bit trickier to implement than the standard algorithm but don see why we shouldn do it >>>new_feature
warm start aware grid search few estimators like `lasso` support the `warm start` option due to how `gridsearchcv` is designed use of `clone` parallelism with `n jobs` setting the `warm start` option wouldn have any effect it would be nice to create grid search object which can benefit from warm start `n jobs` could still be supported but should be with respect to cross validation folds this would require specifying with respect to what parameters the warm starting must be done `alpha` in the case of lasso >>>new_feature
nosetest failed fail doctest sklearn datasets mldata fetch mldata traceback most recent call last file usr lib python2 doctest py line 2163 in runtest raise self failureexception self format failure new getvalue assertionerror failed doctest test for sklearn datasets mldata fetch mldata file usr local lib python2 dist packages sklearn datasets mldata py line 28 in fetch mldata file usr local lib python2 dist packages sklearn datasets mldata py line 85 in sklearn datasets mldata fetch mldata failed example iris fetch mldata iris >>>bug
memory scalability issues in univariate feature selection gensim has re implementation of `selectkbest` to fix memory scalability issues someone should review them and it integrate the fix back into the original sklearn codebase https github com piskvorky gensim blob a73c84e21aecd3cc77ba2d752912f73b712bc60a gensim models selectkbest py maybe agramfort you can have look >>>bug
default value of normalize inconsistent in linear models this was pointed out in the survey >>>easy enhancement
dpgmm sample not working hi guys trying out dpgmm to see if can get more stable solution things look good so far except for one problem the sample method does not work for dpgmm the primary reason is that sample assumes there is self covars but this does not exist there is secondary problem however tried using get covars to set the covars but these covariances are not correct perhaps because of different convention somehow in the definitions of these covariances best and thanks for the good work on sklearn e>>>bug
api proposal genearlized cross validation and early stopping this is proposal to to resolve two api issues in sklearn generalized cross validation early stopping why should we care about that with generalized cross validation mean finding the best setting of some parameter without refitting the entire model this is currently implemented for rfe and some linear models via estimatorcv these don work well together with gridsearchcv as might be required in pipeline or when more than one parameter needs to be found also similar functionality would be great for other models like gradientboosting for estimators and all tree based methods for max depth with early stopping mean saving computations when more computation doesn improve the result we don have that yet but it would be great maybe even necessary feature for sgd based methods and bagging methods random forests and extra trees note that early stopping needs the use of validation set to evaluate the model how can we solve that let start with the generalized cross validation we need it to work together with gridsearchcv this will definitely require changes in both gridsearchcv and the estimators my idea give the estimator an iterable of values you want to try ``max depth range 10 `` during ``fit`` the estimator will fit in way that it can produce predictions for all of these values when ``predict`` is called the estimator will return dict with keys the parameter values and values the prediction values for these parameters we could also add new ``predict all`` function but not sure about that gridsearchcv could then simply incooperate these values into the grid search result for that gridsearchcv needs to be able to ask the estimator for which parameters it can do generalized cv and just pass on the list of parameters it got there so now to early stopping the reason want to treat the two problems as one is that early stopping is basically lazy form of generalized cross validation so you would provide the estimator with an iterable ``n iter range 100 10 `` the estimator fits for all these values as implemented as above but for each setting it also evaluates on validation set and if there is only small improvement training will stop would provide the validation set that is used either as parameter to `` init `` or ``fit`` not sure so it would be enough to add two parameters to the estimator ``early stopping tolerance`` and ``early stopping set none`` if it is none no early stopping there are two choices that made here provide separate validation set not generate one from the training set on the fly this is again so that this can be used inside gridsearchcv and doesn really add that much overhead if the user doesn use gridsearchcv why would they do that any way it is also very explicit and gives the user lot of control provide the parameter settings as an iterable not maximum the reason for that is that you probably don want to evaluate the validation set every iteration but maybe every iterations what another parameter feel like an iterable is good way to specify this also it allows for unified interface with the generalized cross validation case restrictions in pipelines this will only work for the last estimator so not sure to do this with rfe for example do we really need this the changes proposed above are quite big in some sense but think the two issues need to be resolved if you have any better idea feel free to explain it >>>api new_feature
improvements remove unneeded code from balltree as discussed in 1605 with the equal distance warning removed from the ball tree there is some unnecessary code in the ball tree source it should be removed also now that we can rely on cython 17 we should rework balltree to use typed memoryviews rather than raw pointers it should lead to cleaner implementation with negligible speed penalty ve been starting some work in this direction at https github com jakevdp binarytree>>>enhancement
common tests check that results on sparse and dense matrices are identical as proposed by zxtx >>>enhancement
check stability of vbgmm and dpgmm both seem very unstable wrt random initialization maybe we should try multiple random inits by default as in means or we could try to do some smarter initilalization maybe >>>enhancement
implement means based support point sampling for nystroem method just apply means to the training set and use the clusters as support points >>>easy enhancement
make grid search over kernel parameters possible the chi2 kernel has gamma parameter over which one needs to grid search for the use in svc currenlty think this is only possible doing something like this unfortunately this doesn pickle because of the lambda function so only ``n jobs 1`` is possible is there way around that don see >>>enhancement
implement faster ardregression following http books nips cc papers files nips20 nips2007 0976 pdf>>>enhancement
consistency in gmm get covars there are some consistency issues in the mixtures module the covariance matrices in vbgmm and dpgmm are called ``precision`` and ``covars`` in gmm for example there is also `` get covars`` function in gmm that actually seems pretty helpful it provides the full covariance matrices independent of the covariance type but find the docstring misleading and it is privat so it is not visible in the online docs and to autocompletion >>>enhancement
sparse recovery lasso elasticnet example this example thttp scikit learn org stable auto examples linear model plot lasso and elasticnet html doesn have docstring lasso seems to work better than elasticnet am not sure what the example is supposed to illustrate btw think it would be nice if there was some example comparing lasso and ard on some sparse data >>>documentation
fix class weight parametrization in naive bayes there are some loose ends after merging 1491 and 1499 in nb the class weights are now lists they should be list and also accept an auto keyword that should be easy enough to do using ``sklearn utils compute class weight`` see 1491 >>>easy enhancement
improve the explanations of the loss plot in the sgd section we should reuse mix of the answers to http www reddit com machinelearning comments 15to9t loss function figure explanation to better explain the source plot from the doc http scikit learn org dev modules sgd html mathematical formulation>>>documentation easy enhancement
permutation test score is not tested as far as can tell >>>enhancement
add linearsvr our copy of liblinear already contains it we just need to expose it on the python side >>>enhancement
retrieve the mask in selectormixin as was pointed out by one my students there is currently no way to retrieve the selected features upon `transform` in `selectormixin` users should be given the possibility to access in one way or another the `mask` variable that is computed in `transform` >>>enhancement
mnist gets loaded in doctests something seemed to have changed in the doctests recently mnist now gets loaded on both of my boxes meaning can not really run tests in the background any more mnist takes lot of ram to load and my box started swapping on my new laptop the test failed because war running something else and it threw memory error >>>documentation enhancement
mrg add resample to preprocessing there have been several requests lately for class rebalancing using under oversampling this utility function addresses most of the use cases can think of for sampling with replacement from dataset one thing it does not do is to sample without replacement before sampling with replacement because it changes the code substantially and there is no efficient version of ``random sample`` as per https github com scikit learn scikit learn pull 1438 issuecomment 11162893 could add that feature eventually >>>needs_review
mds fall back to svd when the full similarity matrix is known right now the mds uses slow majorization optimization method instead of falling back to the svd when the full matrix is known >>>easy enhancement
add pyflakes and pep8 to travis >>>enhancement
doc sklearn metrics auc score should mention that using probabilities will give better scores the documentation at http scikit learn org dev modules generated sklearn metrics auc score html sklearn metrics auc score says that `y score` can be either probability estimates of the positive class or binary decisions it should warn the reader that by using binary decisions it is only able to compute auc as if the classifier only returned probabilities and and thus not give the real auc here is an example python from sklearn linear model import logisticregression from sklearn import metrics from sklearn import cross validation from sklearn import datasets data datasets load digits data data data target make the classification problem binary clf logisticregression 001 fold cross validation kfold len 10 indices true shuffle true random state 18 aucs aucs proba for train test in fold clf fit train train aucs append metrics auc score test clf predict test aucs proba append metrics auc score test clf predict proba test print aucs print aucs print aucs with probabilities print aucs proba this is the output python aucs 97222222222222221 97058823529411764 aucs with probabilities 99673202614379086 admit this is not very good example as the difference between `aucs` and `aucs proba` could be lot bigger in practice but wanted to use built in data set note that auc computed from binary decisions is always inferior to the auc computed with probability estimates >>>documentation
fit intercept in ridge sparse case in the sparse case `ridge` silently ignores `fit intercept` currently it equivalent to `fit interept false` using the recently added `add dummy feature` it should now be easy to support `fit intercept true` since it corresponds to penalizing the intercept we need to add `intercept scale` option to the constructor like `linearsvc` `add dummy feature` results in copy of `x` suggests to set `fit intercept auto by default in the dense case auto can correspond to `fit intercept true` and in the sparse case to `fit intercept false` to avoid the memory copy this way the code will be backward compatible too >>>bug
single sample test time performance test time performance on single samples is important in real world applications currently performance on individual samples is often governed by input validation rather than model evaluation consider the following profile of ``gradientboostingregressor decision function`` trained on boston using 250 trees 645 151 151 53 array2d dtype dtype order 646 49 49 17 score self init decision function 647 78 78 27 predict stages self estimators self learning rate score 648 return score the major reason is that ``sklearn validation array2d`` calls ``scipy sparse issparse`` twice this could be fixed but still the overhead from checking if the array values are finite is considerable we should optimize input validation or provide means to turn it off >>>easy enhancement
cross validation and tools for undersampling and oversampling in learning with unbalanced classes it is often good to stratify the data not as in stratifiedkfold by making the classes the same size either by oversampling or undersampling think it would be good to have both utility function that does that once and cross validation object that does that differently for all folds >>>new_feature
fetch 20newsgroups should say it takes while this script gives an error from sklearn datasets import fetch 20newsgroups data train fetch 20newsgroups subset train shuffle true random state 42 think it does not download the 20news bydate tar gz file if download it manually to the scikit learn data folder it fixes the error >>>easy need_contributor
precompute square norms in passive aggressive `passiveaggressiveclassifier` is currently times slower than `perceptron` on the document classification example this is because the current implementation recomputes the square norm which is needed to compute the step size for each example every time the solution would be to add `void precompute norms squared true and `double get norm int to `sequentialdataset` doing so expect the performance to be inline with `perceptron` cc pprett >>>enhancement moderate
check that all attributes are documented list of all attributes can be found here https gist github com 3973308 by jaquesgrobler can we check automatically whether they are all documented or do we want to check by hand >>>documentation
missing features in onehotencoder using features where only certain entries are categorical support sparse input support non sparse output>>>enhancement moderate
clean up examples in tutorial the tutorial duplicates many of the examples already present for example now there is color quantization example and grey scale quantization example also there are now three means examples pretty sure we can get rid of some maybe jaquesgrobler can have look >>>documentation easy
update authors rst it seems bit out of date >>>documentation
create examples tests for custom estimators not inheriting from baseestimator also see 1241 >>>documentation enhancement
adaptive learning rate for sgd enhancement considering implementing new adaptive learning rate algorithm proposed here http arxiv org abs 1206 1106 anyone have any feedback about where this should go was thinking it would be added somewhere around https github com scikit learn scikit learn blob master sklearn linear model sgd fast pyx l383 since sgd is also applicable to non linear models neural network models are there any plans to move that outside of linear model >>>new_feature
linking problem with atlas on os see mailing list http sourceforge net mailarchive forum php thread name 507f9b81 3050604 40ais uni bonn de forum name scikit learn general >>>bug
meta estimator for semi supervised learning using self taught learning it is possible to turn any estimator into semi supervised one not that hard to do >>>new_feature
refactor clone testing in ``clone`` there is lot of testing going on but it seems to me that doesn catch all important cases also these tests are not unit tested do we really want to do these tests at runtime instead of in the unit tests and if so how can we improve them maybe we should look into the dict all parameters that are not init parameters should either be internal starting with `` `` or estimated `` `` maybe so that would be easy to check >>>enhancement
add api for user defined distance function in means we had case where we wanted to use levenshtein distance instead of the usual euclidean distances we ended monkey patching the cluster module think it ll be nice to add another parameter to kmeans class which is user defined distance function if it not defined then the code will default to usual euclidean distances >>>new_feature
proposal to decouple classifier output classes from the vector with classes parameter in init from the discussion on the mailing list http sourceforge net mailarchive message php msg id 29883772 think we could have `classes none` constructor parameter in sgdclassifier an possibly many other classifiers when provided we would not use the traditional `self classes np unique idiom already implemented in some classifiers of the project but not all also for raising valueerror exception when `classes none` and if the `y` provided at fit time has some values not in `classes` however we need to check with some benchmarks that this integrity check is not too costly this constructor parameters could be overriden by `fit param` to preserve backward compat especially for classifier models with `partial fit` method the expected behavior for classifier that is passed non none `classes` constructor param would be to never predict class value in case of predict proba method the missing fit time class probabilities should be this protocol including expected exception types and error messages should be formalized as series of common tests in sklearn tests test common py and redundant book keeping code should be factorized in the sklearn base py classifiermixin class imho oliver grisel>>>enhancement
gridsearchcv allow for passing extra vectors to score loss function there are situations when having vectors pred and true is not sufficient to compute the score loss function an easy example would be mean squared absolute error weighted by pre defined column of weights in this case this column vector has to be sliced by the cross validator the same way and are sliced and the sliced vector is to be passed to the score loss function while understand that gridsearchcv can be fairly painlessly modified to account for this say an optional parameter to the score loss func was wondering if anyone else ever faced the same problem >>>enhancement
implement parallelized sgd as in nips 2010 paper see http cs markusweimer com pub 2010 2010 nips pdf this will be trivial to implement efficiently once we have proper support for shared memory both for the coef vector and the data in joblib joblib joblib 44>>>enhancement
add stemming support to countvectorizer an idea for feature enhancement currently using `sklearn feature extraction text countvectorizer` https github com scikit learn scikit learn blob master sklearn feature extraction text py l86 for one of my projects in my opinion it strongly lacks support for stemming an additional attribute such as `self stemmer` which accepts stemming function as callable would be nice together with reasonable default stemmer for english what about an additional stemmer module however there are already some quite good python stemmers available for example as part of the natural language toolkit longer time ago contributed stemmers for 12 languages which are available in `nltk stem snowball` https github com nltk nltk blob master nltk stem snowball py maybe one could support them as dependency in scikit learn the general interface however is not difficult to implement for example one could do it like this introduce an additional attribute `self stemmer` which is initialized as `none` by default write method `build stemmer` as something like this python snowball stemmers could be used as dependency from nltk stem import snowballstemmer def build stemmer self if self stemmer is not none return self stemmer one could provide an english stemmer as default english stemmer snowballstemmer english return lambda tokens english stemmer stem token for token in tokens incorporate this method call in `countvectorizer build analyzer https github com scikit learn scikit learn blob master sklearn feature extraction text py l358 as something like this python def build analyzer self elif self analyzer word stop words self get stop words add stemmer instance here stem self build stemmer tokenize self build tokenizer include stemmer in the method chain return lambda doc self word ngrams stem tokenize preprocess self decode doc stop words what do you think >>>new_feature
univariate feature selection example confusing wrong find the univariate feature selection http scikit learn org dev auto examples plot feature selection html example confusing it claims that the svm assigns small weights to the significant features but to me it looks like it assigns very large weights also the axis is not labeled and it is not immediately clear to me what the meaning is maybe because not accustomed to seeing value plots closer examination shows ``scores `` actually stores the scores not the values so the legend is wrong and the text misleading also maybe document ``scores `` and ``p values`` attributes in the univariate feature selectors >>>documentation
remaining cleanups in rfecv after 1128 is merged the following easy issues remain in rfecv the scores are stored in dictionary where numpy vector would be more appropriate the scores actually store losses not scores in sklearn speak high scores are good for the second remaining one am not sure how to address think the best thing would be to actually use scores and store scores and add ``score func`` parameter that is an incompatible change though as we keep the same name but change the meaning >>>enhancement
hand waving practical documentation for lasso and elasticnet regression the current narrative documentation http scikit learn org dev modules linear model html elastic net for lasso and elasticnet regression is very mathematical and does not give practical hints on the usage of those models as remarked by this user on stackoverflow http stackoverflow com questions 12283184 how is elastic net used we should quickly explain why l1 penalty and l1 l2 penalty are interesting in practice rather than just mathematically ideally pros vs cons section comparing lasso elasticnet and reference to randomizedlasso for regression and or feature selection also add links to `sgdregressor` with matching loss and penalties for the low penalized and large `n samples` case >>>documentation easy
we need way to run examples as tests this is slightly related to 857 as part of the developer workflow we often need to run cd doc make html and scan that horrible horrible output to see if anything fails or prints nasty stuff not even redirecting `stdout` to dev null` helps too much since `stderr` is also polluted by sphinx does anybody know of precedent or if not can we hack together way to run the examples as if they were test suite so we can have an output like maybe we can do this with `nose` itself >>>enhancement
implement cluster stability measure this is probably the only decent unsupervised clustering evaluation see here http arxiv org abs 1007 1075 also this one http www kyb mpg de fileadmin user upload files publications attachments benluxpal06 5b0 5d pdf>>>new_feature
overhaul clustering docs there is to much detail on the evaluation metric and to little on the clustering methods the evaluation metrics formulae and details should go on separate page at least the details also most of the examples look ugly do not appeal to my intuition of the algorithms >>>documentation easy
nearest neighbors interface was just talking to ryan from mlpack he did benchmark of some nearest neighbors implementation when he used sklearn he had hard time figuring out how to use particular method for fitting and just get the nearest neighbors for given set looked at the docs and it is not so obvious to me it seem you have to construct tree and then feed it to ``kneighbors graph`` or use ``nearestneighbors`` and then look at the internal and hence undocumented `` tree`` attribute feel that should be easier to doo >>>easy enhancement
support for sparse matrices in random feature selection ``randomizedlasso`` and ``randomizedlogisticregression`` currently don support sparse matrix input but it should be easy enough to add >>>moderate new_feature
rank normalization of features was talking about this feature with ogrisel and he asked me to place an issue for it this is technique suggested by yoshua bengio to handle features with unknown scale convert the features to rank scale so the lowest rank is and the highest rank is this is superior to transform zero mean unit variance because if you have one huge outlier feature it can mess things up but rank transform is robust you can see description here of python code to do this http stackoverflow com questions 3071415 efficient method to calculate the rank vector of list in python scipy stats rankdata does it they convert to ranks but don normalize by the number of features one thing to be careful of if you have lot of zeros and do rank transform using scipy stats rankdata and then normalize by the number of features they will end up have rank so you lose sparsity would recommend to preserve sparsity that you scale the range of the ranks to and clip any feature from the test set that exceeds the range >>>new_feature
implement roc svm linear pairwise ranking loss with sgd to address highly imbalanced data see http www eecs tufts edu dsculley papers detecting adversarial advertisements pdf >>>new_feature
wip gbrt with built in cross validation two new classes ``gradientboostingclassifiercv`` and ``gradientboostingregressorcv`` which pick ``n estimators`` based on cross validation ``gradientboostingclassifiercv`` fits ``gradientboostingclassifier`` with ``max estimators`` for each fold it picks ``n estimators`` based on the min deviance averaged over all test sets finally it trains the model on the whole training set using the found ``n estimators`` ``gradientboostingclassifiercv`` is implemented as ``gradientboostingclassifier`` decorator it soley implements ``fit`` otherwise it delegates to ``gradientboostingclassifier`` see `` getattr `` and `` setattr `` the current implementation might pose some problems if the client uses ``isinstance`` rather than duck typing ``gradientboostingclassifiercv`` instance is not an instance of ``gradientboostingclassifier`` would really appreciate any remarks feedback to this issue tried to adhere the interface of ``ridgecv`` additionally refactored the prediction routines in order to remove code duplication ``staged predict`` and ``staged predict proba`` has been added to ``gradientboostingclassifier`` limitations it is currently hard wired to pick ``n estimtors`` based on deviance no support for custom loss function yet is this needed no joblib support yet only cross validation support should we add held out and oob estimation too >>>new_feature
wip grid search convenience class this aims at closing 1020 it introduces new class to handle the output of gridsearchcv don like complexity but haven found nice way to do this otherwise if you have less complex solutions please let me know basically this transforms the dicts that are usually in ``gridsearchcv grid scores `` remember this is in general list of dictionaries to list of parameters which is ``sorted param grid keys `` and an array where each axis corresponds to one parameter and the last corresponds to folds think this is already an improvement the reason why added the class is that also want to marginalize parameters want to look at it even if have parameters to adjust maximizing over multiple axis is ugly so wanted to class to handle this as always any comments welcome ll make an example to illustrate the usefulness now going back to wip as think this should be designed with having non grid evaluations of estimators in mind >>>new_feature
better access visualization of grid search results the ``gridsearchcv grid scores `` are stored in not very user friendly way it would be great to have nd arrays and set of labels things that should be easily possible plot in the case of parameter matshow in the case parameters look at slices and marginals mean over rest of parameters when looking at more parameters>>>new_feature
raise warning if only one class presented to classifier if only one class is presented to classifier in non multi label setup it should raise an error give warning this should be tested for in the common testing framework >>>enhancement moderate
mds in the example the reconstruction may sometimes be inversed compared to the real solution mds yields solution which is rotation and symmetric invariant the example code realigns the structure rotation wise but doesn check whether it is mirror version of the true positions to facilitate the visualisation of the results it should >>>documentation easy
enh use bayesian priors in nearest neighbors classifier issue 399 the nearest neighbors and radius neighbors classifiers now make explicit use of the bayesian prior probabilities of the classes `self class prior by default each of these prior probabilities for given class is set to the proportion of sample points which are in the given class but they can also be set to other values by the class user this is my first submission so be grateful for any and all advices thank you >>>new_feature
adding pruning method to the tree have added pruning method to the decision trees method the idea with decision tree is to build huge one prune it via weakest link algorithm until it reaches size that is reasonable neither overfitting nor under fitting also have build helper function cv scores vs leaves that computes the cross validated scores for different sizes of the tree this can be plotted with function such as python def plot cross validated scores scores plots the cross validated scores versus the number of leaves of trees import matplotlib pyplot as plt means np array np mean for in scores stds np array np std for in scores range len scores plt plot means plt plot means stds lw plt plot means stds lw plt xlabel number of leaves plt ylabel cross validated score then we choose which size is the best for the data just couple of notes need to add some tests am not sure how to make them this also needs to be documented before doing that work would gladly have some feedback on these modifications>>>new_feature
mrg svmlight chunk loader hi all am working on an incremental data loader for the svmlight format that reads chunks of big file that is not expected to all fit in memory in smaller csr matrix to be dumped as set of memmapable files in folders to be later reconcatenated into single large csr memmaped matrix the goal being to be able to load big svmlight files multiple tens of gb into an efficient memmaped csr in an out of core manner possible using several workers in the first step is to augment the existing parser to be able to load chunks of svmlight using seeks to bytes offsets >>>needs_review new_feature
implement density based threshold sampling for extra trees just learned trick for feature sampling from extra trees that we might want to implement instead of sampling threshold uniformly it is often better to sample training sample and use its value as threshold this means the thresholds are sampled from the actual feature distribution and thresholds probably lie in dense regions >>>new_feature
wip covariance updates this very much work in process for issue 910 the idea is to get first python version of the covariant updates to work and turn it then step by step into cython code reference friedman hastie and tibshirani regularization paths for generalized linear models via coordinate descent journal of statistical software 33 no 2010 >>>new_feature
add predict to randomized lasso predict is missing from the randomized lasso objects need to add `classifiermixin` and `regressor mixin` too >>>enhancement
feature request probabilistic matrix factorization don believe there are implementations for many algorithms behind recommendation engines would like to start with probabilistic matrix factorization http www cs utoronto ca amnih papers pmf pdf thoughts >>>new_feature
add kmeans parameter for pruning small clusters in kmeans often some clusters have only very little data this might happen for all random initializations for this case would like to have an option to set minimum cluster size after which cluster is dropped and new one is created >>>moderate new_feature
feature request implement mars because why not also according to elements of statistical learning it seems to perform quite well also see http www salford systems com doc mars pdf from http news ycombinator com item id 3947653>>>new_feature
test setup py does not actually prevent multiprocessing ok so thought had fixed this but had not https github com scikit learn scikit learn commit 2b287c3bce451133ea477d28403eeeb2c01af7b7 this has no effect got fooled by having manually set the environment variable as well since this is windows only hack would be to do something like system set var name instead and see if it works >>>bug
graph laplacian difference trying to refactor similarity matrices and graph and related manifold learning components and noticed that what scikits currently calls normed graph laplacian assuming normed means normalized is very different in output from the equations from von luxburg http www kyb mpg de fileadmin user upload files publications attachments luxburg07 tutorial 4488 5b0 5d pdf any thoughts on what normed means in this context the reason this is important is because the normed version is used in spectral clustering and that different from any of the approaches in the paper current output proposed changes based on von luxburg see commit in satra scikit learn 2d289ec >>>enhancement
token pattern in countvectorizer etc hey guess this just may be problem with the documentation on the website class sklearn feature extraction text countvectorizer input content charset utf charset error strict strip accents none lowercase true preprocessor none tokenizer none stop words none token pattern min max analyzer word max df max features none vocabulary none binary false dtype my intuition is that need to write token pattern guess the intern implementation is correct but when try to pass the standard token pattern as written in the documentation get an error the same for the other text feature extraction methods regards philipp>>>documentation
scale params for linear models as discussed on the mailing list the way the regularization parameter is scaled in linear models can be fragile to some simple variations of the data such as when the number of training samples vary this is the case of libsvm for which we tried to come up with rescaling of the parameter that ends up being burden as the resulting api no longer matches closely the libsvm api the problem is more general than libsvm and propose that an optional scale params parameter be added to some linear models to put the regularization parameter in more adimensional form in the future it can be added to other estimators for l1 penalized models the way should be scaled is fairly natural and given by the kkt conditions as implemented in svm l1 min note that to convert the parameter of svms logreg to alpha in lasso and enet you have to take something like alpha samples for l2 penalized model there is no such abrupt change and suggest to investigate using the l2 norm of xy instead of the inf max in svm l1 min here is the battle plan transform amueller gist into scikit learn example https gist github com 2354823 implement scale params for l1 models logistic regression and lasso add these models to the example check that scale params works factor out the logic of l1 min into penalty min heuristic function that works for l1 and l2 and use it to implement l1 minc and l2 min implement scale params for l2 models svm ridge add these models to the example check that the scaling does work add warning in gridsearchcv that check if an estimator has scale params attribute and that raises warning if it is set to false remove the scale parameter from svms this should be it and heard that jaquesgrobler was volunteered to do this >>>enhancement
iterative solvers for ridge and warm start iterative solvers for `ridge` would allow to support the `warm start` constructor option relevant links http docs scipy org doc scipy reference generated scipy sparse linalg cg html scipy sparse linalg cg http docs scipy org doc scipy reference generated scipy optimize fmin cg html scipy optimize fmin cg>>>enhancement
refactor class weight logic it duplicated in svm and sgd code and somewhat in the other linear models >>>enhancement moderate
fixup naive bayes the ``naive bayes`` needs some love the current implementation of ``basediscretenb`` requires that the inputs are positive because otherwise the log computation in line 275 will fail if ``basediscretenb`` is only applicable to non negative inputs we should enforce that ``gaussiannb`` what if the variance of feature is zero did quick fix and add small constant epsilon 1e ``gaussiannb`` and ``basediscretenb`` should provide similar functionality ``gaussiannb`` lacks fit parameter ``class prior`` and ``class weight`` ``gaussiannb`` has performance regression on ``bench covtype`` error rate used to be 23 now 46 this needs to be investigated ``gaussiannb`` has poor test time performance should be similar to ``sgdclassifier`` use fortran style for model parameters >>>enhancement
intercept sign in svm needs love made an ugly hack in 634 to fix regression introduced earlier would prefer prettier solution >>>enhancement
what would be the best way to add precomputed kernel support in ridgecv like to be able to use precomputed kernels with ridgecv would you be interested in this if so like to hear your thoughts on how to do this well so that can contribute pr related question when `n samples features` there is more efficient way to get ridgecv looe for free with an svd of `x` again would you be interested in this how should factor the code to contribute and thanks again for such great package n>>>new_feature
fibonacci heaps for ball tree the ball tree neighbor search could potentially be sped up greatly especially for large `n neighbors` and `n features` by using fibonacci heaps instead of the binary heap priority queue structures used currently there is fast cython fibonacci heap in `sklearn utils graph nearest neighbor pyx` this should be factored out so that it can also be used in the ball tree >>>new_feature
implement adaptive lasso reweighted l1 here is proof of concept implementation by agramfort https gist github com 1610922 implementing this in the scikit would require conversion to the scikit learn api naming conventions example comparing with classic lasso narrative doc what motivated agramfort gist was http books nips cc papers files nips24 nips2011 1135 pdf see references therein for the original contributions >>>new_feature
shape problems building pipeline containing patchextractor and minibatchdictionarylearning as part of learning project was trying to adapt the image denoising example http scikit learn org stable auto examples decomposition plot image denoising html example decomposition plot image denoising py to use pipeline containing `patchextractor` instead of manipulating the image data using `extract patches 2d` the attempt can be found here https gist github com 1525765 for either value of `reshape` the code throws `valueerror` expected result no exceptions thrown perhaps with the introduction of `reshape` preprocessor or `flatten` or `shape` parameter to `patchextractor` as far as can tell the issue is due to shape incompatibilities between `patchextractor` and `minibatchdictionarylearning` `patchextractor` expects and yields 3d tensor of images images image height image width however `minibatchdictionarylearning` wants the `image height` and `image width` flattened images image height image width the original example code explicitly reshapes the output of `extract patches 2d` that not currently easily available in the middle of pipeline >>>enhancement
sparse sparse and sparse dense dot product with dense output `safe sparse dot` has an option `dense output` which allows to specify that the dot product between two sparse matrices or between sparse matrix and numpy array should be output to numpy array however scipy currently always return sparse matrix therefore `safe sparse dot` converts it afterwards with `toarray it would be better if we had cython utilities that can directly output to numpy array thus avoiding needless memory allocation and copying those utilities are fairly low level and should probably be contributed to scipy too but feel we should have them in scikit learn until the version of scipy that implements them becomes mainstream used in euclidean distances linear kernel ridge regression features samples case nmf necessary functions for sparse dense dot csr out dot csr fortran out necessary functions for sparse sparse dot csr csc out other situations can be handled by using dot dot t>>>new_feature
implement gaussian process based hyper parameter optimizer following bergstra nips 2011 jaberg for the intimates it would be great to have gridsearch like object implementing gaussian process based hyper parameter optimization to keep things simple we would optimize only on continuous parameter in hyper cube the api would specify an initial bounding box and also whether each parameter should be varied in the log domain or in the linear domain some notes from discussion with james bergstra the algorithm goes mostly as compute the score on set of random points in the specified hyper cube for in budget fit gaussian process to the pairs parameters scores find the parameters that optimizes the expectation of getting lower score then the best currently available for this optimize it using black box scipy optimizer such as the simulated annealing lot of the gain comes from sequentially choosing the points so as we don have queue mechanism we should do this sequentially for now parallel can be done in internal cross validations or to compute initial points remark to computing the expectancy of with gaussian process we should overide score to use something more clever than the standard linearregression score >>>new_feature
mclust for gmm mclust http www stat washington edu mclust is the standard package for mixture modeling in the stats world essentially it is doing model based clustering as described in fraley and raftery 2002 www stat washington edu raftery research pdf fraley2002 pdf this means gaussian mixture modeling essentially it has number of choices for regularizing the covariance matrix it then uses bic to choose the current implementation of gmm in sklearn has several of these options full diag tied spherical but that is only about half the options in mclust and then while see bic is implemented for lassolars it seems to be tied to that so wonder whether there are plans to update gmm py to include other versions of covariance regularization update model selection to include bic generally couldn find the src for the bic implementation so can tell how bic is implemented now if no plans exist might fork pymix has some model selection using bic http pymix org docu mixture pysrc html modelselection so some of that code could be borrowed thanks all for building such useful stuff >>>enhancement
add gaussian kernel to mean shift clustering motivation currently the mean shift clustering uses flat kernel while this is quick and simple there are some disadvantages there is no guarantee that the center will be the densest part flat kernel it might be that the center is sparse and that single kernel covers the centers of two or more clusters at its periphery if you mess around bit with the test case by changing the bandwidth or seeding method this disturbing behavior comes out the center of the found clusters are in fact between the true cluster centers propose that we should also implement gaussian kernel because such kernel would prefer to have dense regions near the center and this is common kernel in the literature the bandwidth parameter would act as the standard deviation for the gaussian kernel propose that we use the same standard deviation in all dimensions just as we used the same radius in all dimensions for the flat kernel efficiency considerations the flat kernel is efficient because only points with certain radius specified by the bandwidth parameter are relevant for the mean shift calculation these points can be efficiently looked up from the larger dataset using the balltree in theory each time we update gaussian kernel we should also consider all points in the dataset but in practice the method may work well if we ignore all points beyond some distance let call this the cutoff distance given that we expect over 99 of points to fall within three standard deviations of the center of gaussian distribution perhaps we could set this cutoff to 3x the bandwidth >>>new_feature
reorthogonalize we were discussing reorthogonalization on the mailing list since tried it out here trivial patch on top of my other pull request https github com scikit learn scikit learn pull 423 so there some noise in the commits submitting this to contribute to the discussion don think this patch should be accepted as is my observations how much the power iteration helps apparently depends on how quickly the singular spectrum decays the matrices was playing with had slowly decaying spectra found 10 got acceptable results judging by whether the singular values matched the non randomized results didn find that the orthonormalization step helped much for my case and the qr did slow things down significantly so didn pursue this patch further perhaps orthonormalizing every few iterations would be good balance >>>new_feature
bayesian priors in nearest neighbors classification regression currently the classification and regression algorithms in the `neighbors` module use flat prior they should be modified to compute prior based on training data and to optionally accept user defined prior >>>new_feature
implement linear svc predict and decision function in pure python `linearsvc` and `svc` `predict` and `decision function` could easily be implemented in pure python advantages reduce binding size avoid the creation of liblinear and libsvm internal models from numpy arrays sparse matrices at each call disadvantage less memory efficient if `predict` is implemented as the argmax of `decision function` this should also solve the problem that `decision function` is not implemented in `sparse svc` in any case we should probably do little benchmark to make sure we don loose too much in runtime speed >>>enhancement moderate
default tolerance for liblinear libsvm wrappers liblinear and libsvm default tolerance parameters are fine tuned for standardized data by upstream authors and give good compromise between accuracy and convergence speed mblondel suggests having an objective function dependent default values for tol we could set tol to none and use dictionary to set the default value >>>enhancement
wip power iteration clustering here is an early pull request far from being ready for merging to master lacks tests docs and class that implements the estimator public api the goal is to let other developers know about some ongoing work to implement power iteration clustering variant of spectral clustering that is supposed to be more scalable according to the authors of the reference paper in practice am not so sure about the tradeoff speed robustness of the results we need to implement better clustering metrics as mentioned in the paper so as to be able to do principled comparison and evaluate the performance of the algorithm edit here is direct link to the reference paper http www cs cmu edu wcohen postscript icml2010 pic final pdf feedback greatly appreciated status todo before merging find way to quantitatively evaluate the quality of the clusters implemented measure and adjusted rand index now in master investigate with the dependency on the value of the `tol` hyperparameter write documentation write tests cleanup the convergence plot code or find non intrusive generic api to deal with such convergence monitoring maybe using callbacks reproduce convergence results of the paper on the 20 newsgroups dataset >>>new_feature
use python logging to report on convergence progress it level info for long running tasks this is proposal to use python logging module instead of using stdout and verbose flags in the models api using the logging module would make it easier for the user to control the verbosity of the scikit using single and well documented configuration interface and logging api http docs python org library logging html>>>new_feature
add test for multiple drops on lar see comments on commit 269ab1af31d362a7bb2955936646c7478602a68b>>>enhancement
add docs on how to build with custom lapack >>>build_/_ci documentation
